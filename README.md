# bruno-llm

[![Python Version](https://img.shields.io/badge/python-3.9%2B-blue)](https://www.python.org/downloads/)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)
[![Code style: black](https://img.shields.io/badge/code%20style-black-000000.svg)](https://github.com/psf/black)
[![Type Checked: mypy](https://img.shields.io/badge/type%20checked-mypy-blue)](http://mypy-lang.org/)

**bruno-llm** provides production-ready LLM provider implementations for the [bruno-core](https://github.com/meggy-ai/bruno-core) framework. Easily swap between different language model providers (Ollama, OpenAI, Claude, etc.) through a unified interface.

## ğŸ¯ Features

- **ğŸ”Œ Unified Interface**: All providers implement bruno-core's `LLMInterface`
- **âš¡ Async-First**: Non-blocking I/O for all operations
- **ğŸ”„ Streaming Support**: Real-time response streaming
- **ğŸ’° Cost Tracking**: Track API usage and costs per provider
- **ğŸ›¡ï¸ Error Handling**: Comprehensive exception hierarchy
- **ğŸ” Retry Logic**: Automatic retry with exponential backoff
- **â±ï¸ Rate Limiting**: Built-in rate limiting for API calls
- **ğŸ§ª Well Tested**: 90%+ code coverage
- **ğŸ“ Type Safe**: Full type hints and Pydantic validation

## ğŸš€ Quick Start

### Installation

```bash
# Basic installation
pip install bruno-llm

# With OpenAI support
pip install bruno-llm[openai]

# For development
pip install bruno-llm[dev]
```

### Basic Usage

#### Ollama (Local LLM)

```python
import asyncio
from bruno_llm.providers.ollama import OllamaProvider
from bruno_core.models import Message, MessageRole

async def main():
    # Initialize Ollama provider
    llm = OllamaProvider(
        base_url="http://localhost:11434",
        model="llama2"
    )
    
    # Generate response
    messages = [
        Message(role=MessageRole.USER, content="Hello! Tell me a joke.")
    ]
    response = await llm.generate(messages)
    print(response)
    
    # Stream response
    print("\nStreaming response:")
    async for chunk in llm.stream(messages):
        print(chunk, end="", flush=True)

asyncio.run(main())
```

#### OpenAI

```python
import asyncio
from bruno_llm.providers.openai import OpenAIProvider
from bruno_core.models import Message, MessageRole

async def main():
    # Initialize OpenAI provider
    llm = OpenAIProvider(
        api_key="sk-...",
        model="gpt-4"
    )
    
    # Generate response
    messages = [
        Message(role=MessageRole.USER, content="Explain quantum computing")
    ]
    response = await llm.generate(messages, temperature=0.7)
    print(response)

asyncio.run(main())
```

### Integration with bruno-core

```python
from bruno_core.base import BaseAssistant
from bruno_llm.providers.ollama import OllamaProvider
from your_memory import YourMemory  # Your memory implementation

# Create LLM provider
llm = OllamaProvider(model="llama2")

# Create assistant
assistant = BaseAssistant(llm=llm, memory=YourMemory())
await assistant.initialize()

# Process messages
message = Message(role=MessageRole.USER, content="Hello!")
response = await assistant.process_message(message)
print(response.text)
```

## ğŸ“¦ Supported Providers

| Provider | Status | Features |
|----------|--------|----------|
| **Ollama** | âœ… Available | Local inference, streaming, multiple models |
| **OpenAI** | âœ… Available | GPT-3.5/4, streaming, cost tracking, tiktoken |
| **Claude** | ğŸš§ Planned | Anthropic Claude models |
| **Gemini** | ğŸš§ Planned | Google Gemini models |

## ğŸ—ï¸ Architecture

```
bruno-llm/
â”œâ”€â”€ bruno_llm/
â”‚   â”œâ”€â”€ __init__.py          # Public API
â”‚   â”œâ”€â”€ __version__.py       # Version info
â”‚   â”œâ”€â”€ exceptions.py        # Exception hierarchy
â”‚   â”œâ”€â”€ factory.py           # Provider factory
â”‚   â”œâ”€â”€ base/                # Base utilities
â”‚   â”‚   â”œâ”€â”€ base_provider.py # Abstract base provider
â”‚   â”‚   â”œâ”€â”€ token_counter.py # Token counting
â”‚   â”‚   â”œâ”€â”€ rate_limiter.py  # Rate limiting
â”‚   â”‚   â”œâ”€â”€ retry.py         # Retry logic
â”‚   â”‚   â””â”€â”€ cost_tracker.py  # Cost tracking
â”‚   â””â”€â”€ providers/           # Provider implementations
â”‚       â”œâ”€â”€ ollama/          # Ollama provider
â”‚       â””â”€â”€ openai/          # OpenAI provider
```

## ğŸ”§ Configuration

### Environment Variables

```bash
# Ollama
export OLLAMA_BASE_URL=http://localhost:11434
export OLLAMA_MODEL=llama2

# OpenAI
export OPENAI_API_KEY=sk-...
export OPENAI_MODEL=gpt-4
export OPENAI_ORG_ID=org-...

# General
export BRUNO_LLM_LOG_LEVEL=INFO
export BRUNO_LLM_TIMEOUT=30.0
```

### Provider Configuration

```python
from bruno_llm.providers.ollama import OllamaProvider, OllamaConfig

# Using config object
config = OllamaConfig(
    base_url="http://localhost:11434",
    model="llama2",
    timeout=30.0
)
llm = OllamaProvider(config=config)

# Using parameters
llm = OllamaProvider(
    base_url="http://localhost:11434",
    model="llama2",
    timeout=30.0
)
```

## ğŸ“š Documentation

- **[Full Documentation](https://meggy-ai.github.io/bruno-llm/)**
- **[API Reference](https://meggy-ai.github.io/bruno-llm/api/)**
- **[Provider Guides](https://meggy-ai.github.io/bruno-llm/providers/)**
- **[Examples](./examples/)**

## ğŸ§ª Development

### Setup Development Environment

```bash
# Clone repository
git clone https://github.com/meggy-ai/bruno-llm.git
cd bruno-llm

# Create virtual environment
python -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate

# Install development dependencies
pip install -e ".[dev]"

# Install pre-commit hooks
pre-commit install
```

### Running Tests

```bash
# Run all tests
pytest

# Run with coverage
pytest --cov=bruno_llm --cov-report=html

# Run specific provider tests
pytest tests/providers/test_ollama.py -v

# Run integration tests (requires running services)
pytest -m integration
```

### Code Quality

```bash
# Format code
black bruno_llm tests

# Lint
ruff check bruno_llm tests

# Type check
mypy bruno_llm

# Run all checks
pre-commit run --all-files
```

## ğŸ¤ Contributing

Contributions are welcome! Please see [CONTRIBUTING.md](CONTRIBUTING.md) for guidelines.

### Commit Conventions

Follow [Conventional Commits](https://www.conventionalcommits.org/):
- `feat:` New features
- `fix:` Bug fixes
- `docs:` Documentation changes
- `test:` Test changes
- `refactor:` Code restructuring

## ğŸ“„ License

MIT License - see [LICENSE](LICENSE) file for details.

## ğŸ”— Related Projects

- **[bruno-core](https://github.com/meggy-ai/bruno-core)** - Foundation framework
- **bruno-memory** - Memory backend implementations (coming soon)
- **bruno-abilities** - Pre-built abilities (coming soon)
- **bruno-pa** - Personal assistant application (coming soon)

## ğŸ“ Support

- **Issues**: [GitHub Issues](https://github.com/meggy-ai/bruno-llm/issues)
- **Discussions**: [GitHub Discussions](https://github.com/meggy-ai/bruno-llm/discussions)
- **Documentation**: [https://meggy-ai.github.io/bruno-llm/](https://meggy-ai.github.io/bruno-llm/)

---

Made with â¤ï¸ by the Meggy AI team
