{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"bruno-llm","text":"<p>Production-ready LLM provider implementations for the bruno-core framework.</p>"},{"location":"#features","title":"Features","text":"<p>\u2728 Multiple Providers - Ollama (local models) - OpenAI (GPT models) - More coming soon (Claude, Gemini, Azure)</p> <p>\ud83d\ude80 Advanced Features - Response caching (100% test coverage) - Context window management (96% coverage) - Stream aggregation (93% coverage) - Cost tracking (98% coverage) - Middleware system (93% coverage)</p> <p>\ud83d\udee0\ufe0f Production Ready - 203 comprehensive tests - 91% code coverage - Type hints throughout - Async-first design</p>"},{"location":"#quick-start","title":"Quick Start","text":""},{"location":"#installation","title":"Installation","text":"<pre><code>pip install bruno-llm\n</code></pre>"},{"location":"#basic-usage","title":"Basic Usage","text":"<pre><code>from bruno_core.models import Message, MessageRole\nfrom bruno_llm import LLMFactory\n\n# Create provider\nllm = LLMFactory.create(\"ollama\", {\"model\": \"llama2\"})\n\n# Generate response\nmessages = [Message(role=MessageRole.USER, content=\"Hello!\")]\nresponse = await llm.generate(messages)\nprint(response)\n</code></pre>"},{"location":"#documentation-sections","title":"Documentation Sections","text":"<ul> <li>Getting Started - Installation and quick start guide</li> <li>User Guide - Comprehensive usage documentation</li> <li>API Reference - Complete API documentation</li> <li>Development - Contributing guidelines</li> </ul>"},{"location":"#links","title":"Links","text":"<ul> <li>GitHub: meggy-ai/bruno-llm</li> <li>Issues: Report a bug</li> <li>Changelog: View releases</li> </ul>"},{"location":"#license","title":"License","text":"<p>MIT License - see LICENSE for details.</p>"},{"location":"PRE_COMMIT_SETUP/","title":"Pre-commit Hooks Setup","text":"<p>Pre-commit hooks automatically check your code before each commit to catch issues that would fail CI.</p>"},{"location":"PRE_COMMIT_SETUP/#why-pre-commit-hooks","title":"Why Pre-commit Hooks?","text":"<p>Without pre-commit hooks: - \u274c Linting errors discovered in CI (after push) - \u274c Formatting issues found during code review - \u274c Type errors caught late in the process - \u274c Wasted CI minutes on preventable failures</p> <p>With pre-commit hooks: - \u2705 Errors caught before commit - \u2705 Code automatically formatted - \u2705 Fast feedback loop (seconds, not minutes) - \u2705 CI always passes</p>"},{"location":"PRE_COMMIT_SETUP/#installation","title":"Installation","text":""},{"location":"PRE_COMMIT_SETUP/#one-time-setup","title":"One-time Setup","text":"<pre><code># 1. Install pre-commit (if not already installed)\npip install pre-commit\n\n# 2. Install the git hooks\npre-commit install\n\n# 3. (Optional) Run on all files to check current state\npre-commit run --all-files\n</code></pre>"},{"location":"PRE_COMMIT_SETUP/#what-gets-checked","title":"What Gets Checked","text":"<p>On every <code>git commit</code>, these hooks run automatically:</p> <ol> <li>Basic Checks</li> <li>Remove trailing whitespace</li> <li>Fix end-of-file issues</li> <li>Validate YAML/JSON/TOML syntax</li> <li>Check for large files</li> <li>Detect merge conflicts</li> <li> <p>Detect private keys</p> </li> <li> <p>Code Quality</p> </li> <li>Ruff format - Auto-format code</li> <li>Ruff lint - Check code quality (auto-fix when possible)</li> <li>MyPy - Type checking (relaxed settings)</li> </ol>"},{"location":"PRE_COMMIT_SETUP/#usage","title":"Usage","text":""},{"location":"PRE_COMMIT_SETUP/#normal-workflow","title":"Normal Workflow","text":"<pre><code># Make changes\nvim bruno_llm/some_file.py\n\n# Stage changes\ngit add bruno_llm/some_file.py\n\n# Commit (hooks run automatically)\ngit commit -m \"Your message\"\n</code></pre> <p>If hooks fail: - Code is automatically fixed (formatting, some lint issues) - You'll see what failed - Stage the auto-fixes and commit again:</p> <pre><code>git add -u\ngit commit -m \"Your message\"\n</code></pre>"},{"location":"PRE_COMMIT_SETUP/#bypass-hooks-emergency-only","title":"Bypass Hooks (Emergency Only)","text":"<pre><code># Skip hooks (not recommended!)\ngit commit --no-verify -m \"Emergency fix\"\n</code></pre>"},{"location":"PRE_COMMIT_SETUP/#run-manually","title":"Run Manually","text":"<pre><code># Run all hooks on staged files\npre-commit run\n\n# Run all hooks on all files\npre-commit run --all-files\n\n# Run specific hook\npre-commit run ruff --all-files\npre-commit run mypy --all-files\n</code></pre>"},{"location":"PRE_COMMIT_SETUP/#update-hooks","title":"Update Hooks","text":"<pre><code># Update to latest versions\npre-commit autoupdate\n\n# Re-install after changes\npre-commit install\n</code></pre>"},{"location":"PRE_COMMIT_SETUP/#configuration","title":"Configuration","text":"<p>Hooks are configured in <code>.pre-commit-config.yaml</code>:</p> <pre><code>repos:\n  - repo: https://github.com/astral-sh/ruff-pre-commit\n    rev: v0.1.0\n    hooks:\n      - id: ruff\n        args: [--fix]           # Auto-fix issues\n      - id: ruff-format          # Format code\n\n  - repo: https://github.com/pre-commit/mirrors-mypy\n    rev: v1.5.1\n    hooks:\n      - id: mypy\n        args: [--ignore-missing-imports]  # Match CI settings\n        exclude: ^tests/                   # Skip test files\n</code></pre>"},{"location":"PRE_COMMIT_SETUP/#troubleshooting","title":"Troubleshooting","text":""},{"location":"PRE_COMMIT_SETUP/#hooks-not-running","title":"Hooks Not Running","text":"<pre><code># Verify installation\npre-commit --version\n\n# Reinstall hooks\npre-commit uninstall\npre-commit install\n</code></pre>"},{"location":"PRE_COMMIT_SETUP/#hooks-too-slow","title":"Hooks Too Slow","text":"<pre><code># Skip specific hooks temporarily\nSKIP=mypy git commit -m \"Message\"\n\n# Or disable specific hooks in .pre-commit-config.yaml\n</code></pre>"},{"location":"PRE_COMMIT_SETUP/#false-positives","title":"False Positives","text":"<p>If a hook incorrectly flags an issue:</p> <ol> <li>Fix the issue (preferred)</li> <li>Add exception in config file</li> <li>Disable hook in <code>.pre-commit-config.yaml</code> (last resort)</li> </ol>"},{"location":"PRE_COMMIT_SETUP/#ci-alignment","title":"CI Alignment","text":"<p>Pre-commit hooks are configured to match CI workflows:</p> Check Pre-commit CI (.github/workflows) Formatting \u2705 ruff format \u2705 ruff format --check Linting \u2705 ruff check \u2705 ruff check Type checking \u2705 mypy (relaxed) \u2705 mypy (relaxed) Tests \u274c (too slow) \u2705 pytest"},{"location":"PRE_COMMIT_SETUP/#best-practices","title":"Best Practices","text":"<ol> <li>Always install hooks when cloning the repo</li> <li>Don't bypass hooks unless emergency</li> <li>Update regularly with <code>pre-commit autoupdate</code></li> <li>Fix issues rather than suppressing them</li> <li>Run manually on all files after config changes</li> </ol>"},{"location":"PRE_COMMIT_SETUP/#why-some-checks-are-skipped","title":"Why Some Checks Are Skipped","text":"<ul> <li>Tests - Too slow for pre-commit (run locally with <code>pytest</code>)</li> <li>Coverage - Needs full test run</li> <li>Integration tests - Require external services</li> </ul> <p>These run in CI instead, which is fine since pre-commit catches 90% of issues.</p>"},{"location":"PRE_COMMIT_SETUP/#summary","title":"Summary","text":"<pre><code># Setup once\npip install pre-commit\npre-commit install\n\n# Then forget about it - it just works!\ngit commit -m \"Feature: Add new provider\"\n# Hooks run automatically \u2728\n</code></pre> <p>Pre-commit hooks are your first line of defense against CI failures. Install them and save yourself time! \ud83d\ude80</p>"},{"location":"USER_GUIDE/","title":"bruno-llm User Guide","text":"<p>Complete guide to using bruno-llm for LLM provider integration.</p>"},{"location":"USER_GUIDE/#table-of-contents","title":"Table of Contents","text":"<ol> <li>Installation</li> <li>Quick Start</li> <li>Provider Setup</li> <li>Basic Usage</li> <li>Embedding Usage</li> <li>Advanced Features</li> <li>Best Practices</li> <li>Troubleshooting</li> </ol>"},{"location":"USER_GUIDE/#installation","title":"Installation","text":""},{"location":"USER_GUIDE/#basic-installation","title":"Basic Installation","text":"<pre><code>pip install bruno-llm\n</code></pre> <p>This includes: - bruno-core framework - Ollama provider support (LLM + embeddings) - All base utilities and factory patterns</p>"},{"location":"USER_GUIDE/#with-openai-support","title":"With OpenAI Support","text":"<pre><code>pip install bruno-llm[openai]\n</code></pre> <p>Additional packages: - <code>openai</code> - Official OpenAI Python client - <code>tiktoken</code> - Accurate token counting for GPT models - Full embedding support for OpenAI embedding models</p>"},{"location":"USER_GUIDE/#development-installation","title":"Development Installation","text":"<pre><code>git clone https://github.com/meggy-ai/bruno-llm.git\ncd bruno-llm\npip install -e \".[dev]\"\n</code></pre> <p>Includes testing and development tools.</p>"},{"location":"USER_GUIDE/#quick-start","title":"Quick Start","text":""},{"location":"USER_GUIDE/#hello-world-example","title":"Hello World Example","text":"<pre><code>import asyncio\nfrom bruno_llm import LLMFactory\nfrom bruno_core.models import Message, MessageRole\n\nasync def main():\n    # Create provider\n    llm = LLMFactory.create(\"ollama\", {\"model\": \"llama2\"})\n\n    # Create message\n    messages = [\n        Message(role=MessageRole.USER, content=\"Hello! Who are you?\")\n    ]\n\n    # Generate response\n    response = await llm.generate(messages)\n    print(response)\n\n    # Clean up\n    await llm.close()\n\nasyncio.run(main())\n</code></pre>"},{"location":"USER_GUIDE/#streaming-example","title":"Streaming Example","text":"<pre><code>async def streaming_demo():\n    llm = LLMFactory.create(\"ollama\", {\"model\": \"llama2\"})\n\n    messages = [\n        Message(role=MessageRole.USER, content=\"Count from 1 to 10\")\n    ]\n\n    print(\"Response: \", end=\"\", flush=True)\n    async for chunk in llm.stream(messages):\n        print(chunk, end=\"\", flush=True)\n    print()\n\n    await llm.close()\n\nasyncio.run(streaming_demo())\n</code></pre>"},{"location":"USER_GUIDE/#provider-setup","title":"Provider Setup","text":""},{"location":"USER_GUIDE/#ollama","title":"Ollama","text":"<p>Installation:</p> <p>Visit ollama.ai and follow installation instructions.</p> <p>Starting Ollama:</p> <pre><code>ollama serve\n</code></pre> <p>Pulling Models:</p> <pre><code># General purpose\nollama pull llama2\nollama pull mistral\n\n# Code generation\nollama pull codellama\n\n# Larger models\nollama pull llama2:70b\n</code></pre> <p>Usage:</p> <pre><code>from bruno_llm import LLMFactory\n\nllm = LLMFactory.create(\"ollama\", {\n    \"base_url\": \"http://localhost:11434\",\n    \"model\": \"llama2\",\n    \"timeout\": 60.0\n})\n</code></pre> <p>Environment Variables:</p> <pre><code>export OLLAMA_BASE_URL=http://localhost:11434\nexport OLLAMA_MODEL=llama2\n</code></pre> <p>Then:</p> <pre><code>llm = LLMFactory.create_from_env(\"ollama\")\n</code></pre>"},{"location":"USER_GUIDE/#openai","title":"OpenAI","text":"<p>Get API Key:</p> <ol> <li>Sign up at platform.openai.com</li> <li>Navigate to API Keys</li> <li>Create new key</li> </ol> <p>Usage:</p> <pre><code>from bruno_llm import LLMFactory\n\nllm = LLMFactory.create(\"openai\", {\n    \"api_key\": \"sk-...\",\n    \"model\": \"gpt-4\",\n    \"organization\": \"org-...\"  # Optional\n})\n</code></pre> <p>Environment Variables:</p> <pre><code>export OPENAI_API_KEY=sk-...\nexport OPENAI_MODEL=gpt-4\nexport OPENAI_ORG_ID=org-...  # Optional\n</code></pre> <p>Then:</p> <pre><code>llm = LLMFactory.create_from_env(\"openai\")\n</code></pre> <p>Available Models:</p> <ul> <li><code>gpt-4</code> - Most capable</li> <li><code>gpt-4-turbo-preview</code> - Latest GPT-4 with larger context</li> <li><code>gpt-3.5-turbo</code> - Fast and cost-effective</li> </ul>"},{"location":"USER_GUIDE/#basic-usage","title":"Basic Usage","text":""},{"location":"USER_GUIDE/#creating-providers","title":"Creating Providers","text":"<p>Method 1: Factory Pattern (Recommended)</p> <pre><code>from bruno_llm import LLMFactory\n\nllm = LLMFactory.create(\n    provider=\"ollama\",\n    config={\"model\": \"llama2\"}\n)\n</code></pre> <p>Method 2: Direct Instantiation</p> <pre><code>from bruno_llm.providers.ollama import OllamaProvider\n\nllm = OllamaProvider(model=\"llama2\")\n</code></pre> <p>Method 3: From Environment</p> <pre><code>llm = LLMFactory.create_from_env(\"openai\")\n</code></pre>"},{"location":"USER_GUIDE/#message-format","title":"Message Format","text":"<p>bruno-llm uses <code>Message</code> objects from bruno-core:</p> <pre><code>from bruno_core.models import Message, MessageRole\n\n# User message\nuser_msg = Message(\n    role=MessageRole.USER,\n    content=\"What is Python?\"\n)\n\n# System message (sets context/behavior)\nsystem_msg = Message(\n    role=MessageRole.SYSTEM,\n    content=\"You are a helpful programming tutor.\"\n)\n\n# Assistant message (previous AI responses)\nassistant_msg = Message(\n    role=MessageRole.ASSISTANT,\n    content=\"Python is a programming language...\"\n)\n\n# Conversation\nmessages = [system_msg, user_msg]\n</code></pre>"},{"location":"USER_GUIDE/#generating-responses","title":"Generating Responses","text":"<p>Basic Generation:</p> <pre><code>response = await llm.generate(messages)\nprint(response)  # String response\n</code></pre> <p>With Parameters:</p> <pre><code>response = await llm.generate(\n    messages=messages,\n    max_tokens=500,           # Limit response length\n    temperature=0.7,          # Creativity (0.0 = deterministic, 2.0 = very creative)\n    top_p=0.9,               # Nucleus sampling\n    stop=[\"###\", \"END\"]      # Stop sequences\n)\n</code></pre> <p>Streaming Responses:</p> <pre><code>async for chunk in llm.stream(messages, max_tokens=200):\n    print(chunk, end=\"\", flush=True)\n</code></pre>"},{"location":"USER_GUIDE/#provider-methods","title":"Provider Methods","text":"<p>Check Connection:</p> <pre><code>if await llm.check_connection():\n    print(\"Provider is accessible\")\nelse:\n    print(\"Cannot connect to provider\")\n</code></pre> <p>List Available Models:</p> <pre><code>models = await llm.list_models()\nfor model in models:\n    print(f\"- {model}\")\n</code></pre> <p>Get Model Information:</p> <pre><code>info = llm.get_model_info()\nprint(f\"Provider: {info['provider']}\")\nprint(f\"Model: {info['model']}\")\nprint(f\"Context Window: {info.get('max_context_tokens', 'Unknown')}\")\n</code></pre> <p>Token Counting:</p> <pre><code>text = \"Hello, world!\"\ntokens = llm.get_token_count(text)\nprint(f\"Token count: {tokens}\")\n</code></pre> <p>System Prompts:</p> <pre><code># Set system prompt\nllm.set_system_prompt(\"You are a helpful assistant.\")\n\n# Get current system prompt\nprompt = llm.get_system_prompt()\n\n# System prompt is automatically added to messages\nmessages = [Message(role=MessageRole.USER, content=\"Hello\")]\nresponse = await llm.generate(messages)  # System prompt included\n</code></pre>"},{"location":"USER_GUIDE/#embedding-usage","title":"Embedding Usage","text":"<p>Bruno-LLM provides powerful embedding capabilities through multiple providers. Embeddings convert text into numerical vectors that capture semantic meaning, enabling similarity search, clustering, and RAG applications.</p>"},{"location":"USER_GUIDE/#quick-start-with-embeddings","title":"Quick Start with Embeddings","text":"<p>Basic Text Embedding:</p> <pre><code>from bruno_llm.embedding_factory import EmbeddingFactory\n\n# Create embedding provider\nembedder = EmbeddingFactory.create(\"openai\", {\n    \"api_key\": \"sk-...\",\n    \"model\": \"text-embedding-3-small\"\n})\n\n# Generate embedding for single text\ntext = \"Machine learning transforms data into insights\"\nembedding = await embedder.embed_text(text)\nprint(f\"Embedding dimension: {len(embedding)}\")\n\n# Batch processing\ntexts = [\n    \"Artificial intelligence revolutionizes technology\",\n    \"Machine learning enables pattern recognition\",\n    \"Deep learning uses neural networks\"\n]\nembeddings = await embedder.embed_texts(texts)\nprint(f\"Generated {len(embeddings)} embeddings\")\n</code></pre>"},{"location":"USER_GUIDE/#available-embedding-providers","title":"Available Embedding Providers","text":"<p>OpenAI Embeddings (Cloud-based):</p> <pre><code>from bruno_llm.providers.openai import OpenAIEmbeddingProvider\n\n# High-quality cloud embeddings\nembedder = OpenAIEmbeddingProvider(\n    api_key=\"sk-...\",\n    model=\"text-embedding-3-small\"  # or text-embedding-3-large\n)\n</code></pre> <p>Ollama Embeddings (Local):</p> <pre><code>from bruno_llm.providers.ollama import OllamaEmbeddingProvider\n\n# Privacy-focused local embeddings\nembedder = OllamaEmbeddingProvider(\n    base_url=\"http://localhost:11434\",\n    model=\"nomic-embed-text\"  # Make sure model is pulled: ollama pull nomic-embed-text\n)\n</code></pre>"},{"location":"USER_GUIDE/#embedding-factory-patterns","title":"Embedding Factory Patterns","text":"<p>From Environment Variables:</p> <pre><code># Set up environment\nexport OPENAI_API_KEY=sk-...\nexport OPENAI_EMBEDDING_MODEL=text-embedding-3-small\n\n# Or for Ollama\nexport OLLAMA_BASE_URL=http://localhost:11434\nexport OLLAMA_EMBEDDING_MODEL=nomic-embed-text\n</code></pre> <pre><code># Auto-configure from environment\nembedder = EmbeddingFactory.create_from_env(\"openai\")\n# Or\nembedder = EmbeddingFactory.create_from_env(\"ollama\")\n</code></pre> <p>With Fallback Providers:</p> <pre><code># Try OpenAI first, fallback to Ollama\nembedder = EmbeddingFactory.create_with_fallback(\n    providers=[\"openai\", \"ollama\"],\n    configs=[\n        {\"api_key\": \"sk-...\", \"model\": \"text-embedding-3-small\"},\n        {\"base_url\": \"http://localhost:11434\", \"model\": \"nomic-embed-text\"}\n    ]\n)\n</code></pre>"},{"location":"USER_GUIDE/#similarity-search","title":"Similarity Search","text":"<pre><code># Calculate similarity between embeddings\nembedding1 = await embedder.embed_text(\"Python programming\")\nembedding2 = await embedder.embed_text(\"Software development\")\n\nsimilarity = embedder.calculate_similarity(embedding1, embedding2)\nprint(f\"Similarity: {similarity:.3f}\")  # Higher values = more similar\n\n# Find most similar texts\nquery = \"machine learning algorithms\"\ndocuments = [\n    \"Neural networks for classification\",\n    \"Cooking recipes and techniques\",\n    \"Supervised learning methods\",\n    \"Travel destination guides\"\n]\n\nquery_embedding = await embedder.embed_text(query)\ndoc_embeddings = await embedder.embed_texts(documents)\n\n# Calculate similarities\nsimilarities = []\nfor i, doc_embedding in enumerate(doc_embeddings):\n    similarity = embedder.calculate_similarity(query_embedding, doc_embedding)\n    similarities.append((documents[i], similarity))\n\n# Sort by similarity\nsimilarities.sort(key=lambda x: x[1], reverse=True)\n\nprint(\"Most similar documents:\")\nfor doc, score in similarities[:3]:\n    print(f\"  {score:.3f}: {doc}\")\n</code></pre>"},{"location":"USER_GUIDE/#simple-rag-retrieval-augmented-generation","title":"Simple RAG (Retrieval-Augmented Generation)","text":"<p>Combine embeddings with LLMs for knowledge-based generation:</p> <pre><code>from bruno_llm.factory import LLMFactory\nfrom bruno_llm.embedding_factory import EmbeddingFactory\nfrom bruno_core.models import Message, MessageRole\n\nclass SimpleRAG:\n    def __init__(self):\n        self.llm = LLMFactory.create_from_env(\"openai\")\n        self.embedder = EmbeddingFactory.create_from_env(\"openai\")\n        self.knowledge = []  # (text, embedding) pairs\n\n    async def add_knowledge(self, texts: list):\n        \"\"\"Add documents to knowledge base.\"\"\"\n        embeddings = await self.embedder.embed_texts(texts)\n        for text, embedding in zip(texts, embeddings):\n            self.knowledge.append((text, embedding))\n\n    async def search_knowledge(self, query: str, top_k: int = 3):\n        \"\"\"Find relevant documents.\"\"\"\n        query_embedding = await self.embedder.embed_text(query)\n\n        similarities = []\n        for text, doc_embedding in self.knowledge:\n            similarity = self.embedder.calculate_similarity(query_embedding, doc_embedding)\n            similarities.append((text, similarity))\n\n        similarities.sort(key=lambda x: x[1], reverse=True)\n        return [text for text, _ in similarities[:top_k]]\n\n    async def answer_question(self, question: str):\n        \"\"\"Answer using relevant knowledge.\"\"\"\n        context_docs = await self.search_knowledge(question)\n        context = \"\\n\\n\".join(context_docs)\n\n        messages = [\n            Message(role=MessageRole.SYSTEM, content=\n                \"Answer based on the provided context. If insufficient information, say so.\"),\n            Message(role=MessageRole.USER, content=f\"Context:\\n{context}\\n\\nQuestion: {question}\")\n        ]\n\n        return await self.llm.generate(messages)\n\n# Usage\nrag = SimpleRAG()\n\n# Add knowledge\nknowledge = [\n    \"Bruno-LLM provides unified LLM provider interfaces\",\n    \"It supports OpenAI, Ollama, and other providers\",\n    \"The factory pattern enables easy provider switching\",\n    \"Embedding providers enable semantic search capabilities\"\n]\nawait rag.add_knowledge(knowledge)\n\n# Ask questions\nanswer = await rag.answer_question(\"What does Bruno-LLM provide?\")\nprint(answer)\n</code></pre>"},{"location":"USER_GUIDE/#best-practices-for-embeddings","title":"Best Practices for Embeddings","text":"<p>1. Choose the Right Provider:</p> <pre><code>def select_embedding_provider(use_case: str):\n    \"\"\"Select optimal embedding provider.\"\"\"\n    if use_case == \"privacy_critical\":\n        return \"ollama\"  # Local, private\n    elif use_case == \"cost_sensitive\":\n        return \"openai\", \"text-embedding-3-small\"  # Cheapest OpenAI\n    elif use_case == \"high_accuracy\":\n        return \"openai\", \"text-embedding-3-large\"  # Best performance\n    else:\n        return \"openai\", \"text-embedding-ada-002\"  # Balanced\n</code></pre> <p>2. Batch Processing for Efficiency:</p> <pre><code># Process in batches instead of one-by-one\ntexts = [\"text1\", \"text2\", ...]  # Large list\n\nbatch_size = 100\nall_embeddings = []\n\nfor i in range(0, len(texts), batch_size):\n    batch = texts[i:i + batch_size]\n    batch_embeddings = await embedder.embed_texts(batch)\n    all_embeddings.extend(batch_embeddings)\n</code></pre> <p>3. Error Handling:</p> <pre><code>from bruno_llm.exceptions import LLMError\n\nasync def robust_embedding(embedder, text: str):\n    \"\"\"Generate embedding with error handling.\"\"\"\n    try:\n        return await embedder.embed_text(text)\n    except LLMError as e:\n        print(f\"Embedding failed: {e}\")\n        return None  # or default embedding\n</code></pre> <p>For more advanced embedding patterns and integrations, see the Embedding Guide.</p>"},{"location":"USER_GUIDE/#advanced-features","title":"Advanced Features","text":""},{"location":"USER_GUIDE/#response-caching","title":"Response Caching","text":"<p>Reduce API costs and latency by caching responses:</p> <pre><code>from bruno_llm import LLMFactory\nfrom bruno_llm.base import ResponseCache\n\nllm = LLMFactory.create(\"ollama\", {\"model\": \"llama2\"})\ncache = ResponseCache(\n    max_size=100,  # Maximum number of cached responses\n    ttl=300        # Time-to-live in seconds (5 minutes)\n)\n\nmessages = [Message(role=MessageRole.USER, content=\"What is 2+2?\")]\n\n# First request - cache miss\nresponse = await llm.generate(messages, temperature=0.0)\ncache.set(messages, response, temperature=0.0)\n\n# Second request - cache hit\ncached_response = cache.get(messages, temperature=0.0)\nif cached_response:\n    print(\"From cache!\")\n    response = cached_response\nelse:\n    response = await llm.generate(messages, temperature=0.0)\n    cache.set(messages, response, temperature=0.0)\n\n# Cache statistics\nstats = cache.get_stats()\nprint(f\"Hit rate: {stats['hit_rate']:.1%}\")\nprint(f\"Cache size: {stats['size']}/{stats['max_size']}\")\n</code></pre> <p>Cache Management:</p> <pre><code># Clear entire cache\ncache.clear()\n\n# Invalidate specific entry\ncache.invalidate(messages, temperature=0.0)\n\n# Clean up expired entries\nexpired_count = cache.cleanup_expired()\n\n# Get top entries by access count\ntop_entries = cache.get_top_entries(n=10)\n</code></pre>"},{"location":"USER_GUIDE/#context-window-management","title":"Context Window Management","text":"<p>Intelligently truncate conversations that exceed token limits:</p> <pre><code>from bruno_llm.base import (\n    ContextWindowManager,\n    ContextLimits,\n    TruncationStrategy\n)\n\n# Create context manager\ncontext_mgr = ContextWindowManager(\n    model=\"gpt-4\",  # Uses predefined limits for known models\n    limits=ContextLimits(\n        max_tokens=8000,\n        max_output_tokens=500  # Reserve tokens for response\n    ),\n    strategy=TruncationStrategy.SMART  # Preserve important messages\n)\n\n# Check if messages fit\nif context_mgr.check_limit(messages):\n    print(\"Within limit\")\nelse:\n    print(\"Exceeds limit, truncating...\")\n    messages = context_mgr.truncate(messages)\n\n# Get statistics\nstats = context_mgr.get_stats(messages)\nprint(f\"Input tokens: {stats['input_tokens']}\")\nprint(f\"Available output tokens: {stats['available_output_tokens']}\")\nprint(f\"Usage: {stats['usage_percent']:.1f}%\")\n</code></pre> <p>Truncation Strategies:</p> <ul> <li><code>OLDEST_FIRST</code>: Remove oldest messages first (keeps recent context)</li> <li><code>MIDDLE_OUT</code>: Remove middle messages (keeps beginning and end)</li> <li><code>SLIDING_WINDOW</code>: Keep most recent N messages</li> <li><code>SMART</code>: Preserve system messages and recent important messages</li> </ul> <p>Custom Limits:</p> <pre><code># For models without predefined limits\ncontext_mgr = ContextWindowManager(\n    model=\"custom-model\",\n    limits=ContextLimits(max_tokens=4096, max_output_tokens=256)\n)\n</code></pre>"},{"location":"USER_GUIDE/#stream-aggregation","title":"Stream Aggregation","text":"<p>Control how streaming chunks are batched:</p> <pre><code>from bruno_llm.base import StreamAggregator\n\n# Word-by-word aggregation\naggregator = StreamAggregator(strategy=\"word\")\nasync for word in aggregator.aggregate(llm.stream(messages)):\n    print(f\"[{word.strip()}]\", end=\" \")\n\n# Sentence-by-sentence\naggregator = StreamAggregator(strategy=\"sentence\")\nasync for sentence in aggregator.aggregate(llm.stream(messages)):\n    print(f\"\\nSentence: {sentence.strip()}\")\n\n# Fixed size chunks\naggregator = StreamAggregator(strategy=\"fixed\", chunk_size=10)\nasync for chunk in aggregator.aggregate(llm.stream(messages)):\n    print(chunk, end=\"\")\n\n# Time-based batching (wait up to N seconds)\naggregator = StreamAggregator(strategy=\"time\", interval=0.5)\nasync for batch in aggregator.aggregate(llm.stream(messages)):\n    print(f\"Batch: {batch}\")\n\n# Passthrough (no aggregation)\naggregator = StreamAggregator(strategy=\"passthrough\")\nasync for chunk in aggregator.aggregate(llm.stream(messages)):\n    print(chunk, end=\"\")\n</code></pre>"},{"location":"USER_GUIDE/#cost-tracking","title":"Cost Tracking","text":"<p>Monitor API usage and costs:</p> <pre><code>from bruno_llm import LLMFactory\n\nllm = LLMFactory.create_from_env(\"openai\")\n\n# Make some requests\nawait llm.generate(messages)\nawait llm.generate(messages)\n\n# Get usage report\nreport = llm.cost_tracker.get_usage_report()\nprint(f\"Total requests: {report['total_requests']}\")\nprint(f\"Total tokens: {report['total_tokens']}\")\nprint(f\"Total cost: ${report['total_cost']:.4f}\")\n\n# Model breakdown\nfor model, stats in report['model_breakdown'].items():\n    print(f\"{model}:\")\n    print(f\"  Requests: {stats['requests']}\")\n    print(f\"  Tokens: {stats['input_tokens']} in + {stats['output_tokens']} out\")\n    print(f\"  Cost: ${stats['cost']:.4f}\")\n\n# Export to CSV\nllm.cost_tracker.export_to_csv(\"usage_report.csv\")\n\n# Export to JSON\nllm.cost_tracker.export_to_json(\"usage_report.json\")\n\n# Time range report\nfrom datetime import datetime, timedelta\n\nstart = datetime.now() - timedelta(days=7)\nend = datetime.now()\nweekly_report = llm.cost_tracker.get_time_range_report(start, end)\n\n# Budget checking\nstatus = llm.cost_tracker.check_budget(\n    budget_limit=10.0,\n    warning_threshold=0.8  # Warn at 80%\n)\n\nif status['warning']:\n    print(f\"\u26a0\ufe0f Warning: {status['percent_used']:.1f}% of budget used\")\n\nif not status['within_budget']:\n    print(f\"\u274c Budget exceeded! ${status['total_spent']:.2f} / ${status['budget_limit']:.2f}\")\n</code></pre>"},{"location":"USER_GUIDE/#provider-fallback","title":"Provider Fallback","text":"<p>Try multiple providers in order:</p> <pre><code>from bruno_llm import LLMFactory\n\n# Try OpenAI, fallback to Ollama\nllm = await LLMFactory.create_with_fallback(\n    providers=[\"openai\", \"ollama\"],\n    configs=[\n        {\"api_key\": \"sk-...\", \"model\": \"gpt-4\"},\n        {\"model\": \"llama2\"}\n    ]\n)\n\n# The first successfully connected provider is used\ninfo = llm.get_model_info()\nprint(f\"Using provider: {info['provider']}\")\n\nresponse = await llm.generate(messages)\n</code></pre>"},{"location":"USER_GUIDE/#concurrent-requests","title":"Concurrent Requests","text":"<p>Handle multiple requests in parallel:</p> <pre><code>import asyncio\n\nasync def process_multiple():\n    llm = LLMFactory.create(\"ollama\", {\"model\": \"llama2\"})\n\n    # Create multiple message sets\n    message_sets = [\n        [Message(role=MessageRole.USER, content=f\"Tell me about topic {i}\")]\n        for i in range(5)\n    ]\n\n    # Process concurrently\n    tasks = [llm.generate(msgs) for msgs in message_sets]\n    responses = await asyncio.gather(*tasks, return_exceptions=True)\n\n    # Handle results\n    for i, response in enumerate(responses):\n        if isinstance(response, Exception):\n            print(f\"Request {i} failed: {response}\")\n        else:\n            print(f\"Request {i}: {response[:50]}...\")\n\n    await llm.close()\n\nasyncio.run(process_multiple())\n</code></pre>"},{"location":"USER_GUIDE/#best-practices","title":"Best Practices","text":""},{"location":"USER_GUIDE/#1-resource-management","title":"1. Resource Management","text":"<p>Always close providers:</p> <pre><code># Using context manager (recommended)\nasync def with_context_manager():\n    llm = LLMFactory.create(\"ollama\", {\"model\": \"llama2\"})\n    async with llm:\n        response = await llm.generate(messages)\n    # Automatically closed\n\n# Manual cleanup\ntry:\n    llm = LLMFactory.create(\"ollama\", {\"model\": \"llama2\"})\n    response = await llm.generate(messages)\nfinally:\n    await llm.close()\n</code></pre>"},{"location":"USER_GUIDE/#2-error-handling","title":"2. Error Handling","text":"<p>Handle provider-specific errors:</p> <pre><code>from bruno_llm.exceptions import (\n    ModelNotFoundError,\n    RateLimitError,\n    AuthenticationError,\n    ContextLengthExceededError\n)\n\ntry:\n    response = await llm.generate(messages)\nexcept ModelNotFoundError as e:\n    print(f\"Model not available: {e}\")\n    # Try alternative model\nexcept RateLimitError as e:\n    print(f\"Rate limited: {e}\")\n    # Wait and retry\n    await asyncio.sleep(60)\nexcept AuthenticationError as e:\n    print(f\"Authentication failed: {e}\")\n    # Check API key\nexcept ContextLengthExceededError as e:\n    print(f\"Context too long: {e}\")\n    # Truncate messages\n    messages = context_mgr.truncate(messages)\n    response = await llm.generate(messages)\nexcept Exception as e:\n    print(f\"Unexpected error: {e}\")\n</code></pre>"},{"location":"USER_GUIDE/#3-configuration-management","title":"3. Configuration Management","text":"<p>Use environment variables for sensitive data:</p> <pre><code>import os\nfrom dotenv import load_dotenv\n\n# Load from .env file\nload_dotenv()\n\n# Use environment variables\nllm = LLMFactory.create(\"openai\", {\n    \"api_key\": os.getenv(\"OPENAI_API_KEY\"),\n    \"model\": os.getenv(\"OPENAI_MODEL\", \"gpt-4\")\n})\n</code></pre> <p>.env file: <pre><code>OPENAI_API_KEY=sk-...\nOPENAI_MODEL=gpt-4\nOLLAMA_BASE_URL=http://localhost:11434\n</code></pre></p>"},{"location":"USER_GUIDE/#4-temperature-settings","title":"4. Temperature Settings","text":"<p>Choose appropriate temperature based on use case:</p> <pre><code># Deterministic (temperature=0.0) - for factual, consistent responses\nresponse = await llm.generate(messages, temperature=0.0)\n\n# Balanced (temperature=0.7) - good default\nresponse = await llm.generate(messages, temperature=0.7)\n\n# Creative (temperature=1.5) - for brainstorming, creative writing\nresponse = await llm.generate(messages, temperature=1.5)\n</code></pre>"},{"location":"USER_GUIDE/#5-token-management","title":"5. Token Management","text":"<p>Monitor token usage:</p> <pre><code># Check message token count before sending\ntotal_tokens = sum(llm.get_token_count(msg.content) for msg in messages)\nprint(f\"Request will use approximately {total_tokens} tokens\")\n\n# Limit response length\nresponse = await llm.generate(messages, max_tokens=500)\n</code></pre>"},{"location":"USER_GUIDE/#6-caching-strategy","title":"6. Caching Strategy","text":"<p>Use caching for repeated queries:</p> <pre><code># Cache deterministic responses (temperature=0)\ncache = ResponseCache(max_size=1000, ttl=3600)  # 1 hour TTL\n\n# Check cache first\ncached = cache.get(messages, temperature=0.0)\nif cached:\n    response = cached\nelse:\n    response = await llm.generate(messages, temperature=0.0)\n    cache.set(messages, response, temperature=0.0)\n</code></pre> <p>Don't cache: - Creative/random responses (temperature &gt; 0) - Time-sensitive information - User-specific data</p>"},{"location":"USER_GUIDE/#troubleshooting","title":"Troubleshooting","text":""},{"location":"USER_GUIDE/#ollama-issues","title":"Ollama Issues","text":"<p>\"Cannot connect to Ollama\"</p> <pre><code># Check if Ollama is running\nimport httpx\n\ntry:\n    response = httpx.get(\"http://localhost:11434/api/tags\", timeout=5.0)\n    print(f\"Ollama is running. Models: {response.json()}\")\nexcept Exception as e:\n    print(f\"Ollama not accessible: {e}\")\n    print(\"Start Ollama with: ollama serve\")\n</code></pre> <p>\"Model not found\"</p> <pre><code># List installed models\nollama list\n\n# Pull missing model\nollama pull llama2\n</code></pre> <p>Slow responses</p> <ul> <li>Use smaller models (llama2:7b instead of llama2:70b)</li> <li>Reduce max_tokens</li> <li>Use GPU if available</li> </ul>"},{"location":"USER_GUIDE/#openai-issues","title":"OpenAI Issues","text":"<p>\"Authentication failed\"</p> <pre><code># Verify API key\nimport openai\n\ntry:\n    openai.api_key = \"sk-...\"\n    models = openai.Model.list()\n    print(\"API key is valid\")\nexcept openai.error.AuthenticationError:\n    print(\"Invalid API key\")\n</code></pre> <p>\"Rate limit exceeded\"</p> <pre><code>from bruno_llm.base import RateLimiter\n\n# Add rate limiting\nlimiter = RateLimiter(requests_per_minute=50)\n\nasync def rate_limited_request():\n    async with limiter:\n        return await llm.generate(messages)\n</code></pre> <p>High costs</p> <ul> <li>Use gpt-3.5-turbo instead of gpt-4</li> <li>Reduce max_tokens</li> <li>Implement caching</li> <li>Monitor with cost_tracker</li> </ul>"},{"location":"USER_GUIDE/#embedding-issues","title":"Embedding Issues","text":"<p>\"Embedding model not found\" (Ollama)</p> <pre><code># Check available models\nollama list\n\n# Pull embedding model\nollama pull nomic-embed-text\nollama pull mxbai-embed-large\n</code></pre> <p>\"Invalid dimensions\" or similarity errors</p> <pre><code># Ensure embeddings are from the same model\nembedder = EmbeddingFactory.create(\"openai\", {\n    \"model\": \"text-embedding-3-small\"  # Consistent model\n})\n\n# Check embedding dimensions\nembedding = await embedder.embed_text(\"test\")\nprint(f\"Dimension: {len(embedding)}\")\n</code></pre> <p>High embedding costs (OpenAI)</p> <pre><code># Use smaller, cheaper model\nembedder = EmbeddingFactory.create(\"openai\", {\n    \"model\": \"text-embedding-3-small\"  # Cheaper than ada-002\n})\n\n# Or switch to local Ollama\nembedder = EmbeddingFactory.create_from_env(\"ollama\")\n</code></pre> <p>Slow embedding generation</p> <pre><code># Process in batches for efficiency\nbatch_size = 100\ntexts = [\"text1\", \"text2\", ...]  # Your texts\n\nembeddings = []\nfor i in range(0, len(texts), batch_size):\n    batch = texts[i:i + batch_size]\n    batch_embeddings = await embedder.embed_texts(batch)\n    embeddings.extend(batch_embeddings)\n</code></pre> <p>Memory issues with large text collections</p> <pre><code># Use generators for large datasets\nasync def process_large_collection(embedder, texts):\n    batch_size = 50  # Smaller batches\n\n    for i in range(0, len(texts), batch_size):\n        batch = texts[i:i + batch_size]\n        yield await embedder.embed_texts(batch)\n\n# Usage\nasync for batch_embeddings in process_large_collection(embedder, texts):\n    # Process each batch immediately\n    save_embeddings(batch_embeddings)\n</code></pre>"},{"location":"USER_GUIDE/#general-issues","title":"General Issues","text":"<p>Import errors</p> <pre><code># Reinstall package\npip uninstall bruno-llm\npip install bruno-llm\n\n# Or install from source\ngit clone https://github.com/meggy-ai/bruno-llm.git\ncd bruno-llm\npip install -e .\n</code></pre> <p>Timeout errors</p> <pre><code># Increase timeout\nllm = LLMFactory.create(\"ollama\", {\n    \"model\": \"llama2\",\n    \"timeout\": 120.0  # 2 minutes\n})\n</code></pre> <p>Memory issues</p> <ul> <li>Process in batches</li> <li>Clear cache periodically</li> <li>Use streaming for large responses</li> </ul>"},{"location":"USER_GUIDE/#next-steps","title":"Next Steps","text":"<ul> <li>Read examples/ for more complete examples</li> <li>Check API Reference for detailed method documentation</li> <li>See TESTING.md for testing guide</li> <li>Join our community for support</li> </ul> <p>Need help? Open an issue or email contact@meggy.ai</p>"},{"location":"about/changelog/","title":"Changelog","text":"<p>See CHANGELOG.md for complete version history.</p>"},{"location":"about/changelog/#latest-release","title":"Latest Release","text":""},{"location":"about/changelog/#v010-2025-12-09","title":"v0.1.0 (2025-12-09)","text":"<p>Initial release with Ollama and OpenAI providers, comprehensive test coverage, and production-ready features.</p>"},{"location":"about/license/","title":"License","text":"<p>bruno-llm is released under the MIT License.</p> <p>See LICENSE for full text.</p>"},{"location":"about/license/#mit-license-summary","title":"MIT License Summary","text":"<p>Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software.</p> <p>THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"api/API_REFERENCE/","title":"Bruno-LLM API Reference","text":""},{"location":"api/API_REFERENCE/#overview","title":"Overview","text":"<p>Bruno-LLM provides a unified interface for interacting with multiple LLM and embedding providers through a consistent API. This package implements the <code>LLMInterface</code> and <code>EmbeddingInterface</code> from bruno-core, enabling seamless integration with the broader Bruno AI ecosystem.</p>"},{"location":"api/API_REFERENCE/#core-interfaces","title":"Core Interfaces","text":""},{"location":"api/API_REFERENCE/#llminterface","title":"LLMInterface","text":"<p>All LLM providers implement the <code>bruno_core.interfaces.LLMInterface</code>:</p> <pre><code>from bruno_core.interfaces import LLLInterface\nfrom bruno_core.models import Message, MessageRole\n\nclass YourProvider(LLMInterface):\n    async def generate(self, messages: List[Message], temperature: Optional[float] = None, max_tokens: Optional[int] = None, **kwargs) -&gt; str:\n        \"\"\"Generate a complete response.\"\"\"\n\n    async def stream(self, messages: List[Message], temperature: Optional[float] = None, max_tokens: Optional[int] = None, **kwargs) -&gt; AsyncIterator[str]:\n        \"\"\"Stream response tokens.\"\"\"\n\n    async def check_connection(self) -&gt; bool:\n        \"\"\"Check if the provider is accessible.\"\"\"\n\n    async def list_models(self) -&gt; List[str]:\n        \"\"\"List available models.\"\"\"\n\n    def get_token_count(self, text: str) -&gt; int:\n        \"\"\"Estimate token count for text.\"\"\"\n\n    def get_model_info(self) -&gt; Dict[str, Any]:\n        \"\"\"Get current model information.\"\"\"\n\n    def set_system_prompt(self, prompt: str) -&gt; None:\n        \"\"\"Set system prompt for conversations.\"\"\"\n\n    def get_system_prompt(self) -&gt; Optional[str]:\n        \"\"\"Get current system prompt.\"\"\"\n</code></pre>"},{"location":"api/API_REFERENCE/#embeddinginterface","title":"EmbeddingInterface","text":"<p>All embedding providers implement the <code>bruno_core.interfaces.EmbeddingInterface</code>:</p> <pre><code>from bruno_core.interfaces import EmbeddingInterface\n\nclass YourEmbeddingProvider(EmbeddingInterface):\n    async def embed_text(self, text: str) -&gt; List[float]:\n        \"\"\"Generate embeddings for a single text.\"\"\"\n\n    async def embed_texts(self, texts: List[str]) -&gt; List[List[float]]:\n        \"\"\"Generate embeddings for multiple texts.\"\"\"\n\n    async def embed_message(self, message: Message) -&gt; List[float]:\n        \"\"\"Generate embeddings for a message object.\"\"\"\n\n    async def check_connection(self) -&gt; bool:\n        \"\"\"Check if the provider is accessible.\"\"\"\n\n    def get_dimension(self) -&gt; int:\n        \"\"\"Get the embedding dimension.\"\"\"\n\n    def get_model_name(self) -&gt; str:\n        \"\"\"Get the current model name.\"\"\"\n\n    def get_max_batch_size(self) -&gt; int:\n        \"\"\"Get maximum batch size for processing.\"\"\"\n\n    def supports_batch(self) -&gt; bool:\n        \"\"\"Check if provider supports batch processing.\"\"\"\n\n    def calculate_similarity(self, embedding1: List[float], embedding2: List[float]) -&gt; float:\n        \"\"\"Calculate cosine similarity between embeddings.\"\"\"\n</code></pre>"},{"location":"api/API_REFERENCE/#llm-providers","title":"LLM Providers","text":""},{"location":"api/API_REFERENCE/#openai-provider","title":"OpenAI Provider","text":""},{"location":"api/API_REFERENCE/#basic-usage","title":"Basic Usage","text":"<pre><code>from bruno_llm.providers.openai import OpenAIProvider\nfrom bruno_core.models import Message, MessageRole\n\n# Initialize provider\nprovider = OpenAIProvider(\n    api_key=\"sk-...\",\n    model=\"gpt-4\",\n    organization=\"org-...\",  # Optional\n    timeout=30.0\n)\n\n# Generate response\nmessages = [\n    Message(role=MessageRole.SYSTEM, content=\"You are a helpful assistant.\"),\n    Message(role=MessageRole.USER, content=\"Hello!\")\n]\n\nresponse = await provider.generate(\n    messages=messages,\n    temperature=0.7,\n    max_tokens=1000\n)\nprint(response)\n\n# Stream response\nasync for chunk in provider.stream(messages, temperature=0.7):\n    print(chunk, end=\"\", flush=True)\n</code></pre>"},{"location":"api/API_REFERENCE/#configuration","title":"Configuration","text":"<pre><code>from bruno_llm.providers.openai import OpenAIConfig\n\nconfig = OpenAIConfig(\n    api_key=\"sk-...\",\n    model=\"gpt-4\",\n    organization=\"org-...\",\n    base_url=\"https://api.openai.com/v1\",  # Custom endpoint\n    timeout=30.0,\n    max_retries=3,\n    batch_size=100,\n    track_cost=True\n)\n\nprovider = OpenAIProvider(config=config)\n</code></pre>"},{"location":"api/API_REFERENCE/#environment-variables","title":"Environment Variables","text":"<pre><code>export OPENAI_API_KEY=\"sk-...\"\nexport OPENAI_MODEL=\"gpt-4\"\nexport OPENAI_ORG_ID=\"org-...\"\nexport OPENAI_BASE_URL=\"https://api.openai.com/v1\"\nexport OPENAI_TIMEOUT=\"30.0\"\nexport OPENAI_MAX_RETRIES=\"3\"\n</code></pre>"},{"location":"api/API_REFERENCE/#supported-models","title":"Supported Models","text":"<ul> <li><code>gpt-4</code> - GPT-4 base model</li> <li><code>gpt-4-turbo</code> - GPT-4 Turbo model</li> <li><code>gpt-3.5-turbo</code> - GPT-3.5 Turbo model</li> <li><code>gpt-4o</code> - GPT-4 Omni model</li> <li><code>gpt-4o-mini</code> - GPT-4 Omni mini model</li> </ul>"},{"location":"api/API_REFERENCE/#ollama-provider","title":"Ollama Provider","text":""},{"location":"api/API_REFERENCE/#basic-usage_1","title":"Basic Usage","text":"<pre><code>from bruno_llm.providers.ollama import OllamaProvider\nfrom bruno_core.models import Message, MessageRole\n\n# Initialize provider (assumes Ollama running on localhost:11434)\nprovider = OllamaProvider(\n    base_url=\"http://localhost:11434\",\n    model=\"llama2\",\n    timeout=30.0\n)\n\n# Generate response\nmessages = [\n    Message(role=MessageRole.USER, content=\"Explain quantum computing\")\n]\n\nresponse = await provider.generate(\n    messages=messages,\n    temperature=0.8,\n    max_tokens=500  # Translated to num_predict for Ollama\n)\nprint(response)\n\n# Stream response\nasync for chunk in provider.stream(messages, temperature=0.8):\n    print(chunk, end=\"\", flush=True)\n</code></pre>"},{"location":"api/API_REFERENCE/#configuration_1","title":"Configuration","text":"<pre><code>from bruno_llm.providers.ollama import OllamaConfig\n\nconfig = OllamaConfig(\n    base_url=\"http://localhost:11434\",\n    model=\"llama2\",\n    timeout=30.0,\n    keep_alive=\"5m\",\n    num_ctx=4096,\n    repeat_penalty=1.1\n)\n\nprovider = OllamaProvider(config=config)\n</code></pre>"},{"location":"api/API_REFERENCE/#environment-variables_1","title":"Environment Variables","text":"<pre><code>export OLLAMA_BASE_URL=\"http://localhost:11434\"\nexport OLLAMA_MODEL=\"llama2\"\nexport OLLAMA_TIMEOUT=\"30.0\"\nexport OLLAMA_KEEP_ALIVE=\"5m\"\nexport OLLAMA_NUM_CTX=\"4096\"\n</code></pre>"},{"location":"api/API_REFERENCE/#supported-models_1","title":"Supported Models","text":"<p>Popular Ollama models: - <code>llama2</code> - Llama 2 base model - <code>llama2:13b</code> - Llama 2 13B parameter model - <code>codellama</code> - Code Llama for coding tasks - <code>mistral</code> - Mistral 7B model - <code>neural-chat</code> - Intel's Neural Chat model - <code>starling-lm</code> - Starling language model</p>"},{"location":"api/API_REFERENCE/#embedding-providers","title":"Embedding Providers","text":""},{"location":"api/API_REFERENCE/#openai-embeddings","title":"OpenAI Embeddings","text":""},{"location":"api/API_REFERENCE/#basic-usage_2","title":"Basic Usage","text":"<pre><code>from bruno_llm.providers.openai import OpenAIEmbeddingProvider\n\n# Initialize provider\nembedder = OpenAIEmbeddingProvider(\n    api_key=\"sk-...\",\n    model=\"text-embedding-ada-002\"\n)\n\n# Single text embedding\nembedding = await embedder.embed_text(\"Hello world\")\nprint(f\"Embedding dimension: {len(embedding)}\")\n\n# Multiple texts\ntexts = [\"Hello\", \"World\", \"OpenAI\"]\nembeddings = await embedder.embed_texts(texts)\n\n# Message embedding\nfrom bruno_core.models import Message, MessageRole\nmessage = Message(role=MessageRole.USER, content=\"Hello\")\nmsg_embedding = await embedder.embed_message(message)\n\n# Similarity calculation\nsimilarity = embedder.calculate_similarity(embedding, msg_embedding)\nprint(f\"Similarity: {similarity}\")\n</code></pre>"},{"location":"api/API_REFERENCE/#configuration_2","title":"Configuration","text":"<pre><code>from bruno_llm.providers.openai import OpenAIEmbeddingConfig\n\nconfig = OpenAIEmbeddingConfig(\n    api_key=\"sk-...\",\n    model=\"text-embedding-3-large\",\n    dimensions=1024,  # For text-embedding-3-* models\n    batch_size=100,\n    timeout=30.0\n)\n\nembedder = OpenAIEmbeddingProvider(config=config)\n</code></pre>"},{"location":"api/API_REFERENCE/#supported-models_2","title":"Supported Models","text":"<ul> <li><code>text-embedding-ada-002</code> - 1536 dimensions, most cost-effective</li> <li><code>text-embedding-3-small</code> - Up to 1536 dimensions, improved performance</li> <li><code>text-embedding-3-large</code> - Up to 3072 dimensions, highest performance</li> </ul>"},{"location":"api/API_REFERENCE/#ollama-embeddings","title":"Ollama Embeddings","text":""},{"location":"api/API_REFERENCE/#basic-usage_3","title":"Basic Usage","text":"<pre><code>from bruno_llm.providers.ollama import OllamaEmbeddingProvider\n\n# Initialize provider\nembedder = OllamaEmbeddingProvider(\n    base_url=\"http://localhost:11434\",\n    model=\"nomic-embed-text\",\n    batch_size=32\n)\n\n# Single text embedding\nembedding = await embedder.embed_text(\"Local embedding generation\")\nprint(f\"Dimension: {embedder.get_dimension()}\")\n\n# Batch processing\ntexts = [\"Local AI\", \"Privacy-focused\", \"No API costs\"]\nembeddings = await embedder.embed_texts(texts)\n\n# Check connection\nis_available = await embedder.check_connection()\nprint(f\"Ollama available: {is_available}\")\n</code></pre>"},{"location":"api/API_REFERENCE/#configuration_3","title":"Configuration","text":"<pre><code>from bruno_llm.providers.ollama import OllamaEmbeddingConfig\n\nconfig = OllamaEmbeddingConfig(\n    base_url=\"http://localhost:11434\",\n    model=\"mxbai-embed-large\",\n    timeout=60.0,\n    batch_size=16\n)\n\nembedder = OllamaEmbeddingProvider(config=config)\n</code></pre>"},{"location":"api/API_REFERENCE/#supported-models_3","title":"Supported Models","text":"<ul> <li><code>nomic-embed-text</code> - 768 dimensions, efficient for most tasks</li> <li><code>mxbai-embed-large</code> - 1024 dimensions, higher quality embeddings</li> <li><code>snowflake-arctic-embed</code> - 1024 dimensions, specialized for retrieval</li> </ul>"},{"location":"api/API_REFERENCE/#factory-pattern","title":"Factory Pattern","text":""},{"location":"api/API_REFERENCE/#llm-factory","title":"LLM Factory","text":"<pre><code>from bruno_llm.factory import LLMFactory\n\n# List available providers\nproviders = LLMFactory.list_providers()\nprint(providers)  # ['openai', 'ollama']\n\n# Create provider directly\nllm = LLMFactory.create(\n    provider=\"openai\",\n    config={\"api_key\": \"sk-...\", \"model\": \"gpt-4\"}\n)\n\n# Create from environment variables\nllm = LLMFactory.create_from_env(provider=\"openai\")\n\n# Create with fallback\nllm = LLMFactory.create_with_fallback(\n    providers=[\"openai\", \"ollama\"],\n    configs=[\n        {\"api_key\": \"sk-...\", \"model\": \"gpt-4\"},\n        {\"model\": \"llama2\"}\n    ]\n)\n\n# Check if provider is registered\nif LLMFactory.is_registered(\"openai\"):\n    llm = LLMFactory.create(\"openai\", config)\n</code></pre>"},{"location":"api/API_REFERENCE/#embedding-factory","title":"Embedding Factory","text":"<pre><code>from bruno_llm.embedding_factory import EmbeddingFactory\n\n# List available embedding providers\nproviders = EmbeddingFactory.list_providers()\nprint(providers)  # ['openai', 'ollama']\n\n# Create embedding provider\nembedder = EmbeddingFactory.create(\n    provider=\"ollama\",\n    config={\"model\": \"nomic-embed-text\"}\n)\n\n# Create from environment\nembedder = EmbeddingFactory.create_from_env(provider=\"openai\")\n\n# Get provider info\ninfo = EmbeddingFactory.get_provider_info(\"openai\")\nprint(info)\n</code></pre>"},{"location":"api/API_REFERENCE/#error-handling","title":"Error Handling","text":""},{"location":"api/API_REFERENCE/#exception-hierarchy","title":"Exception Hierarchy","text":"<pre><code>from bruno_llm.exceptions import (\n    LLMError,           # Base exception\n    AuthenticationError,\n    RateLimitError,\n    ModelNotFoundError,\n    ContextLengthExceededError,\n    StreamError,\n    TimeoutError,\n    ConfigurationError,\n    InvalidResponseError,\n    ProviderNotFoundError\n)\n\n# Example error handling\ntry:\n    response = await provider.generate(messages)\nexcept AuthenticationError:\n    print(\"Invalid API key\")\nexcept RateLimitError as e:\n    print(f\"Rate limited. Retry after: {e.retry_after}\")\nexcept ModelNotFoundError:\n    print(\"Model not available\")\nexcept ContextLengthExceededError:\n    print(\"Input too long\")\nexcept LLMError as e:\n    print(f\"LLM error: {e}\")\n</code></pre>"},{"location":"api/API_REFERENCE/#connection-checking","title":"Connection Checking","text":"<pre><code># Check if provider is accessible\ntry:\n    is_connected = await provider.check_connection()\n    if not is_connected:\n        print(\"Provider not available\")\nexcept Exception as e:\n    print(f\"Connection check failed: {e}\")\n</code></pre>"},{"location":"api/API_REFERENCE/#advanced-features","title":"Advanced Features","text":""},{"location":"api/API_REFERENCE/#cost-tracking","title":"Cost Tracking","text":"<pre><code>from bruno_llm.providers.openai import OpenAIProvider\n\n# Enable cost tracking\nprovider = OpenAIProvider(\n    api_key=\"sk-...\",\n    model=\"gpt-4\",\n    track_cost=True\n)\n\n# After making requests\ncost_info = provider.get_model_info()\nprint(f\"Total cost: ${cost_info.get('total_cost', 0):.4f}\")\nprint(f\"Total tokens: {cost_info.get('total_tokens', 0)}\")\n</code></pre>"},{"location":"api/API_REFERENCE/#context-management","title":"Context Management","text":"<pre><code>from bruno_llm.base.context import ContextManager\nfrom bruno_core.models import Message, MessageRole\n\n# Initialize context manager\ncontext_mgr = ContextManager(\n    max_tokens=4000,\n    model=\"gpt-4\"\n)\n\n# Add messages and check limits\nmessages = [\n    Message(role=MessageRole.SYSTEM, content=\"You are helpful.\"),\n    Message(role=MessageRole.USER, content=\"Long conversation...\")\n]\n\n# Check if within limits\nif context_mgr.check_limit(messages):\n    response = await provider.generate(messages)\nelse:\n    # Truncate if needed\n    truncated = context_mgr.truncate_messages(messages)\n    response = await provider.generate(truncated)\n</code></pre>"},{"location":"api/API_REFERENCE/#streaming-with-aggregation","title":"Streaming with Aggregation","text":"<pre><code>from bruno_llm.base.streaming import StreamAggregator, AggregationStrategy\n\n# Word-based aggregation\naggregator = StreamAggregator(\n    strategy=AggregationStrategy.WORD,\n    buffer_size=5\n)\n\n# Stream and aggregate\nasync for chunk in provider.stream(messages):\n    aggregated_chunks = aggregator.add_chunk(chunk)\n    for aggregated in aggregated_chunks:\n        print(aggregated, end=\" \", flush=True)\n\n# Get final chunks\nfinal_chunks = aggregator.finalize()\nfor chunk in final_chunks:\n    print(chunk, end=\" \", flush=True)\n</code></pre>"},{"location":"api/API_REFERENCE/#rate-limiting","title":"Rate Limiting","text":"<pre><code>from bruno_llm.base.rate_limiter import RateLimiter\n\n# Create rate limiter (60 requests per minute)\nlimiter = RateLimiter(requests_per_minute=60)\n\n# Use with provider\nasync with limiter:\n    response = await provider.generate(messages)\n</code></pre>"},{"location":"api/API_REFERENCE/#retry-logic","title":"Retry Logic","text":"<pre><code>from bruno_llm.base.retry import retry_async, RetryConfig\n\n# Configure retry behavior\nretry_config = RetryConfig(\n    max_attempts=3,\n    base_delay=1.0,\n    max_delay=10.0,\n    exponential_base=2.0\n)\n\n# Apply retry decorator\n@retry_async(retry_config)\nasync def generate_with_retry():\n    return await provider.generate(messages)\n\nresponse = await generate_with_retry()\n</code></pre>"},{"location":"api/API_REFERENCE/#best-practices","title":"Best Practices","text":""},{"location":"api/API_REFERENCE/#1-use-factory-pattern","title":"1. Use Factory Pattern","text":"<pre><code># Preferred - flexible and configurable\nfrom bruno_llm.factory import LLMFactory\n\nllm = LLMFactory.create_from_env(\"openai\")\n\n# Instead of direct instantiation\nfrom bruno_llm.providers.openai import OpenAIProvider\nllm = OpenAIProvider(api_key=\"...\")\n</code></pre>"},{"location":"api/API_REFERENCE/#2-handle-errors-gracefully","title":"2. Handle Errors Gracefully","text":"<pre><code>from bruno_llm.exceptions import RateLimitError\nimport asyncio\n\nasync def safe_generate(provider, messages, max_retries=3):\n    for attempt in range(max_retries):\n        try:\n            return await provider.generate(messages)\n        except RateLimitError as e:\n            if attempt == max_retries - 1:\n                raise\n            await asyncio.sleep(e.retry_after or 60)\n\n    raise Exception(\"Max retries exceeded\")\n</code></pre>"},{"location":"api/API_REFERENCE/#3-use-context-managers","title":"3. Use Context Managers","text":"<pre><code># Ensure proper cleanup\nasync with provider:\n    response = await provider.generate(messages)\n# Provider is automatically cleaned up\n</code></pre>"},{"location":"api/API_REFERENCE/#4-monitor-usage","title":"4. Monitor Usage","text":"<pre><code># Track token usage and costs\ninfo = provider.get_model_info()\nif info.get('total_cost', 0) &gt; budget_limit:\n    print(\"Budget exceeded!\")\n</code></pre>"},{"location":"api/API_REFERENCE/#5-environment-configuration","title":"5. Environment Configuration","text":"<pre><code># Use environment variables for configuration\nimport os\n\n# Set in environment or .env file\nos.environ['OPENAI_API_KEY'] = 'sk-...'\nos.environ['OLLAMA_BASE_URL'] = 'http://localhost:11434'\n\n# Create providers from environment\nllm = LLMFactory.create_from_env('openai')\nembedder = EmbeddingFactory.create_from_env('ollama')\n</code></pre>"},{"location":"api/API_REFERENCE/#integration-with-bruno-core","title":"Integration with Bruno-Core","text":"<p>Bruno-LLM is designed to work seamlessly with the broader Bruno AI ecosystem:</p> <pre><code>from bruno_core.base import BaseAssistant\nfrom bruno_core.memory import MemoryInterface\nfrom bruno_llm.factory import LLMFactory\nfrom bruno_llm.embedding_factory import EmbeddingFactory\n\n# Create components\nllm = LLMFactory.create_from_env('openai')\nembedder = EmbeddingFactory.create_from_env('openai')\nmemory = SomeMemoryImplementation(embedder)\n\n# Create assistant\nassistant = BaseAssistant(\n    llm=llm,\n    memory=memory,\n    name=\"My Assistant\"\n)\n\n# Use assistant\nawait assistant.initialize()\nresponse = await assistant.process_message(\n    Message(role=MessageRole.USER, content=\"Hello!\")\n)\n</code></pre>"},{"location":"api/API_REFERENCE/#performance-considerations","title":"Performance Considerations","text":""},{"location":"api/API_REFERENCE/#1-batch-processing","title":"1. Batch Processing","text":"<pre><code># Efficient batch embedding\ntexts = [\"text1\", \"text2\", \"text3\", ...]\nembeddings = await embedder.embed_texts(texts)  # More efficient than individual calls\n</code></pre>"},{"location":"api/API_REFERENCE/#2-connection-pooling","title":"2. Connection Pooling","text":"<pre><code># Providers automatically handle connection pooling\n# Reuse provider instances when possible\nprovider = OpenAIProvider(api_key=\"...\")\n\n# Multiple requests use the same connection pool\nfor messages in conversation_batch:\n    await provider.generate(messages)\n</code></pre>"},{"location":"api/API_REFERENCE/#3-streaming-for-long-responses","title":"3. Streaming for Long Responses","text":"<pre><code># Use streaming for real-time feedback\nasync for chunk in provider.stream(messages):\n    # Process chunks as they arrive\n    process_chunk(chunk)\n</code></pre>"},{"location":"api/API_REFERENCE/#4-context-length-management","title":"4. Context Length Management","text":"<pre><code># Monitor and manage context length\nfrom bruno_llm.base.context import ContextManager\n\ncontext_mgr = ContextManager(max_tokens=4000)\nif not context_mgr.check_limit(messages):\n    messages = context_mgr.truncate_messages(messages)\n</code></pre>"},{"location":"api/API_REFERENCE/#troubleshooting","title":"Troubleshooting","text":""},{"location":"api/API_REFERENCE/#common-issues","title":"Common Issues","text":"<ol> <li> <p>Authentication Errors <pre><code># Verify API keys\nexport OPENAI_API_KEY=\"sk-...\"\n\n# Test connection\nis_connected = await provider.check_connection()\n</code></pre></p> </li> <li> <p>Rate Limiting <pre><code># Implement exponential backoff\nfrom bruno_llm.base.retry import retry_async\n\n@retry_async()\nasync def make_request():\n    return await provider.generate(messages)\n</code></pre></p> </li> <li> <p>Ollama Connection Issues <pre><code># Start Ollama service\nollama serve\n\n# Pull required model\nollama pull llama2\n</code></pre></p> </li> <li> <p>Memory Issues with Large Embeddings <pre><code># Process in smaller batches\nbatch_size = embedder.get_max_batch_size()\nfor i in range(0, len(texts), batch_size):\n    batch = texts[i:i+batch_size]\n    batch_embeddings = await embedder.embed_texts(batch)\n</code></pre></p> </li> </ol>"},{"location":"api/API_REFERENCE/#debugging","title":"Debugging","text":"<pre><code>import logging\n\n# Enable debug logging\nlogging.basicConfig(level=logging.DEBUG)\n\n# Provider-specific debug info\ninfo = provider.get_model_info()\nprint(f\"Provider info: {info}\")\n\n# Check provider status\nstatus = await provider.check_connection()\nprint(f\"Connection status: {status}\")\n</code></pre>"},{"location":"api/API_REFERENCE/#api-reference-summary","title":"API Reference Summary","text":""},{"location":"api/API_REFERENCE/#llm-providers_1","title":"LLM Providers","text":"<ul> <li><code>OpenAIProvider</code> - GPT models via OpenAI API</li> <li><code>OllamaProvider</code> - Local models via Ollama</li> </ul>"},{"location":"api/API_REFERENCE/#embedding-providers_1","title":"Embedding Providers","text":"<ul> <li><code>OpenAIEmbeddingProvider</code> - OpenAI embedding models</li> <li><code>OllamaEmbeddingProvider</code> - Local embedding models</li> </ul>"},{"location":"api/API_REFERENCE/#factory-classes","title":"Factory Classes","text":"<ul> <li><code>LLMFactory</code> - Create and manage LLM providers</li> <li><code>EmbeddingFactory</code> - Create and manage embedding providers</li> </ul>"},{"location":"api/API_REFERENCE/#base-classes","title":"Base Classes","text":"<ul> <li><code>BaseProvider</code> - Common provider functionality</li> <li><code>BaseEmbeddingProvider</code> - Common embedding functionality</li> </ul>"},{"location":"api/API_REFERENCE/#utilities","title":"Utilities","text":"<ul> <li><code>ContextManager</code> - Token and context length management</li> <li><code>RateLimiter</code> - Request rate limiting</li> <li><code>CostTracker</code> - Usage and cost tracking</li> <li><code>StreamAggregator</code> - Stream processing and aggregation</li> </ul>"},{"location":"api/API_REFERENCE/#configuration_4","title":"Configuration","text":"<ul> <li><code>OpenAIConfig</code> / <code>OpenAIEmbeddingConfig</code></li> <li><code>OllamaConfig</code> / <code>OllamaEmbeddingConfig</code></li> </ul> <p>For the latest API updates and examples, visit the Bruno-LLM repository.</p>"},{"location":"api/EMBEDDING_GUIDE/","title":"Embedding Guide","text":""},{"location":"api/EMBEDDING_GUIDE/#overview","title":"Overview","text":"<p>Bruno-LLM provides powerful embedding capabilities through various providers. Embeddings convert text into numerical vectors that capture semantic meaning, enabling similarity search, clustering, classification, and retrieval-augmented generation (RAG) systems.</p>"},{"location":"api/EMBEDDING_GUIDE/#supported-embedding-providers","title":"Supported Embedding Providers","text":""},{"location":"api/EMBEDDING_GUIDE/#openai-embeddings","title":"OpenAI Embeddings","text":"<ul> <li>Models: <code>text-embedding-ada-002</code>, <code>text-embedding-3-small</code>, <code>text-embedding-3-large</code></li> <li>Features: High-quality embeddings, batch processing, cost optimization</li> <li>Use Cases: Production applications, high-accuracy requirements</li> </ul>"},{"location":"api/EMBEDDING_GUIDE/#ollama-embeddings","title":"Ollama Embeddings","text":"<ul> <li>Models: <code>nomic-embed-text</code>, <code>mxbai-embed-large</code>, <code>snowflake-arctic-embed</code></li> <li>Features: Local processing, privacy-focused, no usage costs</li> <li>Use Cases: Privacy-sensitive applications, offline processing</li> </ul>"},{"location":"api/EMBEDDING_GUIDE/#quick-start","title":"Quick Start","text":""},{"location":"api/EMBEDDING_GUIDE/#basic-usage","title":"Basic Usage","text":"<pre><code>from bruno_llm.embedding_factory import EmbeddingFactory\nfrom bruno_llm.providers.openai import OpenAIEmbeddingProvider\nfrom bruno_llm.providers.ollama import OllamaEmbeddingProvider\n\n# OpenAI embeddings (cloud-based)\nopenai_embedder = OpenAIEmbeddingProvider(\n    api_key=\"your-api-key\",\n    model=\"text-embedding-3-small\"\n)\n\n# Ollama embeddings (local)\nollama_embedder = OllamaEmbeddingProvider(\n    base_url=\"http://localhost:11434\",\n    model=\"nomic-embed-text\"\n)\n\n# Single text embedding\ntext = \"Machine learning transforms how we process information\"\nembedding = await openai_embedder.embed_text(text)\nprint(f\"Embedding dimension: {len(embedding)}\")\n\n# Batch processing\ntexts = [\n    \"Artificial intelligence is reshaping technology\",\n    \"Machine learning enables pattern recognition\",\n    \"Deep learning uses neural networks\"\n]\nembeddings = await ollama_embedder.embed_texts(texts)\nprint(f\"Generated {len(embeddings)} embeddings\")\n</code></pre>"},{"location":"api/EMBEDDING_GUIDE/#factory-pattern","title":"Factory Pattern","text":"<pre><code>from bruno_llm.embedding_factory import EmbeddingFactory\n\n# Create from configuration\nembedder = EmbeddingFactory.create(\n    provider=\"openai\",\n    config={\n        \"api_key\": \"your-api-key\",\n        \"model\": \"text-embedding-3-small\"\n    }\n)\n\n# Create from environment variables\nembedder = EmbeddingFactory.create_from_env(\"ollama\")\n\n# Auto-selection with fallback\nembedder = EmbeddingFactory.create_with_fallback(\n    providers=[\"openai\", \"ollama\"],\n    configs=[openai_config, ollama_config]\n)\n</code></pre>"},{"location":"api/EMBEDDING_GUIDE/#embedding-models-comparison","title":"Embedding Models Comparison","text":""},{"location":"api/EMBEDDING_GUIDE/#openai-models","title":"OpenAI Models","text":"Model Dimensions Max Input Price/1M tokens Use Case <code>text-embedding-ada-002</code> 1536 8191 $0.10 General purpose <code>text-embedding-3-small</code> 1536 8191 $0.02 Cost-effective <code>text-embedding-3-large</code> 3072 8191 $0.13 High performance"},{"location":"api/EMBEDDING_GUIDE/#ollama-models","title":"Ollama Models","text":"Model Dimensions Context Length Description <code>nomic-embed-text</code> 768 2048 Fast, general purpose <code>mxbai-embed-large</code> 1024 512 High quality embeddings <code>snowflake-arctic-embed</code> 1024 512 Specialized for retrieval <code>all-minilm</code> 384 256 Compact, efficient"},{"location":"api/EMBEDDING_GUIDE/#advanced-usage","title":"Advanced Usage","text":""},{"location":"api/EMBEDDING_GUIDE/#similarity-search","title":"Similarity Search","text":"<pre><code>import numpy as np\nfrom typing import List, Tuple\n\nclass SemanticSearch:\n    \"\"\"Semantic search using embeddings.\"\"\"\n\n    def __init__(self, embedder):\n        self.embedder = embedder\n        self.documents = []\n        self.embeddings = []\n\n    async def add_documents(self, texts: List[str]):\n        \"\"\"Add documents to the search index.\"\"\"\n        # Generate embeddings for all documents\n        embeddings = await self.embedder.embed_texts(texts)\n\n        self.documents.extend(texts)\n        self.embeddings.extend(embeddings)\n\n        print(f\"Added {len(texts)} documents to index\")\n\n    async def search(self, query: str, top_k: int = 5) -&gt; List[Tuple[str, float]]:\n        \"\"\"Search for similar documents.\"\"\"\n        if not self.embeddings:\n            return []\n\n        # Get query embedding\n        query_embedding = await self.embedder.embed_text(query)\n\n        # Calculate similarities\n        similarities = []\n        for i, doc_embedding in enumerate(self.embeddings):\n            similarity = self.embedder.calculate_similarity(query_embedding, doc_embedding)\n            similarities.append((self.documents[i], similarity))\n\n        # Sort by similarity and return top results\n        similarities.sort(key=lambda x: x[1], reverse=True)\n        return similarities[:top_k]\n\n# Usage\nembedder = EmbeddingFactory.create_from_env(\"openai\")\nsearch = SemanticSearch(embedder)\n\n# Build index\ndocuments = [\n    \"Python is a versatile programming language\",\n    \"Machine learning algorithms learn from data\",\n    \"Neural networks are inspired by the brain\",\n    \"Data science combines statistics and programming\",\n    \"APIs enable software communication\"\n]\nawait search.add_documents(documents)\n\n# Search\nresults = await search.search(\"programming languages\", top_k=3)\nfor doc, score in results:\n    print(f\"Score: {score:.3f} - {doc}\")\n</code></pre>"},{"location":"api/EMBEDDING_GUIDE/#document-clustering","title":"Document Clustering","text":"<pre><code>from sklearn.cluster import KMeans\nimport numpy as np\n\nasync def cluster_documents(embedder, texts: List[str], n_clusters: int = 3):\n    \"\"\"Cluster documents based on their embeddings.\"\"\"\n\n    # Generate embeddings\n    print(\"Generating embeddings...\")\n    embeddings = await embedder.embed_texts(texts)\n\n    # Convert to numpy array\n    embedding_matrix = np.array(embeddings)\n\n    # Perform clustering\n    print(f\"Clustering into {n_clusters} groups...\")\n    kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n    labels = kmeans.fit_predict(embedding_matrix)\n\n    # Group documents by cluster\n    clusters = {}\n    for i, label in enumerate(labels):\n        if label not in clusters:\n            clusters[label] = []\n        clusters[label].append(texts[i])\n\n    # Print results\n    for cluster_id, cluster_docs in clusters.items():\n        print(f\"\\n\ud83d\udcc1 Cluster {cluster_id}:\")\n        for doc in cluster_docs:\n            print(f\"  \u2022 {doc}\")\n\n    return clusters\n\n# Usage\nembedder = EmbeddingFactory.create_from_env(\"ollama\")\n\ndocuments = [\n    \"Python programming tutorial\",\n    \"JavaScript web development\",\n    \"Machine learning with scikit-learn\",\n    \"Deep learning neural networks\",\n    \"React frontend framework\",\n    \"Vue.js component system\",\n    \"Supervised learning algorithms\",\n    \"Unsupervised learning techniques\"\n]\n\nclusters = await cluster_documents(embedder, documents, n_clusters=3)\n</code></pre>"},{"location":"api/EMBEDDING_GUIDE/#rag-retrieval-augmented-generation","title":"RAG (Retrieval-Augmented Generation)","text":"<pre><code>from bruno_llm.factory import LLMFactory\nfrom bruno_llm.embedding_factory import EmbeddingFactory\nfrom bruno_core.models import Message, MessageRole\n\nclass RAGSystem:\n    \"\"\"Complete RAG system with embeddings and LLM.\"\"\"\n\n    def __init__(self, llm_provider: str = \"openai\", embedding_provider: str = \"openai\"):\n        self.llm = LLMFactory.create_from_env(llm_provider)\n        self.embedder = EmbeddingFactory.create_from_env(embedding_provider)\n        self.knowledge_base = []  # (text, embedding) pairs\n\n    async def add_knowledge(self, documents: List[str]):\n        \"\"\"Add documents to the knowledge base.\"\"\"\n        print(f\"Processing {len(documents)} documents...\")\n\n        # Generate embeddings in batches\n        embeddings = await self.embedder.embed_texts(documents)\n\n        # Store documents and embeddings\n        for doc, embedding in zip(documents, embeddings):\n            self.knowledge_base.append((doc, embedding))\n\n        print(f\"Knowledge base now contains {len(self.knowledge_base)} documents\")\n\n    async def retrieve_relevant(self, query: str, top_k: int = 3) -&gt; List[str]:\n        \"\"\"Retrieve most relevant documents for a query.\"\"\"\n        if not self.knowledge_base:\n            return []\n\n        # Get query embedding\n        query_embedding = await self.embedder.embed_text(query)\n\n        # Calculate similarities\n        similarities = []\n        for doc, doc_embedding in self.knowledge_base:\n            similarity = self.embedder.calculate_similarity(query_embedding, doc_embedding)\n            similarities.append((doc, similarity))\n\n        # Sort and return top documents\n        similarities.sort(key=lambda x: x[1], reverse=True)\n        return [doc for doc, _ in similarities[:top_k]]\n\n    async def answer_question(self, question: str) -&gt; str:\n        \"\"\"Answer a question using RAG.\"\"\"\n        # Retrieve relevant context\n        context_docs = await self.retrieve_relevant(question, top_k=3)\n\n        if not context_docs:\n            return \"I don't have relevant information to answer this question.\"\n\n        # Build context\n        context = \"\\n\\n\".join(context_docs)\n\n        # Generate answer\n        messages = [\n            Message(role=MessageRole.SYSTEM, content=\n                \"Answer the question based on the provided context. \"\n                \"If the context doesn't contain enough information, say so clearly.\"),\n            Message(role=MessageRole.USER, content=\n                f\"Context:\\n{context}\\n\\nQuestion: {question}\")\n        ]\n\n        response = await self.llm.generate(messages, temperature=0.1)\n        return response\n\n# Usage\nrag = RAGSystem(llm_provider=\"openai\", embedding_provider=\"openai\")\n\n# Build knowledge base\nknowledge = [\n    \"Bruno-LLM is a Python package for LLM provider integration.\",\n    \"It supports OpenAI, Ollama, and other providers through a unified interface.\",\n    \"The factory pattern makes it easy to switch between providers.\",\n    \"Embedding providers enable semantic search and RAG applications.\",\n    \"All providers implement async interfaces for high performance.\"\n]\n\nawait rag.add_knowledge(knowledge)\n\n# Ask questions\nanswer = await rag.answer_question(\"What is Bruno-LLM and what does it support?\")\nprint(f\"Answer: {answer}\")\n</code></pre>"},{"location":"api/EMBEDDING_GUIDE/#performance-optimization","title":"Performance Optimization","text":""},{"location":"api/EMBEDDING_GUIDE/#batch-processing","title":"Batch Processing","text":"<pre><code>async def efficient_batch_processing(embedder, texts: List[str], batch_size: int = 32):\n    \"\"\"Process large text collections efficiently.\"\"\"\n\n    total_texts = len(texts)\n    all_embeddings = []\n\n    print(f\"Processing {total_texts} texts in batches of {batch_size}...\")\n\n    for i in range(0, total_texts, batch_size):\n        batch = texts[i:i + batch_size]\n\n        try:\n            # Process batch\n            batch_embeddings = await embedder.embed_texts(batch)\n            all_embeddings.extend(batch_embeddings)\n\n            # Progress update\n            processed = len(all_embeddings)\n            progress = processed / total_texts * 100\n            print(f\"Progress: {progress:.1f}% ({processed}/{total_texts})\")\n\n            # Rate limiting for API providers\n            if hasattr(embedder, 'api_key'):  # Likely an API provider\n                await asyncio.sleep(0.1)  # Small delay\n\n        except Exception as e:\n            print(f\"Error processing batch starting at index {i}: {e}\")\n            # Add empty embeddings to maintain alignment\n            all_embeddings.extend([None] * len(batch))\n\n    print(f\"\u2705 Completed processing {len(all_embeddings)} embeddings\")\n    return all_embeddings\n</code></pre>"},{"location":"api/EMBEDDING_GUIDE/#caching-for-performance","title":"Caching for Performance","text":"<pre><code>import hashlib\nimport json\nimport os\nfrom pathlib import Path\n\nclass EmbeddingCache:\n    \"\"\"Cache embeddings to avoid recomputation.\"\"\"\n\n    def __init__(self, cache_dir: str = \"embedding_cache\"):\n        self.cache_dir = Path(cache_dir)\n        self.cache_dir.mkdir(exist_ok=True)\n\n    def _get_cache_key(self, text: str, model: str) -&gt; str:\n        \"\"\"Generate cache key for text and model.\"\"\"\n        content = f\"{model}:{text}\"\n        return hashlib.sha256(content.encode()).hexdigest()\n\n    def get_embedding(self, text: str, model: str) -&gt; List[float]:\n        \"\"\"Get embedding from cache.\"\"\"\n        cache_key = self._get_cache_key(text, model)\n        cache_file = self.cache_dir / f\"{cache_key}.json\"\n\n        if cache_file.exists():\n            with open(cache_file, 'r') as f:\n                return json.load(f)\n        return None\n\n    def store_embedding(self, text: str, model: str, embedding: List[float]):\n        \"\"\"Store embedding in cache.\"\"\"\n        cache_key = self._get_cache_key(text, model)\n        cache_file = self.cache_dir / f\"{cache_key}.json\"\n\n        with open(cache_file, 'w') as f:\n            json.dump(embedding, f)\n\n    def clear_cache(self):\n        \"\"\"Clear all cached embeddings.\"\"\"\n        for cache_file in self.cache_dir.glob(\"*.json\"):\n            cache_file.unlink()\n\nclass CachedEmbedder:\n    \"\"\"Embedder with caching support.\"\"\"\n\n    def __init__(self, embedder, cache_dir: str = \"embedding_cache\"):\n        self.embedder = embedder\n        self.cache = EmbeddingCache(cache_dir)\n        self.model = getattr(embedder, 'model', 'unknown')\n\n    async def embed_text(self, text: str) -&gt; List[float]:\n        \"\"\"Get embedding with caching.\"\"\"\n        # Check cache first\n        cached = self.cache.get_embedding(text, self.model)\n        if cached is not None:\n            return cached\n\n        # Generate new embedding\n        embedding = await self.embedder.embed_text(text)\n\n        # Store in cache\n        self.cache.store_embedding(text, self.model, embedding)\n\n        return embedding\n\n    async def embed_texts(self, texts: List[str]) -&gt; List[List[float]]:\n        \"\"\"Batch embedding with caching.\"\"\"\n        embeddings = []\n        uncached_texts = []\n        uncached_indices = []\n\n        # Check cache for each text\n        for i, text in enumerate(texts):\n            cached = self.cache.get_embedding(text, self.model)\n            if cached is not None:\n                embeddings.append(cached)\n            else:\n                embeddings.append(None)  # Placeholder\n                uncached_texts.append(text)\n                uncached_indices.append(i)\n\n        # Generate embeddings for uncached texts\n        if uncached_texts:\n            print(f\"Generating {len(uncached_texts)} new embeddings...\")\n            new_embeddings = await self.embedder.embed_texts(uncached_texts)\n\n            # Store new embeddings and fill placeholders\n            for idx, embedding in zip(uncached_indices, new_embeddings):\n                self.cache.store_embedding(texts[idx], self.model, embedding)\n                embeddings[idx] = embedding\n\n        return embeddings\n\n# Usage\nbase_embedder = EmbeddingFactory.create_from_env(\"openai\")\ncached_embedder = CachedEmbedder(base_embedder)\n\n# First call generates embeddings\nembeddings1 = await cached_embedder.embed_texts([\"Hello world\", \"AI is amazing\"])\n\n# Second call uses cache\nembeddings2 = await cached_embedder.embed_texts([\"Hello world\", \"New text\"])  # Only \"New text\" computed\n</code></pre>"},{"location":"api/EMBEDDING_GUIDE/#memory-efficient-processing","title":"Memory-Efficient Processing","text":"<pre><code>async def memory_efficient_embeddings(embedder, texts: List[str], max_memory_mb: int = 1000):\n    \"\"\"Process embeddings while managing memory usage.\"\"\"\n\n    import sys\n\n    def estimate_memory_usage(num_embeddings: int, embedding_dim: int) -&gt; int:\n        \"\"\"Estimate memory usage in MB.\"\"\"\n        # Each float is 4 bytes\n        bytes_per_embedding = embedding_dim * 4\n        total_bytes = num_embeddings * bytes_per_embedding\n        return total_bytes / (1024 * 1024)  # Convert to MB\n\n    # Get embedding dimension from a sample\n    sample_embedding = await embedder.embed_text(\"sample\")\n    embedding_dim = len(sample_embedding)\n\n    # Calculate optimal batch size\n    max_embeddings_in_memory = int(max_memory_mb / (embedding_dim * 4 / 1024 / 1024))\n    batch_size = min(len(texts), max_embeddings_in_memory)\n\n    print(f\"Processing with batch size: {batch_size}\")\n    print(f\"Estimated memory per batch: {estimate_memory_usage(batch_size, embedding_dim):.1f} MB\")\n\n    all_embeddings = []\n\n    for i in range(0, len(texts), batch_size):\n        batch = texts[i:i + batch_size]\n\n        # Process batch\n        batch_embeddings = await embedder.embed_texts(batch)\n\n        # Store results (could write to disk for very large datasets)\n        all_embeddings.extend(batch_embeddings)\n\n        # Optional: Force garbage collection\n        import gc\n        gc.collect()\n\n        print(f\"Processed batch {i//batch_size + 1}/{(len(texts)-1)//batch_size + 1}\")\n\n    return all_embeddings\n</code></pre>"},{"location":"api/EMBEDDING_GUIDE/#error-handling","title":"Error Handling","text":""},{"location":"api/EMBEDDING_GUIDE/#robust-embedding-generation","title":"Robust Embedding Generation","text":"<pre><code>from bruno_llm.exceptions import LLMError, LLMTimeoutError\nimport asyncio\n\nasync def robust_embed_texts(embedder, texts: List[str], max_retries: int = 3):\n    \"\"\"Generate embeddings with error handling and retries.\"\"\"\n\n    results = []\n\n    for i, text in enumerate(texts):\n        success = False\n\n        for attempt in range(max_retries):\n            try:\n                embedding = await embedder.embed_text(text)\n                results.append(embedding)\n                success = True\n                break\n\n            except LLMTimeoutError:\n                print(f\"Timeout for text {i}, attempt {attempt + 1}\")\n                if attempt &lt; max_retries - 1:\n                    await asyncio.sleep(2 ** attempt)  # Exponential backoff\n\n            except LLMError as e:\n                print(f\"Error for text {i}: {e}\")\n                if attempt &lt; max_retries - 1:\n                    await asyncio.sleep(1)\n                else:\n                    # Add placeholder for failed embedding\n                    results.append(None)\n                    success = True  # Don't retry further\n                    break\n\n        if not success:\n            results.append(None)  # Final fallback\n\n    # Count successful embeddings\n    successful = sum(1 for r in results if r is not None)\n    print(f\"Successfully generated {successful}/{len(texts)} embeddings\")\n\n    return results\n</code></pre>"},{"location":"api/EMBEDDING_GUIDE/#integration-patterns","title":"Integration Patterns","text":""},{"location":"api/EMBEDDING_GUIDE/#vector-database-integration","title":"Vector Database Integration","text":"<pre><code># Example with a simple in-memory vector store\nimport numpy as np\nfrom typing import Dict, Any\n\nclass SimpleVectorStore:\n    \"\"\"Simple vector database for embeddings.\"\"\"\n\n    def __init__(self):\n        self.vectors = []\n        self.metadata = []\n        self.ids = []\n        self._id_counter = 0\n\n    def add_vectors(self, embeddings: List[List[float]], metadata: List[Dict[str, Any]]):\n        \"\"\"Add vectors with metadata.\"\"\"\n        for embedding, meta in zip(embeddings, metadata):\n            self.vectors.append(np.array(embedding))\n            self.metadata.append(meta)\n            self.ids.append(self._id_counter)\n            self._id_counter += 1\n\n    def search(self, query_vector: List[float], top_k: int = 5) -&gt; List[Tuple[int, float, Dict[str, Any]]]:\n        \"\"\"Search for similar vectors.\"\"\"\n        if not self.vectors:\n            return []\n\n        query_array = np.array(query_vector)\n        similarities = []\n\n        for i, vector in enumerate(self.vectors):\n            # Cosine similarity\n            similarity = np.dot(query_array, vector) / (np.linalg.norm(query_array) * np.linalg.norm(vector))\n            similarities.append((self.ids[i], similarity, self.metadata[i]))\n\n        # Sort by similarity\n        similarities.sort(key=lambda x: x[1], reverse=True)\n        return similarities[:top_k]\n\n# Usage with embeddings\nasync def build_vector_database():\n    \"\"\"Build and query a vector database.\"\"\"\n\n    embedder = EmbeddingFactory.create_from_env(\"openai\")\n    vector_store = SimpleVectorStore()\n\n    # Sample documents\n    documents = [\n        {\"text\": \"Python is a programming language\", \"category\": \"programming\"},\n        {\"text\": \"Machine learning uses algorithms\", \"category\": \"AI\"},\n        {\"text\": \"Databases store structured data\", \"category\": \"data\"},\n        {\"text\": \"APIs connect different systems\", \"category\": \"programming\"},\n        {\"text\": \"Neural networks learn patterns\", \"category\": \"AI\"}\n    ]\n\n    # Generate embeddings\n    texts = [doc[\"text\"] for doc in documents]\n    embeddings = await embedder.embed_texts(texts)\n\n    # Add to vector store\n    vector_store.add_vectors(embeddings, documents)\n\n    # Query the database\n    query = \"What are programming concepts?\"\n    query_embedding = await embedder.embed_text(query)\n\n    results = vector_store.search(query_embedding, top_k=3)\n\n    print(f\"Query: {query}\")\n    print(\"Results:\")\n    for doc_id, similarity, metadata in results:\n        print(f\"  {similarity:.3f}: {metadata['text']} (category: {metadata['category']})\")\n\nawait build_vector_database()\n</code></pre>"},{"location":"api/EMBEDDING_GUIDE/#best-practices","title":"Best Practices","text":""},{"location":"api/EMBEDDING_GUIDE/#1-provider-selection","title":"1. Provider Selection","text":"<pre><code>def choose_embedding_provider(use_case: str, budget: str, privacy: str):\n    \"\"\"Choose the best embedding provider for your needs.\"\"\"\n\n    if privacy == \"high\" or budget == \"free\":\n        return \"ollama\"  # Local, private, free\n    elif budget == \"low\":\n        return \"openai\", \"text-embedding-3-small\"  # Cheap OpenAI option\n    elif use_case == \"high_accuracy\":\n        return \"openai\", \"text-embedding-3-large\"  # Best performance\n    else:\n        return \"openai\", \"text-embedding-ada-002\"  # Balanced choice\n\nprovider_name, model = choose_embedding_provider(\n    use_case=\"general\",\n    budget=\"medium\",\n    privacy=\"medium\"\n)\n</code></pre>"},{"location":"api/EMBEDDING_GUIDE/#2-text-preprocessing","title":"2. Text Preprocessing","text":"<pre><code>def preprocess_text_for_embeddings(text: str) -&gt; str:\n    \"\"\"Prepare text for optimal embedding generation.\"\"\"\n\n    # Remove excessive whitespace\n    text = \" \".join(text.split())\n\n    # Truncate if too long (model-dependent)\n    max_length = 8000  # Adjust based on model\n    if len(text) &gt; max_length:\n        text = text[:max_length].rsplit(' ', 1)[0]  # Cut at word boundary\n\n    # Optional: Remove special characters, normalize case, etc.\n    # Be careful not to lose important semantic information\n\n    return text.strip()\n</code></pre>"},{"location":"api/EMBEDDING_GUIDE/#3-monitoring-and-metrics","title":"3. Monitoring and Metrics","text":"<pre><code>import time\nfrom dataclasses import dataclass\nfrom typing import Optional\n\n@dataclass\nclass EmbeddingMetrics:\n    \"\"\"Track embedding generation metrics.\"\"\"\n    total_texts: int = 0\n    successful_embeddings: int = 0\n    failed_embeddings: int = 0\n    total_time: float = 0.0\n    total_tokens: int = 0\n\n    @property\n    def success_rate(self) -&gt; float:\n        return self.successful_embeddings / self.total_texts if self.total_texts &gt; 0 else 0.0\n\n    @property\n    def average_time_per_embedding(self) -&gt; float:\n        return self.total_time / self.successful_embeddings if self.successful_embeddings &gt; 0 else 0.0\n\nclass MetricsTracker:\n    \"\"\"Track embedding metrics.\"\"\"\n\n    def __init__(self):\n        self.metrics = EmbeddingMetrics()\n\n    async def track_embedding(self, embedder, text: str) -&gt; Optional[List[float]]:\n        \"\"\"Generate embedding and track metrics.\"\"\"\n        self.metrics.total_texts += 1\n        start_time = time.time()\n\n        try:\n            embedding = await embedder.embed_text(text)\n\n            self.metrics.successful_embeddings += 1\n            self.metrics.total_time += time.time() - start_time\n            self.metrics.total_tokens += len(text.split())  # Rough estimate\n\n            return embedding\n\n        except Exception as e:\n            self.metrics.failed_embeddings += 1\n            print(f\"Embedding failed: {e}\")\n            return None\n\n    def print_summary(self):\n        \"\"\"Print metrics summary.\"\"\"\n        print(f\"\ud83d\udcca Embedding Metrics Summary:\")\n        print(f\"   Total texts: {self.metrics.total_texts}\")\n        print(f\"   Success rate: {self.metrics.success_rate:.1%}\")\n        print(f\"   Average time: {self.metrics.average_time_per_embedding:.3f}s per embedding\")\n        print(f\"   Total time: {self.metrics.total_time:.2f}s\")\n\n# Usage\ntracker = MetricsTracker()\nembedder = EmbeddingFactory.create_from_env(\"openai\")\n\ntexts = [\"Sample text 1\", \"Sample text 2\", \"Sample text 3\"]\nfor text in texts:\n    embedding = await tracker.track_embedding(embedder, text)\n\ntracker.print_summary()\n</code></pre> <p>For production deployments and advanced integration patterns, see the Integration Examples and Deployment Guide.</p>"},{"location":"api/OLLAMA_PROVIDER/","title":"Ollama Provider Documentation","text":""},{"location":"api/OLLAMA_PROVIDER/#overview","title":"Overview","text":"<p>The Ollama provider enables local LLM and embedding model execution without external API dependencies. It connects to a locally running Ollama instance, providing privacy-focused AI capabilities with no usage costs or rate limits.</p>"},{"location":"api/OLLAMA_PROVIDER/#installation-requirements","title":"Installation Requirements","text":"<p>First, install Ollama on your system:</p>"},{"location":"api/OLLAMA_PROVIDER/#system-installation","title":"System Installation","text":"<p>macOS: <pre><code>brew install ollama\n</code></pre></p> <p>Linux: <pre><code>curl -fsSL https://ollama.ai/install.sh | sh\n</code></pre></p> <p>Windows: Download and install from https://ollama.ai/download</p>"},{"location":"api/OLLAMA_PROVIDER/#start-ollama-service","title":"Start Ollama Service","text":"<pre><code># Start Ollama service\nollama serve\n\n# In another terminal, pull your first model\nollama pull llama2\nollama pull nomic-embed-text  # For embeddings\n</code></pre>"},{"location":"api/OLLAMA_PROVIDER/#python-package","title":"Python Package","text":"<pre><code>pip install bruno-llm  # Ollama support included by default\n</code></pre>"},{"location":"api/OLLAMA_PROVIDER/#quick-start","title":"Quick Start","text":""},{"location":"api/OLLAMA_PROVIDER/#basic-llm-usage","title":"Basic LLM Usage","text":"<pre><code>from bruno_llm.providers.ollama import OllamaProvider\nfrom bruno_core.models import Message, MessageRole\n\n# Initialize the provider (assumes Ollama running on localhost:11434)\nprovider = OllamaProvider(\n    base_url=\"http://localhost:11434\",\n    model=\"llama2\"  # or any model you've pulled\n)\n\n# Create a conversation\nmessages = [\n    Message(role=MessageRole.SYSTEM, content=\"You are a helpful assistant.\"),\n    Message(role=MessageRole.USER, content=\"Explain the benefits of local AI models.\")\n]\n\n# Generate response\nresponse = await provider.generate(messages, temperature=0.7, max_tokens=500)\nprint(response)\n\n# Stream response for real-time output\nprint(\"Streaming response:\")\nasync for chunk in provider.stream(messages, temperature=0.7):\n    print(chunk, end=\"\", flush=True)\n</code></pre>"},{"location":"api/OLLAMA_PROVIDER/#basic-embedding-usage","title":"Basic Embedding Usage","text":"<pre><code>from bruno_llm.providers.ollama import OllamaEmbeddingProvider\n\n# Initialize embedding provider\nembedder = OllamaEmbeddingProvider(\n    base_url=\"http://localhost:11434\",\n    model=\"nomic-embed-text\"  # Make sure this model is pulled\n)\n\n# Single text embedding\ntext = \"Local AI models provide privacy and control\"\nembedding = await embedder.embed_text(text)\nprint(f\"Embedding dimension: {len(embedding)}\")\n\n# Batch embeddings\ntexts = [\n    \"Ollama runs models locally\",\n    \"No internet required for inference\",\n    \"Complete data privacy\"\n]\nembeddings = await embedder.embed_texts(texts)\nprint(f\"Generated {len(embeddings)} embeddings\")\n\n# Calculate similarity\nsimilarity = embedder.calculate_similarity(embeddings[0], embeddings[1])\nprint(f\"Similarity: {similarity:.4f}\")\n</code></pre>"},{"location":"api/OLLAMA_PROVIDER/#configuration","title":"Configuration","text":""},{"location":"api/OLLAMA_PROVIDER/#llm-configuration","title":"LLM Configuration","text":"<pre><code>from bruno_llm.providers.ollama import OllamaConfig, OllamaProvider\n\n# Detailed configuration\nconfig = OllamaConfig(\n    base_url=\"http://localhost:11434\",\n    model=\"llama2:13b\",  # Specify model variant\n    timeout=60.0,        # Longer timeout for larger models\n    keep_alive=\"10m\",    # Keep model loaded for 10 minutes\n    num_ctx=4096,        # Context window size\n    num_predict=512,     # Max tokens to generate\n    repeat_penalty=1.1,  # Reduce repetition\n    temperature=0.8,     # Default temperature\n    top_k=40,           # Top-k sampling\n    top_p=0.9           # Top-p sampling\n)\n\nprovider = OllamaProvider(config=config)\n</code></pre>"},{"location":"api/OLLAMA_PROVIDER/#embedding-configuration","title":"Embedding Configuration","text":"<pre><code>from bruno_llm.providers.ollama import OllamaEmbeddingConfig, OllamaEmbeddingProvider\n\n# Embedding configuration\nconfig = OllamaEmbeddingConfig(\n    base_url=\"http://localhost:11434\",\n    model=\"mxbai-embed-large\",\n    timeout=30.0,\n    batch_size=16  # Process fewer texts at once for larger models\n)\n\nembedder = OllamaEmbeddingProvider(config=config)\n</code></pre>"},{"location":"api/OLLAMA_PROVIDER/#environment-variables","title":"Environment Variables","text":"<p>Set up your environment for automatic configuration:</p> <pre><code># Ollama service configuration\nexport OLLAMA_BASE_URL=\"http://localhost:11434\"\nexport OLLAMA_HOST=\"0.0.0.0:11434\"  # For Ollama service itself\n\n# Bruno-LLM Ollama provider settings\nexport OLLAMA_MODEL=\"llama2\"\nexport OLLAMA_TIMEOUT=\"60.0\"\nexport OLLAMA_KEEP_ALIVE=\"5m\"\nexport OLLAMA_NUM_CTX=\"4096\"\n\n# Embedding settings\nexport OLLAMA_EMBEDDING_MODEL=\"nomic-embed-text\"\nexport EMBEDDING_TIMEOUT=\"30.0\"\nexport EMBEDDING_BATCH_SIZE=\"32\"\n</code></pre> <p>Create providers from environment:</p> <pre><code>from bruno_llm.factory import LLMFactory\nfrom bruno_llm.embedding_factory import EmbeddingFactory\n\n# Auto-configure from environment\nllm = LLMFactory.create_from_env(\"ollama\")\nembedder = EmbeddingFactory.create_from_env(\"ollama\")\n</code></pre>"},{"location":"api/OLLAMA_PROVIDER/#supported-models","title":"Supported Models","text":""},{"location":"api/OLLAMA_PROVIDER/#popular-llm-models","title":"Popular LLM Models","text":"Model Size Description Pull Command <code>llama2</code> 7B Meta's Llama 2 base model <code>ollama pull llama2</code> <code>llama2:13b</code> 13B Larger Llama 2 variant <code>ollama pull llama2:13b</code> <code>llama2:70b</code> 70B Largest Llama 2 model <code>ollama pull llama2:70b</code> <code>codellama</code> 7B Code-specialized Llama <code>ollama pull codellama</code> <code>codellama:13b</code> 13B Larger code model <code>ollama pull codellama:13b</code> <code>mistral</code> 7B Mistral AI's efficient model <code>ollama pull mistral</code> <code>mixtral</code> 8x7B Mixture of experts model <code>ollama pull mixtral</code> <code>neural-chat</code> 7B Intel's conversational model <code>ollama pull neural-chat</code> <code>starling-lm</code> 7B UC Berkeley's model <code>ollama pull starling-lm</code> <code>dolphin-mixtral</code> 8x7B Uncensored Mixtral variant <code>ollama pull dolphin-mixtral</code>"},{"location":"api/OLLAMA_PROVIDER/#embedding-models","title":"Embedding Models","text":"Model Dimensions Description Pull Command <code>nomic-embed-text</code> 768 Nomic AI's embedding model <code>ollama pull nomic-embed-text</code> <code>mxbai-embed-large</code> 1024 MixedBread AI's large embeddings <code>ollama pull mxbai-embed-large</code> <code>snowflake-arctic-embed</code> 1024 Snowflake's Arctic embeddings <code>ollama pull snowflake-arctic-embed</code> <code>all-minilm</code> 384 Sentence Transformers compact model <code>ollama pull all-minilm</code>"},{"location":"api/OLLAMA_PROVIDER/#model-management","title":"Model Management","text":"<pre><code># List available models\nmodels = await provider.list_models()\nprint(\"Available models:\", models)\n\n# Check if a specific model is available\nif \"llama2:13b\" in models:\n    # Switch to a different model\n    provider = OllamaProvider(model=\"llama2:13b\")\nelse:\n    print(\"Model not found. Pull it first:\")\n    print(\"ollama pull llama2:13b\")\n</code></pre>"},{"location":"api/OLLAMA_PROVIDER/#advanced-features","title":"Advanced Features","text":""},{"location":"api/OLLAMA_PROVIDER/#custom-model-parameters","title":"Custom Model Parameters","text":"<pre><code># Advanced generation parameters\nresponse = await provider.generate(\n    messages=messages,\n    temperature=0.8,        # Creativity (0.0-2.0)\n    max_tokens=1000,        # Maps to num_predict\n    top_k=40,              # Top-k sampling\n    top_p=0.9,             # Top-p sampling\n    repeat_penalty=1.1,     # Reduce repetition\n    seed=42,               # Reproducible outputs\n    stop=[\"\\n\\n\", \"###\"]   # Stop sequences\n)\n</code></pre>"},{"location":"api/OLLAMA_PROVIDER/#streaming-with-custom-processing","title":"Streaming with Custom Processing","text":"<pre><code>import asyncio\n\nasync def stream_with_processing():\n    \"\"\"Stream with real-time processing and statistics.\"\"\"\n\n    messages = [Message(role=MessageRole.USER, content=\"Write a creative story\")]\n\n    word_count = 0\n    char_count = 0\n    start_time = time.time()\n\n    print(\"Streaming response:\")\n    print(\"-\" * 50)\n\n    async for chunk in provider.stream(messages, temperature=0.9):\n        print(chunk, end=\"\", flush=True)\n\n        # Update statistics\n        word_count += len(chunk.split())\n        char_count += len(chunk)\n\n        # Show progress every 50 characters\n        if char_count % 50 == 0:\n            elapsed = time.time() - start_time\n            print(f\"\\n[Stats: {word_count} words, {char_count} chars, {elapsed:.1f}s]\", end=\"\")\n\n    total_time = time.time() - start_time\n    print(f\"\\n\\n\ud83d\udcca Final stats:\")\n    print(f\"   Words: {word_count}\")\n    print(f\"   Characters: {char_count}\")\n    print(f\"   Time: {total_time:.2f}s\")\n    print(f\"   Speed: {word_count/total_time:.1f} words/sec\")\n</code></pre>"},{"location":"api/OLLAMA_PROVIDER/#model-performance-optimization","title":"Model Performance Optimization","text":"<pre><code>async def optimize_model_performance():\n    \"\"\"Configure Ollama for optimal performance.\"\"\"\n\n    # For faster response (lower quality)\n    fast_provider = OllamaProvider(\n        model=\"llama2\",\n        num_ctx=2048,      # Smaller context window\n        num_predict=256,   # Fewer tokens\n        top_k=20,         # More focused sampling\n        temperature=0.3    # Less randomness\n    )\n\n    # For higher quality (slower)\n    quality_provider = OllamaProvider(\n        model=\"llama2:13b\",  # Larger model\n        num_ctx=4096,        # Full context\n        num_predict=1000,    # More tokens\n        temperature=0.7,     # Balanced creativity\n        top_p=0.95          # Diverse sampling\n    )\n\n    # Choose based on use case\n    provider = fast_provider if need_speed else quality_provider\n\n    return await provider.generate(messages)\n</code></pre>"},{"location":"api/OLLAMA_PROVIDER/#batch-processing-for-embeddings","title":"Batch Processing for Embeddings","text":"<pre><code>async def process_document_embeddings(documents: List[str]):\n    \"\"\"Process large document collections efficiently.\"\"\"\n\n    embedder = OllamaEmbeddingProvider(\n        model=\"nomic-embed-text\",\n        batch_size=16  # Conservative batch size\n    )\n\n    all_embeddings = []\n\n    print(f\"Processing {len(documents)} documents...\")\n\n    for i in range(0, len(documents), embedder.batch_size):\n        batch = documents[i:i + embedder.batch_size]\n\n        try:\n            # Process batch\n            batch_embeddings = await embedder.embed_texts(batch)\n            all_embeddings.extend(batch_embeddings)\n\n            # Progress update\n            progress = len(all_embeddings) / len(documents) * 100\n            print(f\"Progress: {progress:.1f}% ({len(all_embeddings)}/{len(documents)})\")\n\n            # Small delay to prevent overwhelming Ollama\n            await asyncio.sleep(0.1)\n\n        except Exception as e:\n            print(f\"Error processing batch {i}: {e}\")\n            # Add empty embeddings for failed batch to maintain alignment\n            all_embeddings.extend([None] * len(batch))\n\n    return all_embeddings\n</code></pre>"},{"location":"api/OLLAMA_PROVIDER/#error-handling","title":"Error Handling","text":""},{"location":"api/OLLAMA_PROVIDER/#connection-and-model-management","title":"Connection and Model Management","text":"<pre><code>from bruno_llm.exceptions import (\n    ModelNotFoundError,\n    LLMTimeoutError,\n    LLMError\n)\n\nasync def robust_ollama_usage():\n    \"\"\"Handle common Ollama issues gracefully.\"\"\"\n\n    provider = OllamaProvider(model=\"llama2\")\n\n    # Check Ollama service connectivity\n    try:\n        is_connected = await provider.check_connection()\n        if not is_connected:\n            print(\"\u274c Ollama service not running. Start with: ollama serve\")\n            return None\n    except Exception as e:\n        print(f\"\u274c Cannot connect to Ollama: {e}\")\n        print(\"Ensure Ollama is installed and running:\")\n        print(\"  1. Install: curl -fsSL https://ollama.ai/install.sh | sh\")\n        print(\"  2. Start: ollama serve\")\n        return None\n\n    # Check model availability\n    try:\n        models = await provider.list_models()\n        if \"llama2\" not in models:\n            print(\"\u274c llama2 model not found. Pull it first:\")\n            print(\"  ollama pull llama2\")\n            return None\n    except Exception as e:\n        print(f\"\u26a0\ufe0f  Could not list models: {e}\")\n\n    # Generate with error handling\n    try:\n        messages = [Message(role=MessageRole.USER, content=\"Hello!\")]\n        response = await provider.generate(messages, timeout=30.0)\n        print(f\"\u2705 Response: {response}\")\n        return response\n\n    except ModelNotFoundError:\n        print(\"\u274c Model not loaded. Try: ollama pull llama2\")\n    except LLMTimeoutError:\n        print(\"\u23f0 Request timed out. Large models may need more time.\")\n    except LLMError as e:\n        print(f\"\u274c Generation error: {e}\")\n\n    return None\n</code></pre>"},{"location":"api/OLLAMA_PROVIDER/#memory-and-resource-management","title":"Memory and Resource Management","text":"<pre><code>async def monitor_ollama_resources():\n    \"\"\"Monitor and manage Ollama resource usage.\"\"\"\n\n    import psutil\n    import subprocess\n\n    # Check system resources\n    memory = psutil.virtual_memory()\n    if memory.percent &gt; 85:\n        print(f\"\u26a0\ufe0f  High memory usage: {memory.percent:.1f}%\")\n        print(\"Consider using a smaller model or increasing RAM\")\n\n    # Check if Ollama is running\n    try:\n        result = subprocess.run([\"pgrep\", \"ollama\"], capture_output=True)\n        if result.returncode != 0:\n            print(\"\u274c Ollama process not found\")\n            return False\n    except FileNotFoundError:\n        print(\"\u2139\ufe0f  Cannot check process status (pgrep not available)\")\n\n    # Check GPU availability (if using CUDA)\n    try:\n        result = subprocess.run([\"nvidia-smi\"], capture_output=True, text=True)\n        if result.returncode == 0:\n            print(\"\u2705 CUDA GPU available\")\n        else:\n            print(\"\u2139\ufe0f  No CUDA GPU found, using CPU\")\n    except FileNotFoundError:\n        print(\"\u2139\ufe0f  nvidia-smi not found, likely using CPU\")\n\n    return True\n</code></pre>"},{"location":"api/OLLAMA_PROVIDER/#performance-optimization","title":"Performance Optimization","text":""},{"location":"api/OLLAMA_PROVIDER/#model-selection-strategy","title":"Model Selection Strategy","text":"<pre><code>def select_optimal_model(task_type: str, performance_priority: str = \"balanced\"):\n    \"\"\"Select the best model for a specific task.\"\"\"\n\n    model_recommendations = {\n        \"chat\": {\n            \"speed\": \"llama2\",\n            \"balanced\": \"mistral\",\n            \"quality\": \"llama2:13b\"\n        },\n        \"code\": {\n            \"speed\": \"codellama\",\n            \"balanced\": \"codellama:13b\",\n            \"quality\": \"codellama:34b\"\n        },\n        \"creative\": {\n            \"speed\": \"neural-chat\",\n            \"balanced\": \"starling-lm\",\n            \"quality\": \"mixtral\"\n        },\n        \"analysis\": {\n            \"speed\": \"mistral\",\n            \"balanced\": \"llama2:13b\",\n            \"quality\": \"mixtral\"\n        }\n    }\n\n    return model_recommendations.get(task_type, {}).get(performance_priority, \"llama2\")\n\n# Usage\nmodel = select_optimal_model(\"code\", \"quality\")\nprovider = OllamaProvider(model=model)\n</code></pre>"},{"location":"api/OLLAMA_PROVIDER/#connection-pooling-and-reuse","title":"Connection Pooling and Reuse","text":"<pre><code>class OllamaManager:\n    \"\"\"Manage multiple Ollama providers efficiently.\"\"\"\n\n    def __init__(self):\n        self.providers = {}\n\n    def get_provider(self, model: str, **config):\n        \"\"\"Get or create a provider for the specified model.\"\"\"\n        if model not in self.providers:\n            self.providers[model] = OllamaProvider(model=model, **config)\n        return self.providers[model]\n\n    async def cleanup(self):\n        \"\"\"Clean up all providers.\"\"\"\n        for provider in self.providers.values():\n            await provider.__aexit__(None, None, None)\n        self.providers.clear()\n\n# Global manager instance\nollama_manager = OllamaManager()\n\n# Usage\nasync def multi_model_processing():\n    # Different models for different tasks\n    chat_provider = ollama_manager.get_provider(\"mistral\")\n    code_provider = ollama_manager.get_provider(\"codellama\")\n\n    # Use providers without recreating connections\n    chat_response = await chat_provider.generate(chat_messages)\n    code_response = await code_provider.generate(code_messages)\n\n    return chat_response, code_response\n</code></pre>"},{"location":"api/OLLAMA_PROVIDER/#concurrent-processing","title":"Concurrent Processing","text":"<pre><code>async def concurrent_ollama_requests():\n    \"\"\"Process multiple requests concurrently with Ollama.\"\"\"\n\n    # Note: Ollama typically processes requests sequentially\n    # But you can prepare multiple providers or use async queuing\n\n    provider = OllamaProvider(model=\"llama2\")\n\n    requests = [\n        [Message(role=MessageRole.USER, content=\"Explain AI\")],\n        [Message(role=MessageRole.USER, content=\"What is Python?\")],\n        [Message(role=MessageRole.USER, content=\"How do computers work?\")]\n    ]\n\n    # Process with controlled concurrency\n    semaphore = asyncio.Semaphore(2)  # Max 2 concurrent requests\n\n    async def process_request(messages):\n        async with semaphore:\n            return await provider.generate(messages, max_tokens=200)\n\n    # Process all requests\n    responses = await asyncio.gather(*[\n        process_request(messages) for messages in requests\n    ])\n\n    return responses\n</code></pre>"},{"location":"api/OLLAMA_PROVIDER/#integration-examples","title":"Integration Examples","text":""},{"location":"api/OLLAMA_PROVIDER/#local-rag-retrieval-augmented-generation","title":"Local RAG (Retrieval-Augmented Generation)","text":"<pre><code>from typing import List, Tuple\nimport numpy as np\n\nclass LocalRAGSystem:\n    \"\"\"Privacy-focused RAG using only local Ollama models.\"\"\"\n\n    def __init__(self):\n        self.llm = OllamaProvider(model=\"mistral\")\n        self.embedder = OllamaEmbeddingProvider(model=\"nomic-embed-text\")\n        self.knowledge_base = []  # (text, embedding) pairs\n\n    async def add_document(self, text: str):\n        \"\"\"Add a document to the knowledge base.\"\"\"\n        embedding = await self.embedder.embed_text(text)\n        self.knowledge_base.append((text, embedding))\n\n    async def search_similar(self, query: str, top_k: int = 3) -&gt; List[str]:\n        \"\"\"Find most similar documents to the query.\"\"\"\n        query_embedding = await self.embedder.embed_text(query)\n\n        similarities = []\n        for text, doc_embedding in self.knowledge_base:\n            similarity = self.embedder.calculate_similarity(query_embedding, doc_embedding)\n            similarities.append((text, similarity))\n\n        # Sort by similarity and return top_k\n        similarities.sort(key=lambda x: x[1], reverse=True)\n        return [text for text, _ in similarities[:top_k]]\n\n    async def answer_question(self, question: str) -&gt; str:\n        \"\"\"Answer a question using RAG.\"\"\"\n        # Find relevant documents\n        relevant_docs = await self.search_similar(question)\n\n        # Create context\n        context = \"\\n\\n\".join(relevant_docs)\n\n        # Generate answer\n        messages = [\n            Message(role=MessageRole.SYSTEM, content=\n                \"Answer the question based only on the provided context. \"\n                \"If the context doesn't contain the information, say so.\"),\n            Message(role=MessageRole.USER, content=f\"Context:\\n{context}\\n\\nQuestion: {question}\")\n        ]\n\n        return await self.llm.generate(messages)\n\n# Usage\nrag = LocalRAGSystem()\n\n# Add documents\nawait rag.add_document(\"Ollama is a tool for running large language models locally.\")\nawait rag.add_document(\"Local AI models provide privacy and data security.\")\n\n# Ask questions\nanswer = await rag.answer_question(\"What are the benefits of local AI?\")\nprint(answer)\n</code></pre>"},{"location":"api/OLLAMA_PROVIDER/#document-processing-pipeline","title":"Document Processing Pipeline","text":"<pre><code>async def process_documents_locally(file_paths: List[str]):\n    \"\"\"Process documents entirely with local models.\"\"\"\n\n    llm = OllamaProvider(model=\"llama2\")\n    embedder = OllamaEmbeddingProvider(model=\"nomic-embed-text\")\n\n    results = []\n\n    for file_path in file_paths:\n        print(f\"\ud83d\udcc4 Processing {file_path}...\")\n\n        # Read document\n        with open(file_path, 'r', encoding='utf-8') as f:\n            content = f.read()\n\n        # Generate summary\n        summary_messages = [\n            Message(role=MessageRole.SYSTEM, content=\n                \"Summarize the following document in 2-3 sentences.\"),\n            Message(role=MessageRole.USER, content=content[:4000])  # Truncate if needed\n        ]\n\n        summary = await llm.generate(summary_messages, max_tokens=200)\n\n        # Generate embedding for semantic search\n        embedding = await embedder.embed_text(summary)\n\n        # Extract key topics\n        topics_messages = [\n            Message(role=MessageRole.SYSTEM, content=\n                \"Extract 3-5 key topics from the document as a comma-separated list.\"),\n            Message(role=MessageRole.USER, content=content[:4000])\n        ]\n\n        topics = await llm.generate(topics_messages, max_tokens=100)\n\n        results.append({\n            'file': file_path,\n            'summary': summary,\n            'topics': topics.split(', '),\n            'embedding': embedding\n        })\n\n        print(f\"\u2705 Completed {file_path}\")\n\n    return results\n\n# Usage\ndocuments = [\"doc1.txt\", \"doc2.txt\", \"doc3.txt\"]\nprocessed = await process_documents_locally(documents)\n\nfor doc in processed:\n    print(f\"\\n\ud83d\udcc4 {doc['file']}\")\n    print(f\"Summary: {doc['summary']}\")\n    print(f\"Topics: {', '.join(doc['topics'])}\")\n</code></pre>"},{"location":"api/OLLAMA_PROVIDER/#best-practices","title":"Best Practices","text":""},{"location":"api/OLLAMA_PROVIDER/#1-model-management","title":"1. Model Management","text":"<pre><code># Check model availability before use\nasync def ensure_model_available(model_name: str):\n    \"\"\"Ensure a model is available, pull if necessary.\"\"\"\n\n    provider = OllamaProvider()\n\n    try:\n        models = await provider.list_models()\n        if model_name not in models:\n            print(f\"Model {model_name} not found. Please pull it:\")\n            print(f\"ollama pull {model_name}\")\n            return False\n        return True\n    except Exception as e:\n        print(f\"Could not check models: {e}\")\n        return False\n\n# Use before creating providers\nif await ensure_model_available(\"llama2\"):\n    provider = OllamaProvider(model=\"llama2\")\n</code></pre>"},{"location":"api/OLLAMA_PROVIDER/#2-resource-management","title":"2. Resource Management","text":"<pre><code># Monitor system resources\nimport psutil\n\ndef check_system_resources():\n    \"\"\"Check if system can handle the model.\"\"\"\n    memory = psutil.virtual_memory()\n\n    if memory.available &lt; 4 * 1024**3:  # Less than 4GB available\n        print(\"\u26a0\ufe0f  Low memory. Consider using a smaller model.\")\n        return \"small\"\n    elif memory.available &lt; 16 * 1024**3:  # Less than 16GB\n        return \"medium\"\n    else:\n        return \"large\"\n\n# Choose model based on resources\nresource_level = check_system_resources()\nmodels = {\n    \"small\": \"llama2\",\n    \"medium\": \"llama2:13b\",\n    \"large\": \"llama2:70b\"\n}\n\nprovider = OllamaProvider(model=models[resource_level])\n</code></pre>"},{"location":"api/OLLAMA_PROVIDER/#3-error-recovery","title":"3. Error Recovery","text":"<pre><code>async def resilient_generation(provider, messages, max_retries=3):\n    \"\"\"Generate with automatic retry and fallback.\"\"\"\n\n    for attempt in range(max_retries):\n        try:\n            return await provider.generate(messages)\n\n        except LLMTimeoutError:\n            if attempt &lt; max_retries - 1:\n                print(f\"Timeout on attempt {attempt + 1}, retrying...\")\n                await asyncio.sleep(2 ** attempt)  # Exponential backoff\n            else:\n                # Fallback to smaller model\n                print(\"Falling back to smaller model...\")\n                fallback_provider = OllamaProvider(model=\"llama2\")\n                return await fallback_provider.generate(messages)\n\n        except Exception as e:\n            print(f\"Error on attempt {attempt + 1}: {e}\")\n            if attempt == max_retries - 1:\n                raise\n            await asyncio.sleep(1)\n</code></pre>"},{"location":"api/OLLAMA_PROVIDER/#troubleshooting","title":"Troubleshooting","text":""},{"location":"api/OLLAMA_PROVIDER/#common-issues","title":"Common Issues","text":"<ol> <li> <p>Ollama Service Not Running <pre><code># Check if Ollama is running\nps aux | grep ollama\n\n# Start Ollama\nollama serve\n\n# Or run in background\nnohup ollama serve &gt; ollama.log 2&gt;&amp;1 &amp;\n</code></pre></p> </li> <li> <p>Model Not Found <pre><code># List available models\nollama list\n\n# Pull a model\nollama pull llama2\n\n# Remove unused models to save space\nollama rm unused-model\n</code></pre></p> </li> <li> <p>Memory Issues <pre><code># Monitor memory usage\nasync def check_memory_usage():\n    import psutil\n    memory = psutil.virtual_memory()\n    print(f\"Memory usage: {memory.percent}%\")\n    print(f\"Available: {memory.available / 1024**3:.1f} GB\")\n</code></pre></p> </li> <li> <p>Slow Performance <pre><code># Optimize for speed\nfast_provider = OllamaProvider(\n    model=\"llama2\",      # Use smaller model\n    num_ctx=2048,        # Reduce context\n    num_predict=256,     # Limit output\n    temperature=0.3      # Reduce randomness\n)\n</code></pre></p> </li> </ol>"},{"location":"api/OLLAMA_PROVIDER/#performance-monitoring","title":"Performance Monitoring","text":"<pre><code>import time\nimport asyncio\n\nasync def benchmark_ollama():\n    \"\"\"Benchmark Ollama performance.\"\"\"\n\n    provider = OllamaProvider(model=\"llama2\")\n\n    # Test message\n    messages = [Message(role=MessageRole.USER, content=\"Explain AI in one paragraph.\")]\n\n    # Benchmark generation\n    start_time = time.time()\n    response = await provider.generate(messages, max_tokens=100)\n    generation_time = time.time() - start_time\n\n    # Calculate metrics\n    tokens = len(response.split())\n    tokens_per_second = tokens / generation_time\n\n    print(f\"\ud83d\udcca Performance Metrics:\")\n    print(f\"   Response time: {generation_time:.2f}s\")\n    print(f\"   Tokens generated: {tokens}\")\n    print(f\"   Speed: {tokens_per_second:.1f} tokens/sec\")\n    print(f\"   Response: {response[:100]}...\")\n\n    return {\n        'time': generation_time,\n        'tokens': tokens,\n        'speed': tokens_per_second\n    }\n\n# Run benchmark\nresults = await benchmark_ollama()\n</code></pre> <p>For more examples and deployment scenarios, see the Bruno-LLM Examples directory.</p>"},{"location":"api/OPENAI_PROVIDER/","title":"OpenAI Provider Documentation","text":""},{"location":"api/OPENAI_PROVIDER/#overview","title":"Overview","text":"<p>The OpenAI provider offers seamless integration with OpenAI's GPT models and embedding services through the official OpenAI Python SDK. It supports all major GPT models and provides advanced features like cost tracking, streaming, and comprehensive error handling.</p>"},{"location":"api/OPENAI_PROVIDER/#installation-requirements","title":"Installation Requirements","text":"<pre><code>pip install bruno-llm[openai]\n# or\npip install openai\n</code></pre>"},{"location":"api/OPENAI_PROVIDER/#quick-start","title":"Quick Start","text":""},{"location":"api/OPENAI_PROVIDER/#basic-llm-usage","title":"Basic LLM Usage","text":"<pre><code>from bruno_llm.providers.openai import OpenAIProvider\nfrom bruno_core.models import Message, MessageRole\n\n# Initialize the provider\nprovider = OpenAIProvider(\n    api_key=\"sk-your-openai-key\",\n    model=\"gpt-4\"\n)\n\n# Create a conversation\nmessages = [\n    Message(role=MessageRole.SYSTEM, content=\"You are a helpful AI assistant.\"),\n    Message(role=MessageRole.USER, content=\"Explain quantum computing in simple terms.\")\n]\n\n# Generate response\nresponse = await provider.generate(messages, temperature=0.7, max_tokens=500)\nprint(response)\n\n# Stream response for real-time output\nprint(\"Streaming response:\")\nasync for chunk in provider.stream(messages, temperature=0.7):\n    print(chunk, end=\"\", flush=True)\n</code></pre>"},{"location":"api/OPENAI_PROVIDER/#basic-embedding-usage","title":"Basic Embedding Usage","text":"<pre><code>from bruno_llm.providers.openai import OpenAIEmbeddingProvider\n\n# Initialize embedding provider\nembedder = OpenAIEmbeddingProvider(\n    api_key=\"sk-your-openai-key\",\n    model=\"text-embedding-ada-002\"\n)\n\n# Single text embedding\ntext = \"The quick brown fox jumps over the lazy dog\"\nembedding = await embedder.embed_text(text)\nprint(f\"Embedding dimension: {len(embedding)}\")\n\n# Batch embeddings\ntexts = [\n    \"Machine learning is fascinating\",\n    \"Deep learning uses neural networks\",\n    \"AI will transform many industries\"\n]\nembeddings = await embedder.embed_texts(texts)\nprint(f\"Generated {len(embeddings)} embeddings\")\n\n# Calculate similarity\nsimilarity = embedder.calculate_similarity(embeddings[0], embeddings[1])\nprint(f\"Similarity between first two texts: {similarity:.4f}\")\n</code></pre>"},{"location":"api/OPENAI_PROVIDER/#configuration","title":"Configuration","text":""},{"location":"api/OPENAI_PROVIDER/#llm-configuration","title":"LLM Configuration","text":"<pre><code>from bruno_llm.providers.openai import OpenAIConfig, OpenAIProvider\n\n# Detailed configuration\nconfig = OpenAIConfig(\n    api_key=\"sk-your-openai-key\",\n    model=\"gpt-4\",\n    organization=\"org-your-org-id\",  # Optional\n    base_url=\"https://api.openai.com/v1\",  # Custom endpoint if needed\n    timeout=30.0,\n    max_retries=3,\n    track_cost=True,\n    default_temperature=0.7,\n    default_max_tokens=1000\n)\n\nprovider = OpenAIProvider(config=config)\n</code></pre>"},{"location":"api/OPENAI_PROVIDER/#embedding-configuration","title":"Embedding Configuration","text":"<pre><code>from bruno_llm.providers.openai import OpenAIEmbeddingConfig, OpenAIEmbeddingProvider\n\n# Embedding configuration\nconfig = OpenAIEmbeddingConfig(\n    api_key=\"sk-your-openai-key\",\n    model=\"text-embedding-3-large\",\n    dimensions=1024,  # For v3 models, can reduce dimensions\n    batch_size=100,   # Process up to 100 texts at once\n    timeout=30.0,\n    organization=\"org-your-org-id\"\n)\n\nembedder = OpenAIEmbeddingProvider(config=config)\n</code></pre>"},{"location":"api/OPENAI_PROVIDER/#environment-variables","title":"Environment Variables","text":"<p>Set up your environment for seamless configuration:</p> <pre><code># Required\nexport OPENAI_API_KEY=\"sk-your-openai-key\"\n\n# Optional LLM settings\nexport OPENAI_MODEL=\"gpt-4\"\nexport OPENAI_ORG_ID=\"org-your-org-id\"\nexport OPENAI_BASE_URL=\"https://api.openai.com/v1\"\nexport OPENAI_TIMEOUT=\"30.0\"\nexport OPENAI_MAX_RETRIES=\"3\"\n\n# Optional embedding settings\nexport OPENAI_EMBEDDING_MODEL=\"text-embedding-ada-002\"\nexport OPENAI_EMBEDDING_DIMENSIONS=\"1536\"\nexport OPENAI_EMBEDDING_BATCH_SIZE=\"100\"\n</code></pre> <p>Create providers from environment:</p> <pre><code>from bruno_llm.factory import LLMFactory\nfrom bruno_llm.embedding_factory import EmbeddingFactory\n\n# Auto-configure from environment\nllm = LLMFactory.create_from_env(\"openai\")\nembedder = EmbeddingFactory.create_from_env(\"openai\")\n</code></pre>"},{"location":"api/OPENAI_PROVIDER/#supported-models","title":"Supported Models","text":""},{"location":"api/OPENAI_PROVIDER/#gpt-models-llm","title":"GPT Models (LLM)","text":"Model Description Context Length Cost per 1K tokens <code>gpt-4</code> Most capable model 8,192 tokens $0.03 / $0.06 <code>gpt-4-turbo</code> Latest GPT-4 with 128K context 128,000 tokens $0.01 / $0.03 <code>gpt-4o</code> Optimized for speed and efficiency 128,000 tokens $0.005 / $0.015 <code>gpt-4o-mini</code> Fast and cost-effective 128,000 tokens $0.00015 / $0.0006 <code>gpt-3.5-turbo</code> Fast and efficient 16,385 tokens $0.0015 / $0.002"},{"location":"api/OPENAI_PROVIDER/#embedding-models","title":"Embedding Models","text":"Model Dimensions Max Input Cost per 1M tokens <code>text-embedding-ada-002</code> 1,536 8,191 tokens $0.10 <code>text-embedding-3-small</code> 1,536 (default) 8,191 tokens $0.02 <code>text-embedding-3-large</code> 3,072 (default) 8,191 tokens $0.13 <p>Note: For <code>text-embedding-3-*</code> models, you can specify custom dimensions (minimum 1).</p>"},{"location":"api/OPENAI_PROVIDER/#advanced-features","title":"Advanced Features","text":""},{"location":"api/OPENAI_PROVIDER/#cost-tracking","title":"Cost Tracking","text":"<pre><code>from bruno_llm.providers.openai import OpenAIProvider\n\n# Enable cost tracking\nprovider = OpenAIProvider(\n    api_key=\"sk-your-key\",\n    model=\"gpt-4\",\n    track_cost=True\n)\n\n# After making requests\nawait provider.generate(messages)\n\n# Get cost information\nmodel_info = provider.get_model_info()\nprint(f\"Total cost: ${model_info['total_cost']:.4f}\")\nprint(f\"Total tokens: {model_info['total_tokens']}\")\nprint(f\"Request count: {model_info['request_count']}\")\n\n# Cost breakdown by request\ncost_tracker = provider._cost_tracker\nfor record in cost_tracker.get_usage_history():\n    print(f\"Request: {record.input_tokens} + {record.output_tokens} tokens = ${record.total_cost:.4f}\")\n</code></pre>"},{"location":"api/OPENAI_PROVIDER/#streaming-with-real-time-processing","title":"Streaming with Real-time Processing","text":"<pre><code>import asyncio\n\nasync def process_stream_with_buffer():\n    messages = [Message(role=MessageRole.USER, content=\"Write a short story\")]\n\n    buffer = \"\"\n    word_count = 0\n\n    async for chunk in provider.stream(messages, temperature=0.8):\n        buffer += chunk\n\n        # Process complete words\n        if \" \" in chunk or \".\" in chunk:\n            words = buffer.split()\n            if len(words) &gt; 1:\n                # Process all complete words except the last (might be incomplete)\n                complete_words = words[:-1]\n                word_count += len(complete_words)\n                print(f\"Words processed: {word_count}\", end=\"\\r\")\n\n                # Keep the last potentially incomplete word\n                buffer = words[-1] if words else \"\"\n\n    # Process any remaining content\n    if buffer:\n        word_count += len(buffer.split())\n\n    print(f\"\\nTotal words: {word_count}\")\n</code></pre>"},{"location":"api/OPENAI_PROVIDER/#custom-headers-and-parameters","title":"Custom Headers and Parameters","text":"<pre><code># Custom API parameters\nresponse = await provider.generate(\n    messages=messages,\n    temperature=0.9,\n    max_tokens=2000,\n    top_p=0.95,\n    frequency_penalty=0.5,\n    presence_penalty=0.3,\n    stop=[\"\\n\\n\", \"###\"]\n)\n\n# Function calling (for compatible models)\nfunctions = [\n    {\n        \"name\": \"get_weather\",\n        \"description\": \"Get weather information\",\n        \"parameters\": {\n            \"type\": \"object\",\n            \"properties\": {\n                \"location\": {\"type\": \"string\"}\n            }\n        }\n    }\n]\n\nresponse = await provider.generate(\n    messages=messages,\n    functions=functions,\n    function_call=\"auto\"\n)\n</code></pre>"},{"location":"api/OPENAI_PROVIDER/#batch-processing-for-embeddings","title":"Batch Processing for Embeddings","text":"<pre><code>async def process_large_dataset(texts):\n    \"\"\"Process a large dataset of texts efficiently.\"\"\"\n    embedder = OpenAIEmbeddingProvider(\n        api_key=\"sk-your-key\",\n        batch_size=100  # Process 100 texts at once\n    )\n\n    all_embeddings = []\n\n    # Process in batches to respect rate limits\n    for i in range(0, len(texts), embedder.batch_size):\n        batch = texts[i:i + embedder.batch_size]\n\n        try:\n            batch_embeddings = await embedder.embed_texts(batch)\n            all_embeddings.extend(batch_embeddings)\n\n            print(f\"Processed {len(all_embeddings)}/{len(texts)} texts\")\n\n            # Rate limiting - small delay between batches\n            await asyncio.sleep(0.1)\n\n        except Exception as e:\n            print(f\"Error processing batch {i}: {e}\")\n            # Continue with next batch or implement retry logic\n\n    return all_embeddings\n\n# Usage\ntexts = [\"text 1\", \"text 2\", ...]  # Your large dataset\nembeddings = await process_large_dataset(texts)\n</code></pre>"},{"location":"api/OPENAI_PROVIDER/#error-handling","title":"Error Handling","text":""},{"location":"api/OPENAI_PROVIDER/#comprehensive-error-handling","title":"Comprehensive Error Handling","text":"<pre><code>from bruno_llm.exceptions import (\n    AuthenticationError,\n    RateLimitError,\n    ModelNotFoundError,\n    ContextLengthExceededError,\n    LLMError\n)\nimport asyncio\n\nasync def robust_generate(provider, messages, max_retries=3):\n    \"\"\"Generate response with comprehensive error handling.\"\"\"\n\n    for attempt in range(max_retries):\n        try:\n            return await provider.generate(messages)\n\n        except AuthenticationError:\n            print(\"\u274c Invalid API key. Please check your OpenAI API key.\")\n            raise  # Don't retry auth errors\n\n        except RateLimitError as e:\n            print(f\"\u23f3 Rate limit hit. Waiting {e.retry_after or 60} seconds...\")\n            await asyncio.sleep(e.retry_after or 60)\n\n        except ModelNotFoundError:\n            print(\"\u274c Model not found. Check if model name is correct.\")\n            raise  # Don't retry model errors\n\n        except ContextLengthExceededError:\n            print(\"\u26a0\ufe0f  Context too long. Try reducing message length or use a model with larger context.\")\n            # Could implement automatic truncation here\n            raise\n\n        except LLLError as e:\n            print(f\"\ud83d\udd04 LLM error (attempt {attempt + 1}/{max_retries}): {e}\")\n            if attempt == max_retries - 1:\n                raise\n            await asyncio.sleep(2 ** attempt)  # Exponential backoff\n\n    raise Exception(\"Max retries exceeded\")\n\n# Usage\ntry:\n    response = await robust_generate(provider, messages)\n    print(f\"\u2705 Success: {response}\")\nexcept Exception as e:\n    print(f\"\u274c Failed: {e}\")\n</code></pre>"},{"location":"api/OPENAI_PROVIDER/#connection-and-model-validation","title":"Connection and Model Validation","text":"<pre><code>async def validate_setup(provider):\n    \"\"\"Validate OpenAI provider setup.\"\"\"\n\n    print(\"\ud83d\udd0d Validating OpenAI setup...\")\n\n    # Check connection\n    try:\n        is_connected = await provider.check_connection()\n        if is_connected:\n            print(\"\u2705 Connection successful\")\n        else:\n            print(\"\u274c Connection failed\")\n            return False\n    except Exception as e:\n        print(f\"\u274c Connection error: {e}\")\n        return False\n\n    # List available models\n    try:\n        models = await provider.list_models()\n        print(f\"\u2705 Available models: {len(models)} found\")\n\n        # Check if current model is available\n        current_model = provider.get_model_info()[\"model\"]\n        if current_model in models:\n            print(f\"\u2705 Current model '{current_model}' is available\")\n        else:\n            print(f\"\u26a0\ufe0f  Current model '{current_model}' not found in available models\")\n\n    except Exception as e:\n        print(f\"\u274c Could not list models: {e}\")\n\n    # Test generation with a simple request\n    try:\n        test_messages = [Message(role=MessageRole.USER, content=\"Say 'test'\")]\n        response = await provider.generate(test_messages, max_tokens=10)\n        print(f\"\u2705 Test generation successful: '{response.strip()}'\")\n        return True\n    except Exception as e:\n        print(f\"\u274c Test generation failed: {e}\")\n        return False\n\n# Validate before using\nif await validate_setup(provider):\n    # Proceed with actual usage\n    pass\n</code></pre>"},{"location":"api/OPENAI_PROVIDER/#performance-optimization","title":"Performance Optimization","text":""},{"location":"api/OPENAI_PROVIDER/#connection-pooling-and-reuse","title":"Connection Pooling and Reuse","text":"<pre><code># Reuse provider instances for better performance\nclass OpenAIService:\n    def __init__(self, api_key: str):\n        self.provider = OpenAIProvider(api_key=api_key, track_cost=True)\n\n    async def __aenter__(self):\n        return self\n\n    async def __aexit__(self, exc_type, exc_val, exc_tb):\n        await self.provider.__aexit__(exc_type, exc_val, exc_tb)\n\n    async def generate_response(self, messages, **kwargs):\n        return await self.provider.generate(messages, **kwargs)\n\n# Usage with context manager\nasync with OpenAIService(\"sk-your-key\") as service:\n    # Multiple requests reuse the same connection\n    for conversation in conversations:\n        response = await service.generate_response(conversation)\n        process_response(response)\n</code></pre>"},{"location":"api/OPENAI_PROVIDER/#efficient-token-management","title":"Efficient Token Management","text":"<pre><code>async def smart_token_management(provider, long_conversation):\n    \"\"\"Manage long conversations by tracking tokens.\"\"\"\n\n    # Get token count for messages\n    total_tokens = sum(provider.get_token_count(msg.content) for msg in long_conversation)\n\n    model_info = provider.get_model_info()\n    max_tokens = model_info.get(\"max_context_tokens\", 4096)\n\n    if total_tokens &gt; max_tokens * 0.8:  # Use 80% of context as safety margin\n        print(f\"\u26a0\ufe0f  Conversation too long ({total_tokens} tokens). Truncating...\")\n\n        # Keep system message and recent messages\n        system_messages = [msg for msg in long_conversation if msg.role == MessageRole.SYSTEM]\n        user_assistant_messages = [msg for msg in long_conversation if msg.role != MessageRole.SYSTEM]\n\n        # Take last N messages that fit in context\n        truncated_messages = system_messages\n        current_tokens = sum(provider.get_token_count(msg.content) for msg in system_messages)\n\n        for msg in reversed(user_assistant_messages):\n            msg_tokens = provider.get_token_count(msg.content)\n            if current_tokens + msg_tokens &lt; max_tokens * 0.8:\n                truncated_messages.insert(-len(system_messages) if system_messages else 0, msg)\n                current_tokens += msg_tokens\n            else:\n                break\n\n        long_conversation = truncated_messages\n\n    return await provider.generate(long_conversation)\n</code></pre>"},{"location":"api/OPENAI_PROVIDER/#integration-examples","title":"Integration Examples","text":""},{"location":"api/OPENAI_PROVIDER/#with-asyncio-and-concurrent-requests","title":"With AsyncIO and Concurrent Requests","text":"<pre><code>import asyncio\nimport time\n\nasync def concurrent_generations():\n    \"\"\"Process multiple requests concurrently.\"\"\"\n\n    provider = OpenAIProvider(api_key=\"sk-your-key\", model=\"gpt-3.5-turbo\")\n\n    # Create different requests\n    requests = [\n        [Message(role=MessageRole.USER, content=\"Explain AI\")],\n        [Message(role=MessageRole.USER, content=\"What is Python?\")],\n        [Message(role=MessageRole.USER, content=\"How does the internet work?\")],\n    ]\n\n    start_time = time.time()\n\n    # Process concurrently (respects rate limits automatically)\n    responses = await asyncio.gather(*[\n        provider.generate(messages, max_tokens=200)\n        for messages in requests\n    ])\n\n    end_time = time.time()\n\n    print(f\"Processed {len(responses)} requests in {end_time - start_time:.2f} seconds\")\n\n    for i, response in enumerate(responses):\n        print(f\"Response {i+1}: {response[:100]}...\")\n\nasyncio.run(concurrent_generations())\n</code></pre>"},{"location":"api/OPENAI_PROVIDER/#with-fastapi-web-service","title":"With FastAPI Web Service","text":"<pre><code>from fastapi import FastAPI, HTTPException\nfrom pydantic import BaseModel\nfrom typing import List\nimport asyncio\n\napp = FastAPI()\n\n# Global provider instance (reuse connection pool)\nprovider = None\n\n@app.on_event(\"startup\")\nasync def startup():\n    global provider\n    provider = OpenAIProvider(\n        api_key=os.getenv(\"OPENAI_API_KEY\"),\n        model=\"gpt-3.5-turbo\"\n    )\n\n@app.on_event(\"shutdown\")\nasync def shutdown():\n    if provider:\n        await provider.__aexit__(None, None, None)\n\nclass ChatRequest(BaseModel):\n    messages: List[dict]\n    temperature: float = 0.7\n    max_tokens: int = 1000\n\nclass ChatResponse(BaseModel):\n    response: str\n    usage: dict\n\n@app.post(\"/chat\", response_model=ChatResponse)\nasync def chat_endpoint(request: ChatRequest):\n    try:\n        # Convert dict messages to Message objects\n        messages = [\n            Message(role=MessageRole(msg[\"role\"]), content=msg[\"content\"])\n            for msg in request.messages\n        ]\n\n        response = await provider.generate(\n            messages,\n            temperature=request.temperature,\n            max_tokens=request.max_tokens\n        )\n\n        usage_info = provider.get_model_info()\n\n        return ChatResponse(\n            response=response,\n            usage={\n                \"total_tokens\": usage_info.get(\"total_tokens\", 0),\n                \"total_cost\": usage_info.get(\"total_cost\", 0)\n            }\n        )\n\n    except Exception as e:\n        raise HTTPException(status_code=500, detail=str(e))\n\n# Run with: uvicorn main:app --reload\n</code></pre>"},{"location":"api/OPENAI_PROVIDER/#best-practices","title":"Best Practices","text":""},{"location":"api/OPENAI_PROVIDER/#1-api-key-security","title":"1. API Key Security","text":"<pre><code>import os\nfrom dotenv import load_dotenv\n\n# Use environment variables (never hardcode keys)\nload_dotenv()  # Load from .env file\n\nprovider = OpenAIProvider(\n    api_key=os.getenv(\"OPENAI_API_KEY\"),  # \u2705 Good\n    # api_key=\"sk-hardcoded-key\",  # \u274c Never do this\n)\n\n# For production, use secrets management\n# provider = OpenAIProvider(api_key=get_secret(\"openai-api-key\"))\n</code></pre>"},{"location":"api/OPENAI_PROVIDER/#2-cost-management","title":"2. Cost Management","text":"<pre><code># Set up cost alerts and tracking\nclass CostAwareProvider:\n    def __init__(self, api_key: str, daily_budget: float = 10.0):\n        self.provider = OpenAIProvider(api_key=api_key, track_cost=True)\n        self.daily_budget = daily_budget\n\n    async def generate_with_budget_check(self, messages, **kwargs):\n        # Check current spending\n        model_info = self.provider.get_model_info()\n        current_cost = model_info.get(\"total_cost\", 0)\n\n        if current_cost &gt;= self.daily_budget:\n            raise Exception(f\"Daily budget of ${self.daily_budget} exceeded (spent: ${current_cost:.2f})\")\n\n        return await self.provider.generate(messages, **kwargs)\n</code></pre>"},{"location":"api/OPENAI_PROVIDER/#3-response-quality-optimization","title":"3. Response Quality Optimization","text":"<pre><code># Template for consistent, high-quality responses\nasync def generate_structured_response(provider, user_query: str, response_format: str = \"helpful\"):\n    \"\"\"Generate structured, high-quality responses.\"\"\"\n\n    format_templates = {\n        \"helpful\": \"Provide a helpful, accurate, and detailed response.\",\n        \"concise\": \"Provide a concise, direct answer in 1-2 sentences.\",\n        \"educational\": \"Explain the concept step-by-step as if teaching a beginner.\",\n        \"creative\": \"Provide a creative, engaging, and original response.\"\n    }\n\n    system_prompt = f\"\"\"You are a helpful AI assistant. {format_templates.get(response_format, format_templates['helpful'])}\n\nGuidelines:\n- Be accurate and factual\n- Cite sources when relevant\n- Ask for clarification if the question is ambiguous\n- Admit when you don't know something\"\"\"\n\n    messages = [\n        Message(role=MessageRole.SYSTEM, content=system_prompt),\n        Message(role=MessageRole.USER, content=user_query)\n    ]\n\n    return await provider.generate(\n        messages,\n        temperature=0.7,  # Balanced creativity and accuracy\n        max_tokens=1000\n    )\n\n# Usage\nresponse = await generate_structured_response(\n    provider,\n    \"Explain machine learning\",\n    response_format=\"educational\"\n)\n</code></pre>"},{"location":"api/OPENAI_PROVIDER/#troubleshooting","title":"Troubleshooting","text":""},{"location":"api/OPENAI_PROVIDER/#common-issues-and-solutions","title":"Common Issues and Solutions","text":"<ol> <li> <p>Rate Limit Errors <pre><code># Implement exponential backoff\nfrom bruno_llm.base.retry import retry_async, RetryConfig\n\nretry_config = RetryConfig(max_attempts=5, base_delay=1.0)\n\n@retry_async(retry_config)\nasync def generate_with_retry():\n    return await provider.generate(messages)\n</code></pre></p> </li> <li> <p>Context Length Issues <pre><code># Automatic message truncation\ndef truncate_conversation(messages, max_tokens=3000):\n    # Keep system messages and recent context\n    system_msgs = [m for m in messages if m.role == MessageRole.SYSTEM]\n    other_msgs = [m for m in messages if m.role != MessageRole.SYSTEM]\n\n    # Take last N messages that fit\n    total_tokens = 0\n    kept_msgs = []\n\n    for msg in reversed(other_msgs):\n        msg_tokens = len(msg.content.split()) * 1.3  # Rough estimate\n        if total_tokens + msg_tokens &lt; max_tokens:\n            kept_msgs.insert(0, msg)\n            total_tokens += msg_tokens\n        else:\n            break\n\n    return system_msgs + kept_msgs\n</code></pre></p> </li> <li> <p>Network Connectivity Issues <pre><code># Test and validate connection\nasync def test_connection():\n    try:\n        await asyncio.wait_for(provider.check_connection(), timeout=10.0)\n        print(\"\u2705 Connection OK\")\n    except asyncio.TimeoutError:\n        print(\"\u274c Connection timeout - check internet/proxy settings\")\n    except Exception as e:\n        print(f\"\u274c Connection error: {e}\")\n</code></pre></p> </li> </ol>"},{"location":"api/OPENAI_PROVIDER/#debug-mode","title":"Debug Mode","text":"<pre><code>import logging\n\n# Enable debug logging for detailed information\nlogging.basicConfig(level=logging.DEBUG)\nlogging.getLogger(\"openai\").setLevel(logging.DEBUG)\nlogging.getLogger(\"bruno_llm\").setLevel(logging.DEBUG)\n\n# This will show all HTTP requests and responses\n</code></pre> <p>For more examples and advanced usage patterns, see the Bruno-LLM Examples directory.</p>"},{"location":"api/base/","title":"Base Utilities","text":""},{"location":"api/base/#bruno_llm.base","title":"<code>bruno_llm.base</code>","text":"<p>Base utilities and common functionality for LLM providers.</p> <p>This module provides shared utilities that all provider implementations can use, including token counting, rate limiting, retry logic, cost tracking, caching, streaming, context management, and middleware.</p>"},{"location":"api/base/#bruno_llm.base.BaseProvider","title":"<code>BaseProvider</code>","text":"<p>               Bases: <code>LLMInterface</code>, <code>ABC</code></p> <p>Base class for LLM provider implementations.</p> <p>Provides common functionality that all providers can use: - Retry logic with exponential backoff - Rate limiting - Cost tracking - Error handling patterns</p> <p>Subclasses must implement the LLMInterface methods: - generate() - stream() - get_token_count() - check_connection() - list_models() - get_model_info() - set_system_prompt() - get_system_prompt()</p> Example <p>class MyProvider(BaseProvider): ...     async def generate(self, messages, kwargs): ...         return await self._with_retry( ...             self._generate_impl(messages, kwargs) ...         )</p> Source code in <code>bruno_llm/base/base_provider.py</code> <pre><code>class BaseProvider(LLMInterface, ABC):\n    \"\"\"\n    Base class for LLM provider implementations.\n\n    Provides common functionality that all providers can use:\n    - Retry logic with exponential backoff\n    - Rate limiting\n    - Cost tracking\n    - Error handling patterns\n\n    Subclasses must implement the LLMInterface methods:\n    - generate()\n    - stream()\n    - get_token_count()\n    - check_connection()\n    - list_models()\n    - get_model_info()\n    - set_system_prompt()\n    - get_system_prompt()\n\n    Example:\n        &gt;&gt;&gt; class MyProvider(BaseProvider):\n        ...     async def generate(self, messages, **kwargs):\n        ...         return await self._with_retry(\n        ...             self._generate_impl(messages, **kwargs)\n        ...         )\n    \"\"\"\n\n    def __init__(\n        self,\n        provider_name: str,\n        max_retries: int = 3,\n        timeout: float = 30.0,\n        **kwargs: Any,\n    ):\n        \"\"\"\n        Initialize base provider.\n\n        Args:\n            provider_name: Name of the provider (e.g., \"ollama\", \"openai\")\n            max_retries: Maximum number of retry attempts\n            timeout: Request timeout in seconds\n            **kwargs: Additional provider-specific configuration\n        \"\"\"\n        self.provider_name = provider_name\n        self.max_retries = max_retries\n        self.timeout = timeout\n        self._system_prompt: Optional[str] = None\n        self._config = kwargs\n\n    async def _with_retry(\n        self,\n        coro: Callable[[], T],\n        max_retries: Optional[int] = None,\n    ) -&gt; T:\n        \"\"\"\n        Execute coroutine with retry logic.\n\n        Implements exponential backoff with jitter for retries.\n\n        Args:\n            coro: Async function to execute\n            max_retries: Override default max_retries\n\n        Returns:\n            Result from coroutine\n\n        Raises:\n            LLMError: If all retries are exhausted\n        \"\"\"\n        retries = max_retries if max_retries is not None else self.max_retries\n        last_error = None\n\n        for attempt in range(retries + 1):\n            try:\n                return await coro()\n            except Exception as e:\n                last_error = e\n\n                if attempt &lt; retries:\n                    # Exponential backoff with jitter\n                    delay = (2**attempt) + (asyncio.get_event_loop().time() % 1)\n                    await asyncio.sleep(delay)\n                    continue\n\n                # All retries exhausted\n                if isinstance(e, LLMError):\n                    raise\n                raise LLMError(\n                    f\"Request failed after {retries + 1} attempts\",\n                    provider=self.provider_name,\n                    original_error=e,\n                ) from e\n\n        # Should never reach here, but for type safety\n        raise LLMError(\n            \"Unexpected retry loop exit\",\n            provider=self.provider_name,\n            original_error=last_error,\n        )\n\n    def set_system_prompt(self, prompt: str) -&gt; None:\n        \"\"\"\n        Set system prompt for the provider.\n\n        Args:\n            prompt: System prompt text\n        \"\"\"\n        self._system_prompt = prompt\n\n    def get_system_prompt(self) -&gt; Optional[str]:\n        \"\"\"\n        Get current system prompt.\n\n        Returns:\n            Current system prompt or None\n        \"\"\"\n        return self._system_prompt\n\n    def _add_system_prompt(self, messages: list[Message]) -&gt; list[Message]:\n        \"\"\"\n        Add system prompt to messages if set.\n\n        Args:\n            messages: Original messages\n\n        Returns:\n            Messages with system prompt prepended if set\n        \"\"\"\n        if not self._system_prompt:\n            return messages\n\n        from bruno_core.models import MessageRole\n\n        # Check if first message is already a system message\n        if messages and messages[0].role == MessageRole.SYSTEM:\n            return messages\n\n        # Prepend system prompt\n        system_message = Message(\n            role=MessageRole.SYSTEM,\n            content=self._system_prompt,\n        )\n        return [system_message] + messages\n\n    def get_model_info(self) -&gt; dict[str, Any]:\n        \"\"\"\n        Get provider and configuration information.\n\n        Returns:\n            Dict with provider information\n        \"\"\"\n        return {\n            \"provider\": self.provider_name,\n            \"max_retries\": self.max_retries,\n            \"timeout\": self.timeout,\n            \"system_prompt\": self._system_prompt,\n            **self._config,\n        }\n</code></pre>"},{"location":"api/base/#bruno_llm.base.BaseProvider.__init__","title":"<code>__init__(provider_name, max_retries=3, timeout=30.0, **kwargs)</code>","text":"<p>Initialize base provider.</p> <p>Parameters:</p> Name Type Description Default <code>provider_name</code> <code>str</code> <p>Name of the provider (e.g., \"ollama\", \"openai\")</p> required <code>max_retries</code> <code>int</code> <p>Maximum number of retry attempts</p> <code>3</code> <code>timeout</code> <code>float</code> <p>Request timeout in seconds</p> <code>30.0</code> <code>**kwargs</code> <code>Any</code> <p>Additional provider-specific configuration</p> <code>{}</code> Source code in <code>bruno_llm/base/base_provider.py</code> <pre><code>def __init__(\n    self,\n    provider_name: str,\n    max_retries: int = 3,\n    timeout: float = 30.0,\n    **kwargs: Any,\n):\n    \"\"\"\n    Initialize base provider.\n\n    Args:\n        provider_name: Name of the provider (e.g., \"ollama\", \"openai\")\n        max_retries: Maximum number of retry attempts\n        timeout: Request timeout in seconds\n        **kwargs: Additional provider-specific configuration\n    \"\"\"\n    self.provider_name = provider_name\n    self.max_retries = max_retries\n    self.timeout = timeout\n    self._system_prompt: Optional[str] = None\n    self._config = kwargs\n</code></pre>"},{"location":"api/base/#bruno_llm.base.BaseProvider.set_system_prompt","title":"<code>set_system_prompt(prompt)</code>","text":"<p>Set system prompt for the provider.</p> <p>Parameters:</p> Name Type Description Default <code>prompt</code> <code>str</code> <p>System prompt text</p> required Source code in <code>bruno_llm/base/base_provider.py</code> <pre><code>def set_system_prompt(self, prompt: str) -&gt; None:\n    \"\"\"\n    Set system prompt for the provider.\n\n    Args:\n        prompt: System prompt text\n    \"\"\"\n    self._system_prompt = prompt\n</code></pre>"},{"location":"api/base/#bruno_llm.base.BaseProvider.get_system_prompt","title":"<code>get_system_prompt()</code>","text":"<p>Get current system prompt.</p> <p>Returns:</p> Type Description <code>Optional[str]</code> <p>Current system prompt or None</p> Source code in <code>bruno_llm/base/base_provider.py</code> <pre><code>def get_system_prompt(self) -&gt; Optional[str]:\n    \"\"\"\n    Get current system prompt.\n\n    Returns:\n        Current system prompt or None\n    \"\"\"\n    return self._system_prompt\n</code></pre>"},{"location":"api/base/#bruno_llm.base.BaseProvider.get_model_info","title":"<code>get_model_info()</code>","text":"<p>Get provider and configuration information.</p> <p>Returns:</p> Type Description <code>dict[str, Any]</code> <p>Dict with provider information</p> Source code in <code>bruno_llm/base/base_provider.py</code> <pre><code>def get_model_info(self) -&gt; dict[str, Any]:\n    \"\"\"\n    Get provider and configuration information.\n\n    Returns:\n        Dict with provider information\n    \"\"\"\n    return {\n        \"provider\": self.provider_name,\n        \"max_retries\": self.max_retries,\n        \"timeout\": self.timeout,\n        \"system_prompt\": self._system_prompt,\n        **self._config,\n    }\n</code></pre>"},{"location":"api/base/#bruno_llm.base.CacheEntry","title":"<code>CacheEntry</code>  <code>dataclass</code>","text":"<p>Cache entry for a response.</p> <p>Attributes:</p> Name Type Description <code>response</code> <code>str</code> <p>The cached response</p> <code>timestamp</code> <code>float</code> <p>When the entry was cached</p> <code>hit_count</code> <code>int</code> <p>Number of times this entry was accessed</p> <code>tokens</code> <code>Optional[int]</code> <p>Token count for the response (if available)</p> Source code in <code>bruno_llm/base/cache.py</code> <pre><code>@dataclass\nclass CacheEntry:\n    \"\"\"\n    Cache entry for a response.\n\n    Attributes:\n        response: The cached response\n        timestamp: When the entry was cached\n        hit_count: Number of times this entry was accessed\n        tokens: Token count for the response (if available)\n    \"\"\"\n\n    response: str\n    timestamp: float\n    hit_count: int = 0\n    tokens: Optional[int] = None\n</code></pre>"},{"location":"api/base/#bruno_llm.base.ResponseCache","title":"<code>ResponseCache</code>","text":"<p>LRU cache for LLM responses with TTL support.</p> <p>Caches responses to avoid redundant API calls. Uses message content and parameters as cache keys. Includes TTL (time-to-live) to ensure responses don't become stale.</p> <p>Features: - LRU eviction when max_size is reached - TTL-based expiration - Hit/miss statistics - Thread-safe operations (async-safe)</p> <p>Parameters:</p> Name Type Description Default <code>max_size</code> <code>int</code> <p>Maximum number of entries to cache (default: 1000)</p> <code>1000</code> <code>ttl</code> <code>float</code> <p>Time-to-live in seconds (default: 3600 = 1 hour)</p> <code>3600</code> Example <p>cache = ResponseCache(max_size=100, ttl=300)</p> Source code in <code>bruno_llm/base/cache.py</code> <pre><code>class ResponseCache:\n    \"\"\"\n    LRU cache for LLM responses with TTL support.\n\n    Caches responses to avoid redundant API calls. Uses message content\n    and parameters as cache keys. Includes TTL (time-to-live) to ensure\n    responses don't become stale.\n\n    Features:\n    - LRU eviction when max_size is reached\n    - TTL-based expiration\n    - Hit/miss statistics\n    - Thread-safe operations (async-safe)\n\n    Args:\n        max_size: Maximum number of entries to cache (default: 1000)\n        ttl: Time-to-live in seconds (default: 3600 = 1 hour)\n\n    Example:\n        &gt;&gt;&gt; cache = ResponseCache(max_size=100, ttl=300)\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Check if response is cached\n        &gt;&gt;&gt; response = cache.get(messages, temperature=0.7)\n        &gt;&gt;&gt; if response is None:\n        ...     response = await provider.generate(messages)\n        ...     cache.set(messages, response, temperature=0.7)\n    \"\"\"\n\n    def __init__(self, max_size: int = 1000, ttl: float = 3600):\n        \"\"\"\n        Initialize response cache.\n\n        Args:\n            max_size: Maximum number of cached entries\n            ttl: Time-to-live in seconds for cached entries\n        \"\"\"\n        self._cache: OrderedDict[str, CacheEntry] = OrderedDict()\n        self._max_size = max_size\n        self._ttl = ttl\n        self._hits = 0\n        self._misses = 0\n\n    def _generate_key(self, messages: list[Message], **kwargs: Any) -&gt; str:\n        \"\"\"\n        Generate cache key from messages and parameters.\n\n        Args:\n            messages: List of conversation messages\n            **kwargs: Additional generation parameters\n\n        Returns:\n            Cache key as hex string\n        \"\"\"\n        # Convert messages to dict representation\n        messages_dict = [{\"role\": msg.role.value, \"content\": msg.content} for msg in messages]\n\n        # Create deterministic JSON representation\n        key_data = {\"messages\": messages_dict, \"params\": dict(sorted(kwargs.items()))}\n\n        key_json = json.dumps(key_data, sort_keys=True)\n        return hashlib.sha256(key_json.encode()).hexdigest()\n\n    def get(self, messages: list[Message], **kwargs: Any) -&gt; Optional[str]:\n        \"\"\"\n        Get cached response if available and not expired.\n\n        Args:\n            messages: List of conversation messages\n            **kwargs: Additional generation parameters\n\n        Returns:\n            Cached response or None if not found/expired\n        \"\"\"\n        key = self._generate_key(messages, **kwargs)\n\n        if key not in self._cache:\n            self._misses += 1\n            return None\n\n        entry = self._cache[key]\n        current_time = time.time()\n\n        # Check if entry has expired\n        if current_time - entry.timestamp &gt; self._ttl:\n            # Remove expired entry\n            del self._cache[key]\n            self._misses += 1\n            return None\n\n        # Move to end (most recently used)\n        self._cache.move_to_end(key)\n        entry.hit_count += 1\n        self._hits += 1\n\n        return entry.response\n\n    def set(\n        self, messages: list[Message], response: str, tokens: Optional[int] = None, **kwargs: Any\n    ) -&gt; None:\n        \"\"\"\n        Cache a response.\n\n        Args:\n            messages: List of conversation messages\n            response: The response to cache\n            tokens: Token count for the response (optional)\n            **kwargs: Additional generation parameters\n        \"\"\"\n        key = self._generate_key(messages, **kwargs)\n\n        # Create cache entry\n        entry = CacheEntry(\n            response=response,\n            timestamp=time.time(),\n            hit_count=0,\n            tokens=tokens,\n        )\n\n        # Add to cache\n        self._cache[key] = entry\n        self._cache.move_to_end(key)\n\n        # Evict oldest entry if max size exceeded\n        if len(self._cache) &gt; self._max_size:\n            self._cache.popitem(last=False)\n\n    def clear(self) -&gt; None:\n        \"\"\"Clear all cached entries.\"\"\"\n        self._cache.clear()\n        self._hits = 0\n        self._misses = 0\n\n    def invalidate(self, messages: list[Message], **kwargs: Any) -&gt; bool:\n        \"\"\"\n        Invalidate a specific cache entry.\n\n        Args:\n            messages: List of conversation messages\n            **kwargs: Additional generation parameters\n\n        Returns:\n            True if entry was found and removed, False otherwise\n        \"\"\"\n        key = self._generate_key(messages, **kwargs)\n\n        if key in self._cache:\n            del self._cache[key]\n            return True\n        return False\n\n    def get_stats(self) -&gt; dict[str, Any]:\n        \"\"\"\n        Get cache statistics.\n\n        Returns:\n            Dictionary with cache statistics\n        \"\"\"\n        total_requests = self._hits + self._misses\n        hit_rate = self._hits / total_requests if total_requests &gt; 0 else 0.0\n\n        return {\n            \"size\": len(self._cache),\n            \"max_size\": self._max_size,\n            \"hits\": self._hits,\n            \"misses\": self._misses,\n            \"hit_rate\": hit_rate,\n            \"ttl\": self._ttl,\n        }\n\n    def get_size_bytes(self) -&gt; int:\n        \"\"\"\n        Estimate cache size in bytes.\n\n        Returns:\n            Approximate cache size in bytes\n        \"\"\"\n        total_size = 0\n        for key, entry in self._cache.items():\n            # Key size\n            total_size += len(key.encode())\n            # Response size\n            total_size += len(entry.response.encode())\n            # Overhead for entry metadata (~100 bytes)\n            total_size += 100\n\n        return total_size\n\n    def cleanup_expired(self) -&gt; int:\n        \"\"\"\n        Remove all expired entries.\n\n        Returns:\n            Number of entries removed\n        \"\"\"\n        current_time = time.time()\n        expired_keys = [\n            key for key, entry in self._cache.items() if current_time - entry.timestamp &gt; self._ttl\n        ]\n\n        for key in expired_keys:\n            del self._cache[key]\n\n        return len(expired_keys)\n\n    def get_top_entries(self, n: int = 10) -&gt; list[tuple[str, CacheEntry]]:\n        \"\"\"\n        Get top N most frequently accessed entries.\n\n        Args:\n            n: Number of entries to return\n\n        Returns:\n            List of (key, entry) tuples sorted by hit count\n        \"\"\"\n        sorted_entries = sorted(self._cache.items(), key=lambda x: x[1].hit_count, reverse=True)\n        return sorted_entries[:n]\n</code></pre>"},{"location":"api/base/#bruno_llm.base.ResponseCache--check-if-response-is-cached","title":"Check if response is cached","text":"<p>response = cache.get(messages, temperature=0.7) if response is None: ...     response = await provider.generate(messages) ...     cache.set(messages, response, temperature=0.7)</p>"},{"location":"api/base/#bruno_llm.base.ResponseCache.__init__","title":"<code>__init__(max_size=1000, ttl=3600)</code>","text":"<p>Initialize response cache.</p> <p>Parameters:</p> Name Type Description Default <code>max_size</code> <code>int</code> <p>Maximum number of cached entries</p> <code>1000</code> <code>ttl</code> <code>float</code> <p>Time-to-live in seconds for cached entries</p> <code>3600</code> Source code in <code>bruno_llm/base/cache.py</code> <pre><code>def __init__(self, max_size: int = 1000, ttl: float = 3600):\n    \"\"\"\n    Initialize response cache.\n\n    Args:\n        max_size: Maximum number of cached entries\n        ttl: Time-to-live in seconds for cached entries\n    \"\"\"\n    self._cache: OrderedDict[str, CacheEntry] = OrderedDict()\n    self._max_size = max_size\n    self._ttl = ttl\n    self._hits = 0\n    self._misses = 0\n</code></pre>"},{"location":"api/base/#bruno_llm.base.ResponseCache.get","title":"<code>get(messages, **kwargs)</code>","text":"<p>Get cached response if available and not expired.</p> <p>Parameters:</p> Name Type Description Default <code>messages</code> <code>list[Message]</code> <p>List of conversation messages</p> required <code>**kwargs</code> <code>Any</code> <p>Additional generation parameters</p> <code>{}</code> <p>Returns:</p> Type Description <code>Optional[str]</code> <p>Cached response or None if not found/expired</p> Source code in <code>bruno_llm/base/cache.py</code> <pre><code>def get(self, messages: list[Message], **kwargs: Any) -&gt; Optional[str]:\n    \"\"\"\n    Get cached response if available and not expired.\n\n    Args:\n        messages: List of conversation messages\n        **kwargs: Additional generation parameters\n\n    Returns:\n        Cached response or None if not found/expired\n    \"\"\"\n    key = self._generate_key(messages, **kwargs)\n\n    if key not in self._cache:\n        self._misses += 1\n        return None\n\n    entry = self._cache[key]\n    current_time = time.time()\n\n    # Check if entry has expired\n    if current_time - entry.timestamp &gt; self._ttl:\n        # Remove expired entry\n        del self._cache[key]\n        self._misses += 1\n        return None\n\n    # Move to end (most recently used)\n    self._cache.move_to_end(key)\n    entry.hit_count += 1\n    self._hits += 1\n\n    return entry.response\n</code></pre>"},{"location":"api/base/#bruno_llm.base.ResponseCache.set","title":"<code>set(messages, response, tokens=None, **kwargs)</code>","text":"<p>Cache a response.</p> <p>Parameters:</p> Name Type Description Default <code>messages</code> <code>list[Message]</code> <p>List of conversation messages</p> required <code>response</code> <code>str</code> <p>The response to cache</p> required <code>tokens</code> <code>Optional[int]</code> <p>Token count for the response (optional)</p> <code>None</code> <code>**kwargs</code> <code>Any</code> <p>Additional generation parameters</p> <code>{}</code> Source code in <code>bruno_llm/base/cache.py</code> <pre><code>def set(\n    self, messages: list[Message], response: str, tokens: Optional[int] = None, **kwargs: Any\n) -&gt; None:\n    \"\"\"\n    Cache a response.\n\n    Args:\n        messages: List of conversation messages\n        response: The response to cache\n        tokens: Token count for the response (optional)\n        **kwargs: Additional generation parameters\n    \"\"\"\n    key = self._generate_key(messages, **kwargs)\n\n    # Create cache entry\n    entry = CacheEntry(\n        response=response,\n        timestamp=time.time(),\n        hit_count=0,\n        tokens=tokens,\n    )\n\n    # Add to cache\n    self._cache[key] = entry\n    self._cache.move_to_end(key)\n\n    # Evict oldest entry if max size exceeded\n    if len(self._cache) &gt; self._max_size:\n        self._cache.popitem(last=False)\n</code></pre>"},{"location":"api/base/#bruno_llm.base.ResponseCache.clear","title":"<code>clear()</code>","text":"<p>Clear all cached entries.</p> Source code in <code>bruno_llm/base/cache.py</code> <pre><code>def clear(self) -&gt; None:\n    \"\"\"Clear all cached entries.\"\"\"\n    self._cache.clear()\n    self._hits = 0\n    self._misses = 0\n</code></pre>"},{"location":"api/base/#bruno_llm.base.ResponseCache.invalidate","title":"<code>invalidate(messages, **kwargs)</code>","text":"<p>Invalidate a specific cache entry.</p> <p>Parameters:</p> Name Type Description Default <code>messages</code> <code>list[Message]</code> <p>List of conversation messages</p> required <code>**kwargs</code> <code>Any</code> <p>Additional generation parameters</p> <code>{}</code> <p>Returns:</p> Type Description <code>bool</code> <p>True if entry was found and removed, False otherwise</p> Source code in <code>bruno_llm/base/cache.py</code> <pre><code>def invalidate(self, messages: list[Message], **kwargs: Any) -&gt; bool:\n    \"\"\"\n    Invalidate a specific cache entry.\n\n    Args:\n        messages: List of conversation messages\n        **kwargs: Additional generation parameters\n\n    Returns:\n        True if entry was found and removed, False otherwise\n    \"\"\"\n    key = self._generate_key(messages, **kwargs)\n\n    if key in self._cache:\n        del self._cache[key]\n        return True\n    return False\n</code></pre>"},{"location":"api/base/#bruno_llm.base.ResponseCache.get_stats","title":"<code>get_stats()</code>","text":"<p>Get cache statistics.</p> <p>Returns:</p> Type Description <code>dict[str, Any]</code> <p>Dictionary with cache statistics</p> Source code in <code>bruno_llm/base/cache.py</code> <pre><code>def get_stats(self) -&gt; dict[str, Any]:\n    \"\"\"\n    Get cache statistics.\n\n    Returns:\n        Dictionary with cache statistics\n    \"\"\"\n    total_requests = self._hits + self._misses\n    hit_rate = self._hits / total_requests if total_requests &gt; 0 else 0.0\n\n    return {\n        \"size\": len(self._cache),\n        \"max_size\": self._max_size,\n        \"hits\": self._hits,\n        \"misses\": self._misses,\n        \"hit_rate\": hit_rate,\n        \"ttl\": self._ttl,\n    }\n</code></pre>"},{"location":"api/base/#bruno_llm.base.ResponseCache.get_size_bytes","title":"<code>get_size_bytes()</code>","text":"<p>Estimate cache size in bytes.</p> <p>Returns:</p> Type Description <code>int</code> <p>Approximate cache size in bytes</p> Source code in <code>bruno_llm/base/cache.py</code> <pre><code>def get_size_bytes(self) -&gt; int:\n    \"\"\"\n    Estimate cache size in bytes.\n\n    Returns:\n        Approximate cache size in bytes\n    \"\"\"\n    total_size = 0\n    for key, entry in self._cache.items():\n        # Key size\n        total_size += len(key.encode())\n        # Response size\n        total_size += len(entry.response.encode())\n        # Overhead for entry metadata (~100 bytes)\n        total_size += 100\n\n    return total_size\n</code></pre>"},{"location":"api/base/#bruno_llm.base.ResponseCache.cleanup_expired","title":"<code>cleanup_expired()</code>","text":"<p>Remove all expired entries.</p> <p>Returns:</p> Type Description <code>int</code> <p>Number of entries removed</p> Source code in <code>bruno_llm/base/cache.py</code> <pre><code>def cleanup_expired(self) -&gt; int:\n    \"\"\"\n    Remove all expired entries.\n\n    Returns:\n        Number of entries removed\n    \"\"\"\n    current_time = time.time()\n    expired_keys = [\n        key for key, entry in self._cache.items() if current_time - entry.timestamp &gt; self._ttl\n    ]\n\n    for key in expired_keys:\n        del self._cache[key]\n\n    return len(expired_keys)\n</code></pre>"},{"location":"api/base/#bruno_llm.base.ResponseCache.get_top_entries","title":"<code>get_top_entries(n=10)</code>","text":"<p>Get top N most frequently accessed entries.</p> <p>Parameters:</p> Name Type Description Default <code>n</code> <code>int</code> <p>Number of entries to return</p> <code>10</code> <p>Returns:</p> Type Description <code>list[tuple[str, CacheEntry]]</code> <p>List of (key, entry) tuples sorted by hit count</p> Source code in <code>bruno_llm/base/cache.py</code> <pre><code>def get_top_entries(self, n: int = 10) -&gt; list[tuple[str, CacheEntry]]:\n    \"\"\"\n    Get top N most frequently accessed entries.\n\n    Args:\n        n: Number of entries to return\n\n    Returns:\n        List of (key, entry) tuples sorted by hit count\n    \"\"\"\n    sorted_entries = sorted(self._cache.items(), key=lambda x: x[1].hit_count, reverse=True)\n    return sorted_entries[:n]\n</code></pre>"},{"location":"api/base/#bruno_llm.base.ContextLimits","title":"<code>ContextLimits</code>  <code>dataclass</code>","text":"<p>Context window limits for a model.</p> <p>Attributes:</p> Name Type Description <code>max_tokens</code> <code>int</code> <p>Maximum total tokens (input + output)</p> <code>max_input_tokens</code> <code>Optional[int]</code> <p>Maximum input tokens</p> <code>max_output_tokens</code> <code>Optional[int]</code> <p>Maximum output tokens</p> <code>warning_threshold</code> <code>float</code> <p>Warn when this % of limit is reached (0.0-1.0)</p> Source code in <code>bruno_llm/base/context.py</code> <pre><code>@dataclass\nclass ContextLimits:\n    \"\"\"\n    Context window limits for a model.\n\n    Attributes:\n        max_tokens: Maximum total tokens (input + output)\n        max_input_tokens: Maximum input tokens\n        max_output_tokens: Maximum output tokens\n        warning_threshold: Warn when this % of limit is reached (0.0-1.0)\n    \"\"\"\n\n    max_tokens: int\n    max_input_tokens: Optional[int] = None\n    max_output_tokens: Optional[int] = None\n    warning_threshold: float = 0.9\n\n    def __post_init__(self):\n        \"\"\"Validate limits after initialization.\"\"\"\n        if self.max_input_tokens is None:\n            self.max_input_tokens = self.max_tokens\n        if self.max_output_tokens is None:\n            self.max_output_tokens = self.max_tokens // 4  # Default 25% for output\n</code></pre>"},{"location":"api/base/#bruno_llm.base.ContextLimits.__post_init__","title":"<code>__post_init__()</code>","text":"<p>Validate limits after initialization.</p> Source code in <code>bruno_llm/base/context.py</code> <pre><code>def __post_init__(self):\n    \"\"\"Validate limits after initialization.\"\"\"\n    if self.max_input_tokens is None:\n        self.max_input_tokens = self.max_tokens\n    if self.max_output_tokens is None:\n        self.max_output_tokens = self.max_tokens // 4  # Default 25% for output\n</code></pre>"},{"location":"api/base/#bruno_llm.base.ContextWindowManager","title":"<code>ContextWindowManager</code>","text":"<p>Manage context windows and message truncation.</p> <p>Handles: - Token counting for messages - Context limit checking - Automatic message truncation - Warning when approaching limits</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>str</code> <p>Model name for context limits</p> required <code>token_counter</code> <code>Optional[TokenCounter]</code> <p>Token counter instance</p> <code>None</code> <code>limits</code> <code>Optional[ContextLimits]</code> <p>Custom context limits (overrides model defaults)</p> <code>None</code> <code>strategy</code> <code>TruncationStrategy</code> <p>Truncation strategy to use</p> <code>SLIDING_WINDOW</code> Example <p>manager = ContextWindowManager(model=\"gpt-4\")</p> Source code in <code>bruno_llm/base/context.py</code> <pre><code>class ContextWindowManager:\n    \"\"\"\n    Manage context windows and message truncation.\n\n    Handles:\n    - Token counting for messages\n    - Context limit checking\n    - Automatic message truncation\n    - Warning when approaching limits\n\n    Args:\n        model: Model name for context limits\n        token_counter: Token counter instance\n        limits: Custom context limits (overrides model defaults)\n        strategy: Truncation strategy to use\n\n    Example:\n        &gt;&gt;&gt; manager = ContextWindowManager(model=\"gpt-4\")\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Check if messages fit\n        &gt;&gt;&gt; if manager.check_limit(messages):\n        ...     response = await provider.generate(messages)\n        ... else:\n        ...     # Truncate messages\n        ...     truncated = manager.truncate(messages)\n        ...     response = await provider.generate(truncated)\n    \"\"\"\n\n    def __init__(\n        self,\n        model: str,\n        token_counter: Optional[TokenCounter] = None,\n        limits: Optional[ContextLimits] = None,\n        strategy: TruncationStrategy = TruncationStrategy.SLIDING_WINDOW,\n    ):\n        \"\"\"\n        Initialize context window manager.\n\n        Args:\n            model: Model name\n            token_counter: Token counter instance\n            limits: Custom context limits\n            strategy: Truncation strategy\n        \"\"\"\n        self.model = model\n        self.token_counter = token_counter or create_token_counter(model)\n        self.limits = limits or self._get_model_limits(model)\n        self.strategy = strategy\n        self._warning_callback: Optional[Callable[[int, int], None]] = None\n\n    def _get_model_limits(self, model: str) -&gt; ContextLimits:\n        \"\"\"\n        Get context limits for a model.\n\n        Args:\n            model: Model name\n\n        Returns:\n            Context limits for the model\n        \"\"\"\n        # Try exact match first\n        if model in MODEL_LIMITS:\n            return MODEL_LIMITS[model]\n\n        # Try partial match\n        for model_name, limits in MODEL_LIMITS.items():\n            if model.startswith(model_name):\n                return limits\n\n        # Default conservative limit\n        return ContextLimits(max_tokens=4096)\n\n    def count_tokens(self, messages: list[Message]) -&gt; int:\n        \"\"\"\n        Count tokens in messages.\n\n        Args:\n            messages: List of messages\n\n        Returns:\n            Total token count\n        \"\"\"\n        return self.token_counter.count_messages_tokens(messages)\n\n    def check_limit(\n        self,\n        messages: list[Message],\n        max_output_tokens: Optional[int] = None,\n    ) -&gt; bool:\n        \"\"\"\n        Check if messages fit within context limit.\n\n        Args:\n            messages: List of messages\n            max_output_tokens: Expected output tokens\n\n        Returns:\n            True if messages fit, False otherwise\n        \"\"\"\n        input_tokens = self.count_tokens(messages)\n        output_tokens = max_output_tokens or self.limits.max_output_tokens\n        total_tokens = input_tokens + output_tokens\n\n        # Check warning threshold\n        if input_tokens / self.limits.max_input_tokens &gt;= self.limits.warning_threshold:\n            if self._warning_callback:\n                self._warning_callback(input_tokens, self.limits.max_input_tokens)\n\n        return total_tokens &lt;= self.limits.max_tokens\n\n    def get_available_tokens(self, messages: list[Message]) -&gt; int:\n        \"\"\"\n        Get number of tokens available for output.\n\n        Args:\n            messages: List of messages\n\n        Returns:\n            Available tokens for output\n        \"\"\"\n        input_tokens = self.count_tokens(messages)\n        return max(0, self.limits.max_tokens - input_tokens)\n\n    def truncate(\n        self,\n        messages: list[Message],\n        max_output_tokens: Optional[int] = None,\n    ) -&gt; list[Message]:\n        \"\"\"\n        Truncate messages to fit within context limit.\n\n        Args:\n            messages: List of messages\n            max_output_tokens: Expected output tokens\n\n        Returns:\n            Truncated message list\n\n        Raises:\n            ContextLengthExceededError: If messages can't be truncated enough\n        \"\"\"\n        output_tokens = max_output_tokens or self.limits.max_output_tokens\n        target_input_tokens = self.limits.max_tokens - output_tokens\n\n        if target_input_tokens &lt;= 0:\n            raise ContextLengthExceededError(\n                f\"Output tokens ({output_tokens}) exceed total limit ({self.limits.max_tokens})\"\n            )\n\n        if self.strategy == TruncationStrategy.OLDEST_FIRST:\n            return self._truncate_oldest_first(messages, target_input_tokens)\n        elif self.strategy == TruncationStrategy.MIDDLE_OUT:\n            return self._truncate_middle_out(messages, target_input_tokens)\n        elif self.strategy == TruncationStrategy.SLIDING_WINDOW:\n            return self._truncate_sliding_window(messages, target_input_tokens)\n        elif self.strategy == TruncationStrategy.SMART:\n            return self._truncate_smart(messages, target_input_tokens)\n        else:\n            return self._truncate_oldest_first(messages, target_input_tokens)\n\n    def _truncate_oldest_first(\n        self,\n        messages: list[Message],\n        target_tokens: int,\n    ) -&gt; list[Message]:\n        \"\"\"Remove oldest messages first (keep system message).\"\"\"\n        # Always keep system messages\n        system_messages = [m for m in messages if m.role == MessageRole.SYSTEM]\n        other_messages = [m for m in messages if m.role != MessageRole.SYSTEM]\n\n        # Start with system messages\n        result = system_messages[:]\n        current_tokens = self.count_tokens(result)\n\n        # Add messages from newest to oldest\n        for message in reversed(other_messages):\n            message_tokens = self.token_counter.count_message_tokens(message)\n            if current_tokens + message_tokens &lt;= target_tokens:\n                result.insert(len(system_messages), message)\n                current_tokens += message_tokens\n            else:\n                break\n\n        # Re-order to maintain chronological order (except system at start)\n        return system_messages + list(reversed(result[len(system_messages) :]))\n\n    def _truncate_middle_out(\n        self,\n        messages: list[Message],\n        target_tokens: int,\n    ) -&gt; list[Message]:\n        \"\"\"Keep first and last messages, remove middle.\"\"\"\n        if len(messages) &lt;= 2:\n            return messages\n\n        # Keep system messages and last message\n        system_messages = [m for m in messages if m.role == MessageRole.SYSTEM]\n        other_messages = [m for m in messages if m.role != MessageRole.SYSTEM]\n\n        if not other_messages:\n            return messages\n\n        result = system_messages + [other_messages[-1]]\n        current_tokens = self.count_tokens(result)\n\n        # Add messages from the start\n        for message in other_messages[:-1]:\n            message_tokens = self.token_counter.count_message_tokens(message)\n            if current_tokens + message_tokens &lt;= target_tokens:\n                result.insert(len(system_messages), message)\n                current_tokens += message_tokens\n            else:\n                break\n\n        return result\n\n    def _truncate_sliding_window(\n        self,\n        messages: list[Message],\n        target_tokens: int,\n    ) -&gt; list[Message]:\n        \"\"\"Keep most recent N messages.\"\"\"\n        # Always keep system messages\n        system_messages = [m for m in messages if m.role == MessageRole.SYSTEM]\n        other_messages = [m for m in messages if m.role != MessageRole.SYSTEM]\n\n        result = system_messages[:]\n        current_tokens = self.count_tokens(result)\n\n        # Add messages from newest to oldest\n        for message in reversed(other_messages):\n            message_tokens = self.token_counter.count_message_tokens(message)\n            if current_tokens + message_tokens &lt;= target_tokens:\n                result.append(message)\n                current_tokens += message_tokens\n            else:\n                break\n\n        # Keep system messages at start, reverse others\n        return system_messages + list(reversed(result[len(system_messages) :]))\n\n    def _truncate_smart(\n        self,\n        messages: list[Message],\n        target_tokens: int,\n    ) -&gt; list[Message]:\n        \"\"\"\n        Smart truncation: keep system + important messages + recent.\n\n        Priority:\n        1. System messages (always keep)\n        2. Last 2 messages (recent context)\n        3. Messages with high token count (likely important)\n        4. Fill remaining space with recent messages\n        \"\"\"\n        system_messages = [m for m in messages if m.role == MessageRole.SYSTEM]\n        other_messages = [m for m in messages if m.role != MessageRole.SYSTEM]\n\n        if not other_messages:\n            return messages\n\n        # Start with system messages\n        result = system_messages[:]\n        current_tokens = self.count_tokens(result)\n\n        # Always include last 2 messages (most recent context)\n        priority_messages = other_messages[-2:]\n        for message in priority_messages:\n            message_tokens = self.token_counter.count_message_tokens(message)\n            if current_tokens + message_tokens &lt;= target_tokens:\n                result.append(message)\n                current_tokens += message_tokens\n\n        # Fill remaining space with other messages (newest first)\n        remaining = list(other_messages[:-2])\n        for message in reversed(remaining):\n            message_tokens = self.token_counter.count_message_tokens(message)\n            if current_tokens + message_tokens &lt;= target_tokens:\n                result.insert(len(system_messages), message)\n                current_tokens += message_tokens\n            else:\n                break\n\n        return result\n\n    def set_warning_callback(self, callback: Callable[[int, int], None]) -&gt; None:\n        \"\"\"\n        Set callback for context limit warnings.\n\n        Args:\n            callback: Function (current_tokens, max_tokens) -&gt; None\n        \"\"\"\n        self._warning_callback = callback\n\n    def get_stats(self, messages: list[Message]) -&gt; dict:\n        \"\"\"\n        Get statistics about context usage.\n\n        Args:\n            messages: List of messages\n\n        Returns:\n            Dictionary with context statistics\n        \"\"\"\n        input_tokens = self.count_tokens(messages)\n        available_tokens = self.get_available_tokens(messages)\n        usage_percent = (input_tokens / self.limits.max_input_tokens) * 100\n\n        return {\n            \"model\": self.model,\n            \"input_tokens\": input_tokens,\n            \"max_input_tokens\": self.limits.max_input_tokens,\n            \"available_output_tokens\": available_tokens,\n            \"max_output_tokens\": self.limits.max_output_tokens,\n            \"total_limit\": self.limits.max_tokens,\n            \"usage_percent\": usage_percent,\n            \"within_limit\": self.check_limit(messages),\n            \"message_count\": len(messages),\n        }\n</code></pre>"},{"location":"api/base/#bruno_llm.base.ContextWindowManager--check-if-messages-fit","title":"Check if messages fit","text":"<p>if manager.check_limit(messages): ...     response = await provider.generate(messages) ... else: ...     # Truncate messages ...     truncated = manager.truncate(messages) ...     response = await provider.generate(truncated)</p>"},{"location":"api/base/#bruno_llm.base.ContextWindowManager.__init__","title":"<code>__init__(model, token_counter=None, limits=None, strategy=TruncationStrategy.SLIDING_WINDOW)</code>","text":"<p>Initialize context window manager.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>str</code> <p>Model name</p> required <code>token_counter</code> <code>Optional[TokenCounter]</code> <p>Token counter instance</p> <code>None</code> <code>limits</code> <code>Optional[ContextLimits]</code> <p>Custom context limits</p> <code>None</code> <code>strategy</code> <code>TruncationStrategy</code> <p>Truncation strategy</p> <code>SLIDING_WINDOW</code> Source code in <code>bruno_llm/base/context.py</code> <pre><code>def __init__(\n    self,\n    model: str,\n    token_counter: Optional[TokenCounter] = None,\n    limits: Optional[ContextLimits] = None,\n    strategy: TruncationStrategy = TruncationStrategy.SLIDING_WINDOW,\n):\n    \"\"\"\n    Initialize context window manager.\n\n    Args:\n        model: Model name\n        token_counter: Token counter instance\n        limits: Custom context limits\n        strategy: Truncation strategy\n    \"\"\"\n    self.model = model\n    self.token_counter = token_counter or create_token_counter(model)\n    self.limits = limits or self._get_model_limits(model)\n    self.strategy = strategy\n    self._warning_callback: Optional[Callable[[int, int], None]] = None\n</code></pre>"},{"location":"api/base/#bruno_llm.base.ContextWindowManager.count_tokens","title":"<code>count_tokens(messages)</code>","text":"<p>Count tokens in messages.</p> <p>Parameters:</p> Name Type Description Default <code>messages</code> <code>list[Message]</code> <p>List of messages</p> required <p>Returns:</p> Type Description <code>int</code> <p>Total token count</p> Source code in <code>bruno_llm/base/context.py</code> <pre><code>def count_tokens(self, messages: list[Message]) -&gt; int:\n    \"\"\"\n    Count tokens in messages.\n\n    Args:\n        messages: List of messages\n\n    Returns:\n        Total token count\n    \"\"\"\n    return self.token_counter.count_messages_tokens(messages)\n</code></pre>"},{"location":"api/base/#bruno_llm.base.ContextWindowManager.check_limit","title":"<code>check_limit(messages, max_output_tokens=None)</code>","text":"<p>Check if messages fit within context limit.</p> <p>Parameters:</p> Name Type Description Default <code>messages</code> <code>list[Message]</code> <p>List of messages</p> required <code>max_output_tokens</code> <code>Optional[int]</code> <p>Expected output tokens</p> <code>None</code> <p>Returns:</p> Type Description <code>bool</code> <p>True if messages fit, False otherwise</p> Source code in <code>bruno_llm/base/context.py</code> <pre><code>def check_limit(\n    self,\n    messages: list[Message],\n    max_output_tokens: Optional[int] = None,\n) -&gt; bool:\n    \"\"\"\n    Check if messages fit within context limit.\n\n    Args:\n        messages: List of messages\n        max_output_tokens: Expected output tokens\n\n    Returns:\n        True if messages fit, False otherwise\n    \"\"\"\n    input_tokens = self.count_tokens(messages)\n    output_tokens = max_output_tokens or self.limits.max_output_tokens\n    total_tokens = input_tokens + output_tokens\n\n    # Check warning threshold\n    if input_tokens / self.limits.max_input_tokens &gt;= self.limits.warning_threshold:\n        if self._warning_callback:\n            self._warning_callback(input_tokens, self.limits.max_input_tokens)\n\n    return total_tokens &lt;= self.limits.max_tokens\n</code></pre>"},{"location":"api/base/#bruno_llm.base.ContextWindowManager.get_available_tokens","title":"<code>get_available_tokens(messages)</code>","text":"<p>Get number of tokens available for output.</p> <p>Parameters:</p> Name Type Description Default <code>messages</code> <code>list[Message]</code> <p>List of messages</p> required <p>Returns:</p> Type Description <code>int</code> <p>Available tokens for output</p> Source code in <code>bruno_llm/base/context.py</code> <pre><code>def get_available_tokens(self, messages: list[Message]) -&gt; int:\n    \"\"\"\n    Get number of tokens available for output.\n\n    Args:\n        messages: List of messages\n\n    Returns:\n        Available tokens for output\n    \"\"\"\n    input_tokens = self.count_tokens(messages)\n    return max(0, self.limits.max_tokens - input_tokens)\n</code></pre>"},{"location":"api/base/#bruno_llm.base.ContextWindowManager.truncate","title":"<code>truncate(messages, max_output_tokens=None)</code>","text":"<p>Truncate messages to fit within context limit.</p> <p>Parameters:</p> Name Type Description Default <code>messages</code> <code>list[Message]</code> <p>List of messages</p> required <code>max_output_tokens</code> <code>Optional[int]</code> <p>Expected output tokens</p> <code>None</code> <p>Returns:</p> Type Description <code>list[Message]</code> <p>Truncated message list</p> <p>Raises:</p> Type Description <code>ContextLengthExceededError</code> <p>If messages can't be truncated enough</p> Source code in <code>bruno_llm/base/context.py</code> <pre><code>def truncate(\n    self,\n    messages: list[Message],\n    max_output_tokens: Optional[int] = None,\n) -&gt; list[Message]:\n    \"\"\"\n    Truncate messages to fit within context limit.\n\n    Args:\n        messages: List of messages\n        max_output_tokens: Expected output tokens\n\n    Returns:\n        Truncated message list\n\n    Raises:\n        ContextLengthExceededError: If messages can't be truncated enough\n    \"\"\"\n    output_tokens = max_output_tokens or self.limits.max_output_tokens\n    target_input_tokens = self.limits.max_tokens - output_tokens\n\n    if target_input_tokens &lt;= 0:\n        raise ContextLengthExceededError(\n            f\"Output tokens ({output_tokens}) exceed total limit ({self.limits.max_tokens})\"\n        )\n\n    if self.strategy == TruncationStrategy.OLDEST_FIRST:\n        return self._truncate_oldest_first(messages, target_input_tokens)\n    elif self.strategy == TruncationStrategy.MIDDLE_OUT:\n        return self._truncate_middle_out(messages, target_input_tokens)\n    elif self.strategy == TruncationStrategy.SLIDING_WINDOW:\n        return self._truncate_sliding_window(messages, target_input_tokens)\n    elif self.strategy == TruncationStrategy.SMART:\n        return self._truncate_smart(messages, target_input_tokens)\n    else:\n        return self._truncate_oldest_first(messages, target_input_tokens)\n</code></pre>"},{"location":"api/base/#bruno_llm.base.ContextWindowManager.set_warning_callback","title":"<code>set_warning_callback(callback)</code>","text":"<p>Set callback for context limit warnings.</p> <p>Parameters:</p> Name Type Description Default <code>callback</code> <code>Callable[[int, int], None]</code> <p>Function (current_tokens, max_tokens) -&gt; None</p> required Source code in <code>bruno_llm/base/context.py</code> <pre><code>def set_warning_callback(self, callback: Callable[[int, int], None]) -&gt; None:\n    \"\"\"\n    Set callback for context limit warnings.\n\n    Args:\n        callback: Function (current_tokens, max_tokens) -&gt; None\n    \"\"\"\n    self._warning_callback = callback\n</code></pre>"},{"location":"api/base/#bruno_llm.base.ContextWindowManager.get_stats","title":"<code>get_stats(messages)</code>","text":"<p>Get statistics about context usage.</p> <p>Parameters:</p> Name Type Description Default <code>messages</code> <code>list[Message]</code> <p>List of messages</p> required <p>Returns:</p> Type Description <code>dict</code> <p>Dictionary with context statistics</p> Source code in <code>bruno_llm/base/context.py</code> <pre><code>def get_stats(self, messages: list[Message]) -&gt; dict:\n    \"\"\"\n    Get statistics about context usage.\n\n    Args:\n        messages: List of messages\n\n    Returns:\n        Dictionary with context statistics\n    \"\"\"\n    input_tokens = self.count_tokens(messages)\n    available_tokens = self.get_available_tokens(messages)\n    usage_percent = (input_tokens / self.limits.max_input_tokens) * 100\n\n    return {\n        \"model\": self.model,\n        \"input_tokens\": input_tokens,\n        \"max_input_tokens\": self.limits.max_input_tokens,\n        \"available_output_tokens\": available_tokens,\n        \"max_output_tokens\": self.limits.max_output_tokens,\n        \"total_limit\": self.limits.max_tokens,\n        \"usage_percent\": usage_percent,\n        \"within_limit\": self.check_limit(messages),\n        \"message_count\": len(messages),\n    }\n</code></pre>"},{"location":"api/base/#bruno_llm.base.TruncationStrategy","title":"<code>TruncationStrategy</code>","text":"<p>               Bases: <code>Enum</code></p> <p>Strategy for truncating messages when context limit is exceeded.</p> Source code in <code>bruno_llm/base/context.py</code> <pre><code>class TruncationStrategy(Enum):\n    \"\"\"Strategy for truncating messages when context limit is exceeded.\"\"\"\n\n    OLDEST_FIRST = \"oldest_first\"  # Remove oldest messages first\n    MIDDLE_OUT = \"middle_out\"  # Keep first and last, remove middle\n    SLIDING_WINDOW = \"sliding_window\"  # Keep most recent N messages\n    SMART = \"smart\"  # Keep system + important messages + recent\n</code></pre>"},{"location":"api/base/#bruno_llm.base.CostTracker","title":"<code>CostTracker</code>","text":"<p>Track API usage costs across requests.</p> <p>Maintains history of API calls with token usage and costs. Supports multiple models with different pricing.</p> Example <p>tracker = CostTracker( ...     provider_name=\"openai\", ...     pricing={ ...         \"gpt-4\": {\"input\": 0.03, \"output\": 0.06}, ...         \"gpt-3.5-turbo\": {\"input\": 0.001, \"output\": 0.002}, ...     } ... ) tracker.track_request( ...     model=\"gpt-4\", ...     input_tokens=100, ...     output_tokens=50 ... ) print(tracker.get_total_cost()) 6.0  # $0.06 (in cents)</p> Source code in <code>bruno_llm/base/cost_tracker.py</code> <pre><code>class CostTracker:\n    \"\"\"\n    Track API usage costs across requests.\n\n    Maintains history of API calls with token usage and costs.\n    Supports multiple models with different pricing.\n\n    Example:\n        &gt;&gt;&gt; tracker = CostTracker(\n        ...     provider_name=\"openai\",\n        ...     pricing={\n        ...         \"gpt-4\": {\"input\": 0.03, \"output\": 0.06},\n        ...         \"gpt-3.5-turbo\": {\"input\": 0.001, \"output\": 0.002},\n        ...     }\n        ... )\n        &gt;&gt;&gt; tracker.track_request(\n        ...     model=\"gpt-4\",\n        ...     input_tokens=100,\n        ...     output_tokens=50\n        ... )\n        &gt;&gt;&gt; print(tracker.get_total_cost())\n        6.0  # $0.06 (in cents)\n    \"\"\"\n\n    def __init__(\n        self,\n        provider_name: str,\n        pricing: dict[str, dict[str, float]],\n        currency: str = \"USD\",\n    ):\n        \"\"\"\n        Initialize cost tracker.\n\n        Args:\n            provider_name: Name of the provider\n            pricing: Pricing per model (per 1K tokens)\n                Format: {\"model_name\": {\"input\": price, \"output\": price}}\n            currency: Currency code (default: \"USD\")\n        \"\"\"\n        self.provider_name = provider_name\n        self.pricing = pricing\n        self.currency = currency\n        self.usage_history: list[UsageRecord] = []\n\n    def track_request(\n        self,\n        model: str,\n        input_tokens: int,\n        output_tokens: int,\n        metadata: Optional[dict[str, str]] = None,\n    ) -&gt; UsageRecord:\n        \"\"\"\n        Track a single API request.\n\n        Args:\n            model: Model name\n            input_tokens: Number of input tokens\n            output_tokens: Number of output tokens\n            metadata: Optional metadata about the request\n\n        Returns:\n            UsageRecord with calculated costs\n        \"\"\"\n        # Get pricing for model (with fallback to default if available)\n        model_pricing = self.pricing.get(model, self.pricing.get(\"default\", {}))\n\n        # Calculate costs (pricing is per 1K tokens)\n        input_cost = (input_tokens / 1000.0) * model_pricing.get(\"input\", 0.0)\n        output_cost = (output_tokens / 1000.0) * model_pricing.get(\"output\", 0.0)\n        total_cost = input_cost + output_cost\n\n        # Create record\n        record = UsageRecord(\n            timestamp=time.time(),\n            model=model,\n            input_tokens=input_tokens,\n            output_tokens=output_tokens,\n            input_cost=input_cost,\n            output_cost=output_cost,\n            total_cost=total_cost,\n            metadata=metadata or {},\n        )\n\n        self.usage_history.append(record)\n        return record\n\n    def get_total_cost(self, model: Optional[str] = None) -&gt; float:\n        \"\"\"\n        Get total cost across all requests.\n\n        Args:\n            model: Optional model filter (None = all models)\n\n        Returns:\n            Total cost in the configured currency\n        \"\"\"\n        total = 0.0\n        for record in self.usage_history:\n            if model is None or record.model == model:\n                total += record.total_cost\n        return total\n\n    def get_total_tokens(self, model: Optional[str] = None) -&gt; dict[str, int]:\n        \"\"\"\n        Get total tokens used.\n\n        Args:\n            model: Optional model filter (None = all models)\n\n        Returns:\n            Dict with input, output, and total token counts\n        \"\"\"\n        input_tokens = 0\n        output_tokens = 0\n\n        for record in self.usage_history:\n            if model is None or record.model == model:\n                input_tokens += record.input_tokens\n                output_tokens += record.output_tokens\n\n        return {\n            \"input\": input_tokens,\n            \"output\": output_tokens,\n            \"total\": input_tokens + output_tokens,\n        }\n\n    def get_request_count(self, model: Optional[str] = None) -&gt; int:\n        \"\"\"\n        Get number of requests made.\n\n        Args:\n            model: Optional model filter (None = all models)\n\n        Returns:\n            Number of requests\n        \"\"\"\n        if model is None:\n            return len(self.usage_history)\n        return sum(1 for r in self.usage_history if r.model == model)\n\n    def get_model_breakdown(self) -&gt; dict[str, dict[str, float]]:\n        \"\"\"\n        Get cost breakdown by model.\n\n        Returns:\n            Dict mapping model names to their usage statistics\n        \"\"\"\n        breakdown: dict[str, dict[str, float]] = {}\n\n        for record in self.usage_history:\n            if record.model not in breakdown:\n                breakdown[record.model] = {\n                    \"cost\": 0.0,\n                    \"input_tokens\": 0,\n                    \"output_tokens\": 0,\n                    \"requests\": 0,\n                }\n\n            breakdown[record.model][\"cost\"] += record.total_cost\n            breakdown[record.model][\"input_tokens\"] += record.input_tokens\n            breakdown[record.model][\"output_tokens\"] += record.output_tokens\n            breakdown[record.model][\"requests\"] += 1\n\n        return breakdown\n\n    def get_usage_report(self) -&gt; dict:\n        \"\"\"\n        Get comprehensive usage report.\n\n        Returns:\n            Dict with complete usage statistics\n        \"\"\"\n        return {\n            \"provider\": self.provider_name,\n            \"currency\": self.currency,\n            \"total_cost\": self.get_total_cost(),\n            \"total_requests\": self.get_request_count(),\n            \"total_tokens\": self.get_total_tokens(),\n            \"model_breakdown\": self.get_model_breakdown(),\n            \"first_request\": (\n                self.usage_history[0].datetime.isoformat() if self.usage_history else None\n            ),\n            \"last_request\": (\n                self.usage_history[-1].datetime.isoformat() if self.usage_history else None\n            ),\n        }\n\n    def clear_history(self) -&gt; None:\n        \"\"\"Clear all usage history.\"\"\"\n        self.usage_history.clear()\n\n    def export_history(self) -&gt; list[dict]:\n        \"\"\"\n        Export usage history as list of dicts.\n\n        Returns:\n            List of usage records as dictionaries\n        \"\"\"\n        return [\n            {\n                \"timestamp\": record.datetime.isoformat(),\n                \"model\": record.model,\n                \"input_tokens\": record.input_tokens,\n                \"output_tokens\": record.output_tokens,\n                \"input_cost\": record.input_cost,\n                \"output_cost\": record.output_cost,\n                \"total_cost\": record.total_cost,\n                \"metadata\": record.metadata,\n            }\n            for record in self.usage_history\n        ]\n\n    def export_to_csv(self, filepath: str) -&gt; None:\n        \"\"\"\n        Export usage history to CSV file.\n\n        Args:\n            filepath: Path to output CSV file\n        \"\"\"\n        import csv\n\n        if not self.usage_history:\n            return\n\n        with open(filepath, \"w\", newline=\"\", encoding=\"utf-8\") as f:\n            writer = csv.writer(f)\n            writer.writerow(\n                [\n                    \"Timestamp\",\n                    \"Model\",\n                    \"Input Tokens\",\n                    \"Output Tokens\",\n                    \"Total Tokens\",\n                    \"Input Cost\",\n                    \"Output Cost\",\n                    \"Total Cost\",\n                ]\n            )\n\n            for record in self.usage_history:\n                writer.writerow(\n                    [\n                        record.datetime.isoformat(),\n                        record.model,\n                        record.input_tokens,\n                        record.output_tokens,\n                        record.total_tokens,\n                        f\"{record.input_cost:.6f}\",\n                        f\"{record.output_cost:.6f}\",\n                        f\"{record.total_cost:.6f}\",\n                    ]\n                )\n\n    def export_to_json(self, filepath: str) -&gt; None:\n        \"\"\"\n        Export usage history to JSON file.\n\n        Args:\n            filepath: Path to output JSON file\n        \"\"\"\n        import json\n\n        data = {\n            \"provider\": self.provider_name,\n            \"currency\": self.currency,\n            \"export_date\": datetime.now().isoformat(),\n            \"summary\": self.get_usage_report(),\n            \"history\": self.export_history(),\n        }\n\n        with open(filepath, \"w\", encoding=\"utf-8\") as f:\n            json.dump(data, f, indent=2)\n\n    def get_time_range_report(\n        self,\n        start_time: Optional[float] = None,\n        end_time: Optional[float] = None,\n    ) -&gt; dict:\n        \"\"\"\n        Get usage report for a specific time range.\n\n        Args:\n            start_time: Start timestamp (inclusive)\n            end_time: End timestamp (inclusive)\n\n        Returns:\n            Usage report for the time range\n        \"\"\"\n        filtered_records = self.usage_history\n\n        if start_time:\n            filtered_records = [r for r in filtered_records if r.timestamp &gt;= start_time]\n\n        if end_time:\n            filtered_records = [r for r in filtered_records if r.timestamp &lt;= end_time]\n\n        if not filtered_records:\n            return {\n                \"total_cost\": 0.0,\n                \"total_tokens\": 0,\n                \"total_requests\": 0,\n                \"model_breakdown\": {},\n            }\n\n        total_cost = sum(r.total_cost for r in filtered_records)\n        total_tokens = sum(r.total_tokens for r in filtered_records)\n\n        # Model breakdown\n        breakdown = {}\n        for record in filtered_records:\n            if record.model not in breakdown:\n                breakdown[record.model] = {\n                    \"cost\": 0.0,\n                    \"input_tokens\": 0,\n                    \"output_tokens\": 0,\n                    \"requests\": 0,\n                }\n\n            breakdown[record.model][\"cost\"] += record.total_cost\n            breakdown[record.model][\"input_tokens\"] += record.input_tokens\n            breakdown[record.model][\"output_tokens\"] += record.output_tokens\n            breakdown[record.model][\"requests\"] += 1\n\n        return {\n            \"total_cost\": total_cost,\n            \"total_tokens\": total_tokens,\n            \"total_requests\": len(filtered_records),\n            \"model_breakdown\": breakdown,\n            \"start_time\": (filtered_records[0].datetime.isoformat() if filtered_records else None),\n            \"end_time\": (filtered_records[-1].datetime.isoformat() if filtered_records else None),\n        }\n\n    def check_budget(self, budget_limit: float) -&gt; dict:\n        \"\"\"\n        Check if spending is within budget.\n\n        Args:\n            budget_limit: Budget limit in currency units\n\n        Returns:\n            Budget status information\n        \"\"\"\n        total_cost = self.get_total_cost()\n        remaining = budget_limit - total_cost\n        percent_used = (total_cost / budget_limit * 100) if budget_limit &gt; 0 else 0\n\n        return {\n            \"budget_limit\": budget_limit,\n            \"total_spent\": total_cost,\n            \"remaining\": remaining,\n            \"percent_used\": percent_used,\n            \"within_budget\": total_cost &lt;= budget_limit,\n            \"near_limit\": percent_used &gt;= 90,  # Warning at 90%\n        }\n</code></pre>"},{"location":"api/base/#bruno_llm.base.CostTracker.__init__","title":"<code>__init__(provider_name, pricing, currency='USD')</code>","text":"<p>Initialize cost tracker.</p> <p>Parameters:</p> Name Type Description Default <code>provider_name</code> <code>str</code> <p>Name of the provider</p> required <code>pricing</code> <code>dict[str, dict[str, float]]</code> <p>Pricing per model (per 1K tokens) Format: {\"model_name\": {\"input\": price, \"output\": price}}</p> required <code>currency</code> <code>str</code> <p>Currency code (default: \"USD\")</p> <code>'USD'</code> Source code in <code>bruno_llm/base/cost_tracker.py</code> <pre><code>def __init__(\n    self,\n    provider_name: str,\n    pricing: dict[str, dict[str, float]],\n    currency: str = \"USD\",\n):\n    \"\"\"\n    Initialize cost tracker.\n\n    Args:\n        provider_name: Name of the provider\n        pricing: Pricing per model (per 1K tokens)\n            Format: {\"model_name\": {\"input\": price, \"output\": price}}\n        currency: Currency code (default: \"USD\")\n    \"\"\"\n    self.provider_name = provider_name\n    self.pricing = pricing\n    self.currency = currency\n    self.usage_history: list[UsageRecord] = []\n</code></pre>"},{"location":"api/base/#bruno_llm.base.CostTracker.track_request","title":"<code>track_request(model, input_tokens, output_tokens, metadata=None)</code>","text":"<p>Track a single API request.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>str</code> <p>Model name</p> required <code>input_tokens</code> <code>int</code> <p>Number of input tokens</p> required <code>output_tokens</code> <code>int</code> <p>Number of output tokens</p> required <code>metadata</code> <code>Optional[dict[str, str]]</code> <p>Optional metadata about the request</p> <code>None</code> <p>Returns:</p> Type Description <code>UsageRecord</code> <p>UsageRecord with calculated costs</p> Source code in <code>bruno_llm/base/cost_tracker.py</code> <pre><code>def track_request(\n    self,\n    model: str,\n    input_tokens: int,\n    output_tokens: int,\n    metadata: Optional[dict[str, str]] = None,\n) -&gt; UsageRecord:\n    \"\"\"\n    Track a single API request.\n\n    Args:\n        model: Model name\n        input_tokens: Number of input tokens\n        output_tokens: Number of output tokens\n        metadata: Optional metadata about the request\n\n    Returns:\n        UsageRecord with calculated costs\n    \"\"\"\n    # Get pricing for model (with fallback to default if available)\n    model_pricing = self.pricing.get(model, self.pricing.get(\"default\", {}))\n\n    # Calculate costs (pricing is per 1K tokens)\n    input_cost = (input_tokens / 1000.0) * model_pricing.get(\"input\", 0.0)\n    output_cost = (output_tokens / 1000.0) * model_pricing.get(\"output\", 0.0)\n    total_cost = input_cost + output_cost\n\n    # Create record\n    record = UsageRecord(\n        timestamp=time.time(),\n        model=model,\n        input_tokens=input_tokens,\n        output_tokens=output_tokens,\n        input_cost=input_cost,\n        output_cost=output_cost,\n        total_cost=total_cost,\n        metadata=metadata or {},\n    )\n\n    self.usage_history.append(record)\n    return record\n</code></pre>"},{"location":"api/base/#bruno_llm.base.CostTracker.get_total_cost","title":"<code>get_total_cost(model=None)</code>","text":"<p>Get total cost across all requests.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>Optional[str]</code> <p>Optional model filter (None = all models)</p> <code>None</code> <p>Returns:</p> Type Description <code>float</code> <p>Total cost in the configured currency</p> Source code in <code>bruno_llm/base/cost_tracker.py</code> <pre><code>def get_total_cost(self, model: Optional[str] = None) -&gt; float:\n    \"\"\"\n    Get total cost across all requests.\n\n    Args:\n        model: Optional model filter (None = all models)\n\n    Returns:\n        Total cost in the configured currency\n    \"\"\"\n    total = 0.0\n    for record in self.usage_history:\n        if model is None or record.model == model:\n            total += record.total_cost\n    return total\n</code></pre>"},{"location":"api/base/#bruno_llm.base.CostTracker.get_total_tokens","title":"<code>get_total_tokens(model=None)</code>","text":"<p>Get total tokens used.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>Optional[str]</code> <p>Optional model filter (None = all models)</p> <code>None</code> <p>Returns:</p> Type Description <code>dict[str, int]</code> <p>Dict with input, output, and total token counts</p> Source code in <code>bruno_llm/base/cost_tracker.py</code> <pre><code>def get_total_tokens(self, model: Optional[str] = None) -&gt; dict[str, int]:\n    \"\"\"\n    Get total tokens used.\n\n    Args:\n        model: Optional model filter (None = all models)\n\n    Returns:\n        Dict with input, output, and total token counts\n    \"\"\"\n    input_tokens = 0\n    output_tokens = 0\n\n    for record in self.usage_history:\n        if model is None or record.model == model:\n            input_tokens += record.input_tokens\n            output_tokens += record.output_tokens\n\n    return {\n        \"input\": input_tokens,\n        \"output\": output_tokens,\n        \"total\": input_tokens + output_tokens,\n    }\n</code></pre>"},{"location":"api/base/#bruno_llm.base.CostTracker.get_request_count","title":"<code>get_request_count(model=None)</code>","text":"<p>Get number of requests made.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>Optional[str]</code> <p>Optional model filter (None = all models)</p> <code>None</code> <p>Returns:</p> Type Description <code>int</code> <p>Number of requests</p> Source code in <code>bruno_llm/base/cost_tracker.py</code> <pre><code>def get_request_count(self, model: Optional[str] = None) -&gt; int:\n    \"\"\"\n    Get number of requests made.\n\n    Args:\n        model: Optional model filter (None = all models)\n\n    Returns:\n        Number of requests\n    \"\"\"\n    if model is None:\n        return len(self.usage_history)\n    return sum(1 for r in self.usage_history if r.model == model)\n</code></pre>"},{"location":"api/base/#bruno_llm.base.CostTracker.get_model_breakdown","title":"<code>get_model_breakdown()</code>","text":"<p>Get cost breakdown by model.</p> <p>Returns:</p> Type Description <code>dict[str, dict[str, float]]</code> <p>Dict mapping model names to their usage statistics</p> Source code in <code>bruno_llm/base/cost_tracker.py</code> <pre><code>def get_model_breakdown(self) -&gt; dict[str, dict[str, float]]:\n    \"\"\"\n    Get cost breakdown by model.\n\n    Returns:\n        Dict mapping model names to their usage statistics\n    \"\"\"\n    breakdown: dict[str, dict[str, float]] = {}\n\n    for record in self.usage_history:\n        if record.model not in breakdown:\n            breakdown[record.model] = {\n                \"cost\": 0.0,\n                \"input_tokens\": 0,\n                \"output_tokens\": 0,\n                \"requests\": 0,\n            }\n\n        breakdown[record.model][\"cost\"] += record.total_cost\n        breakdown[record.model][\"input_tokens\"] += record.input_tokens\n        breakdown[record.model][\"output_tokens\"] += record.output_tokens\n        breakdown[record.model][\"requests\"] += 1\n\n    return breakdown\n</code></pre>"},{"location":"api/base/#bruno_llm.base.CostTracker.get_usage_report","title":"<code>get_usage_report()</code>","text":"<p>Get comprehensive usage report.</p> <p>Returns:</p> Type Description <code>dict</code> <p>Dict with complete usage statistics</p> Source code in <code>bruno_llm/base/cost_tracker.py</code> <pre><code>def get_usage_report(self) -&gt; dict:\n    \"\"\"\n    Get comprehensive usage report.\n\n    Returns:\n        Dict with complete usage statistics\n    \"\"\"\n    return {\n        \"provider\": self.provider_name,\n        \"currency\": self.currency,\n        \"total_cost\": self.get_total_cost(),\n        \"total_requests\": self.get_request_count(),\n        \"total_tokens\": self.get_total_tokens(),\n        \"model_breakdown\": self.get_model_breakdown(),\n        \"first_request\": (\n            self.usage_history[0].datetime.isoformat() if self.usage_history else None\n        ),\n        \"last_request\": (\n            self.usage_history[-1].datetime.isoformat() if self.usage_history else None\n        ),\n    }\n</code></pre>"},{"location":"api/base/#bruno_llm.base.CostTracker.clear_history","title":"<code>clear_history()</code>","text":"<p>Clear all usage history.</p> Source code in <code>bruno_llm/base/cost_tracker.py</code> <pre><code>def clear_history(self) -&gt; None:\n    \"\"\"Clear all usage history.\"\"\"\n    self.usage_history.clear()\n</code></pre>"},{"location":"api/base/#bruno_llm.base.CostTracker.export_history","title":"<code>export_history()</code>","text":"<p>Export usage history as list of dicts.</p> <p>Returns:</p> Type Description <code>list[dict]</code> <p>List of usage records as dictionaries</p> Source code in <code>bruno_llm/base/cost_tracker.py</code> <pre><code>def export_history(self) -&gt; list[dict]:\n    \"\"\"\n    Export usage history as list of dicts.\n\n    Returns:\n        List of usage records as dictionaries\n    \"\"\"\n    return [\n        {\n            \"timestamp\": record.datetime.isoformat(),\n            \"model\": record.model,\n            \"input_tokens\": record.input_tokens,\n            \"output_tokens\": record.output_tokens,\n            \"input_cost\": record.input_cost,\n            \"output_cost\": record.output_cost,\n            \"total_cost\": record.total_cost,\n            \"metadata\": record.metadata,\n        }\n        for record in self.usage_history\n    ]\n</code></pre>"},{"location":"api/base/#bruno_llm.base.CostTracker.export_to_csv","title":"<code>export_to_csv(filepath)</code>","text":"<p>Export usage history to CSV file.</p> <p>Parameters:</p> Name Type Description Default <code>filepath</code> <code>str</code> <p>Path to output CSV file</p> required Source code in <code>bruno_llm/base/cost_tracker.py</code> <pre><code>def export_to_csv(self, filepath: str) -&gt; None:\n    \"\"\"\n    Export usage history to CSV file.\n\n    Args:\n        filepath: Path to output CSV file\n    \"\"\"\n    import csv\n\n    if not self.usage_history:\n        return\n\n    with open(filepath, \"w\", newline=\"\", encoding=\"utf-8\") as f:\n        writer = csv.writer(f)\n        writer.writerow(\n            [\n                \"Timestamp\",\n                \"Model\",\n                \"Input Tokens\",\n                \"Output Tokens\",\n                \"Total Tokens\",\n                \"Input Cost\",\n                \"Output Cost\",\n                \"Total Cost\",\n            ]\n        )\n\n        for record in self.usage_history:\n            writer.writerow(\n                [\n                    record.datetime.isoformat(),\n                    record.model,\n                    record.input_tokens,\n                    record.output_tokens,\n                    record.total_tokens,\n                    f\"{record.input_cost:.6f}\",\n                    f\"{record.output_cost:.6f}\",\n                    f\"{record.total_cost:.6f}\",\n                ]\n            )\n</code></pre>"},{"location":"api/base/#bruno_llm.base.CostTracker.export_to_json","title":"<code>export_to_json(filepath)</code>","text":"<p>Export usage history to JSON file.</p> <p>Parameters:</p> Name Type Description Default <code>filepath</code> <code>str</code> <p>Path to output JSON file</p> required Source code in <code>bruno_llm/base/cost_tracker.py</code> <pre><code>def export_to_json(self, filepath: str) -&gt; None:\n    \"\"\"\n    Export usage history to JSON file.\n\n    Args:\n        filepath: Path to output JSON file\n    \"\"\"\n    import json\n\n    data = {\n        \"provider\": self.provider_name,\n        \"currency\": self.currency,\n        \"export_date\": datetime.now().isoformat(),\n        \"summary\": self.get_usage_report(),\n        \"history\": self.export_history(),\n    }\n\n    with open(filepath, \"w\", encoding=\"utf-8\") as f:\n        json.dump(data, f, indent=2)\n</code></pre>"},{"location":"api/base/#bruno_llm.base.CostTracker.get_time_range_report","title":"<code>get_time_range_report(start_time=None, end_time=None)</code>","text":"<p>Get usage report for a specific time range.</p> <p>Parameters:</p> Name Type Description Default <code>start_time</code> <code>Optional[float]</code> <p>Start timestamp (inclusive)</p> <code>None</code> <code>end_time</code> <code>Optional[float]</code> <p>End timestamp (inclusive)</p> <code>None</code> <p>Returns:</p> Type Description <code>dict</code> <p>Usage report for the time range</p> Source code in <code>bruno_llm/base/cost_tracker.py</code> <pre><code>def get_time_range_report(\n    self,\n    start_time: Optional[float] = None,\n    end_time: Optional[float] = None,\n) -&gt; dict:\n    \"\"\"\n    Get usage report for a specific time range.\n\n    Args:\n        start_time: Start timestamp (inclusive)\n        end_time: End timestamp (inclusive)\n\n    Returns:\n        Usage report for the time range\n    \"\"\"\n    filtered_records = self.usage_history\n\n    if start_time:\n        filtered_records = [r for r in filtered_records if r.timestamp &gt;= start_time]\n\n    if end_time:\n        filtered_records = [r for r in filtered_records if r.timestamp &lt;= end_time]\n\n    if not filtered_records:\n        return {\n            \"total_cost\": 0.0,\n            \"total_tokens\": 0,\n            \"total_requests\": 0,\n            \"model_breakdown\": {},\n        }\n\n    total_cost = sum(r.total_cost for r in filtered_records)\n    total_tokens = sum(r.total_tokens for r in filtered_records)\n\n    # Model breakdown\n    breakdown = {}\n    for record in filtered_records:\n        if record.model not in breakdown:\n            breakdown[record.model] = {\n                \"cost\": 0.0,\n                \"input_tokens\": 0,\n                \"output_tokens\": 0,\n                \"requests\": 0,\n            }\n\n        breakdown[record.model][\"cost\"] += record.total_cost\n        breakdown[record.model][\"input_tokens\"] += record.input_tokens\n        breakdown[record.model][\"output_tokens\"] += record.output_tokens\n        breakdown[record.model][\"requests\"] += 1\n\n    return {\n        \"total_cost\": total_cost,\n        \"total_tokens\": total_tokens,\n        \"total_requests\": len(filtered_records),\n        \"model_breakdown\": breakdown,\n        \"start_time\": (filtered_records[0].datetime.isoformat() if filtered_records else None),\n        \"end_time\": (filtered_records[-1].datetime.isoformat() if filtered_records else None),\n    }\n</code></pre>"},{"location":"api/base/#bruno_llm.base.CostTracker.check_budget","title":"<code>check_budget(budget_limit)</code>","text":"<p>Check if spending is within budget.</p> <p>Parameters:</p> Name Type Description Default <code>budget_limit</code> <code>float</code> <p>Budget limit in currency units</p> required <p>Returns:</p> Type Description <code>dict</code> <p>Budget status information</p> Source code in <code>bruno_llm/base/cost_tracker.py</code> <pre><code>def check_budget(self, budget_limit: float) -&gt; dict:\n    \"\"\"\n    Check if spending is within budget.\n\n    Args:\n        budget_limit: Budget limit in currency units\n\n    Returns:\n        Budget status information\n    \"\"\"\n    total_cost = self.get_total_cost()\n    remaining = budget_limit - total_cost\n    percent_used = (total_cost / budget_limit * 100) if budget_limit &gt; 0 else 0\n\n    return {\n        \"budget_limit\": budget_limit,\n        \"total_spent\": total_cost,\n        \"remaining\": remaining,\n        \"percent_used\": percent_used,\n        \"within_budget\": total_cost &lt;= budget_limit,\n        \"near_limit\": percent_used &gt;= 90,  # Warning at 90%\n    }\n</code></pre>"},{"location":"api/base/#bruno_llm.base.UsageRecord","title":"<code>UsageRecord</code>  <code>dataclass</code>","text":"<p>Record of a single API usage event.</p> <p>Attributes:</p> Name Type Description <code>timestamp</code> <code>float</code> <p>When the request was made</p> <code>model</code> <code>str</code> <p>Model name used</p> <code>input_tokens</code> <code>int</code> <p>Number of input tokens</p> <code>output_tokens</code> <code>int</code> <p>Number of output tokens</p> <code>input_cost</code> <code>float</code> <p>Cost for input tokens</p> <code>output_cost</code> <code>float</code> <p>Cost for output tokens</p> <code>total_cost</code> <code>float</code> <p>Total cost for this request</p> Source code in <code>bruno_llm/base/cost_tracker.py</code> <pre><code>@dataclass\nclass UsageRecord:\n    \"\"\"\n    Record of a single API usage event.\n\n    Attributes:\n        timestamp: When the request was made\n        model: Model name used\n        input_tokens: Number of input tokens\n        output_tokens: Number of output tokens\n        input_cost: Cost for input tokens\n        output_cost: Cost for output tokens\n        total_cost: Total cost for this request\n    \"\"\"\n\n    timestamp: float\n    model: str\n    input_tokens: int\n    output_tokens: int\n    input_cost: float\n    output_cost: float\n    total_cost: float\n    metadata: dict[str, str] = field(default_factory=dict)\n\n    @property\n    def datetime(self) -&gt; datetime:\n        \"\"\"Get datetime from timestamp.\"\"\"\n        return datetime.fromtimestamp(self.timestamp)\n\n    @property\n    def total_tokens(self) -&gt; int:\n        \"\"\"Get total tokens used.\"\"\"\n        return self.input_tokens + self.output_tokens\n</code></pre>"},{"location":"api/base/#bruno_llm.base.UsageRecord.datetime","title":"<code>datetime</code>  <code>property</code>","text":"<p>Get datetime from timestamp.</p>"},{"location":"api/base/#bruno_llm.base.UsageRecord.total_tokens","title":"<code>total_tokens</code>  <code>property</code>","text":"<p>Get total tokens used.</p>"},{"location":"api/base/#bruno_llm.base.BaseEmbeddingProvider","title":"<code>BaseEmbeddingProvider</code>","text":"<p>               Bases: <code>EmbeddingInterface</code>, <code>ABC</code></p> <p>Base class for embedding providers using numpy for vector operations.</p> <p>This base class implements the EmbeddingInterface from bruno-core and provides: - Efficient vector operations using numpy - Input validation and error handling - Similarity calculation using numpy's optimized functions - Batch processing utilities</p> <p>All embedding providers should inherit from this class and implement the abstract methods for their specific API or model.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>str</code> <p>Model name or identifier</p> required <code>timeout</code> <code>float</code> <p>Request timeout in seconds</p> <code>30.0</code> <p>Examples:</p> <pre><code>&gt;&gt;&gt; class MyEmbeddingProvider(BaseEmbeddingProvider):\n...     async def embed_text(self, text: str) -&gt; List[float]:\n...         # Implementation specific to your provider\n...         return await self._call_api(text)\n...\n...     def get_dimension(self) -&gt; int:\n...         return 768  # Your model's dimension\n</code></pre> Source code in <code>bruno_llm/base/embedding_interface.py</code> <pre><code>class BaseEmbeddingProvider(EmbeddingInterface, ABC):\n    \"\"\"\n    Base class for embedding providers using numpy for vector operations.\n\n    This base class implements the EmbeddingInterface from bruno-core and provides:\n    - Efficient vector operations using numpy\n    - Input validation and error handling\n    - Similarity calculation using numpy's optimized functions\n    - Batch processing utilities\n\n    All embedding providers should inherit from this class and implement the\n    abstract methods for their specific API or model.\n\n    Args:\n        model: Model name or identifier\n        timeout: Request timeout in seconds\n\n    Examples:\n        &gt;&gt;&gt; class MyEmbeddingProvider(BaseEmbeddingProvider):\n        ...     async def embed_text(self, text: str) -&gt; List[float]:\n        ...         # Implementation specific to your provider\n        ...         return await self._call_api(text)\n        ...\n        ...     def get_dimension(self) -&gt; int:\n        ...         return 768  # Your model's dimension\n    \"\"\"\n\n    def __init__(\n        self,\n        model: str,\n        timeout: float = 30.0,\n        **kwargs,\n    ):\n        self.model = model\n        self.timeout = timeout\n        self.config = kwargs\n        self._dimension_cache: Optional[int] = None\n\n    @abstractmethod\n    async def embed_text(self, text: str) -&gt; list[float]:\n        \"\"\"\n        Generate embedding for a single text.\n\n        Args:\n            text: Input text to embed\n\n        Returns:\n            Embedding vector as list of floats\n\n        Raises:\n            LLMError: If embedding generation fails\n        \"\"\"\n        pass\n\n    async def embed_texts(self, texts: list[str]) -&gt; list[list[float]]:\n        \"\"\"\n        Generate embeddings for multiple texts.\n\n        Default implementation calls embed_text for each text.\n        Override for providers with native batch support.\n\n        Args:\n            texts: List of texts to embed\n\n        Returns:\n            List of embedding vectors\n\n        Raises:\n            LLMError: If any embedding generation fails\n        \"\"\"\n        embeddings = []\n        for text in texts:\n            embedding = await self.embed_text(text)\n            embeddings.append(embedding)\n        return embeddings\n\n    async def embed_message(self, message: Message) -&gt; list[float]:\n        \"\"\"\n        Generate embedding for a message.\n\n        Args:\n            message: Message object to embed\n\n        Returns:\n            Embedding vector for the message content\n        \"\"\"\n        return await self.embed_text(message.content)\n\n    @abstractmethod\n    def get_dimension(self) -&gt; int:\n        \"\"\"\n        Get the embedding dimension for this provider.\n\n        Returns:\n            Number of dimensions in embeddings\n        \"\"\"\n        pass\n\n    def get_model_name(self) -&gt; str:\n        \"\"\"\n        Get the model name used by this provider.\n\n        Returns:\n            Model name string\n        \"\"\"\n        return self.model\n\n    def calculate_similarity(self, embedding1: list[float], embedding2: list[float]) -&gt; float:\n        \"\"\"\n        Calculate cosine similarity between two embeddings using numpy.\n\n        Uses numpy's optimized dot product and norm calculations for efficiency.\n\n        Args:\n            embedding1: First embedding vector\n            embedding2: Second embedding vector\n\n        Returns:\n            Cosine similarity (-1.0 to 1.0)\n\n        Raises:\n            ValueError: If vectors have different dimensions\n        \"\"\"\n        # Convert to numpy arrays for efficient computation\n        vec1 = np.array(embedding1, dtype=np.float32)\n        vec2 = np.array(embedding2, dtype=np.float32)\n\n        if vec1.shape != vec2.shape:\n            raise ValueError(f\"Vector dimensions don't match: {vec1.shape} != {vec2.shape}\")\n\n        # Handle edge cases\n        if vec1.size == 0 or vec2.size == 0:\n            return 0.0\n\n        # Calculate cosine similarity using numpy\n        dot_product = np.dot(vec1, vec2)\n        norm1 = np.linalg.norm(vec1)\n        norm2 = np.linalg.norm(vec2)\n\n        # Handle zero vectors\n        if norm1 == 0.0 or norm2 == 0.0:\n            return 0.0\n\n        similarity = dot_product / (norm1 * norm2)\n\n        # Ensure result is in valid range due to floating point precision\n        return float(np.clip(similarity, -1.0, 1.0))\n\n    @abstractmethod\n    async def check_connection(self) -&gt; bool:\n        \"\"\"\n        Check if the provider is accessible.\n\n        Returns:\n            True if provider can be used, False otherwise\n        \"\"\"\n        pass\n\n    # Utility methods for common operations\n\n    def validate_embedding(\n        self, embedding: list[float], expected_dimension: Optional[int] = None\n    ) -&gt; None:\n        \"\"\"\n        Validate an embedding vector using numpy.\n\n        Args:\n            embedding: Embedding to validate\n            expected_dimension: Expected dimension (uses get_dimension() if None)\n\n        Raises:\n            ValueError: If embedding is invalid\n        \"\"\"\n        if not isinstance(embedding, list):\n            raise ValueError(f\"Embedding must be a list, got {type(embedding)}\")\n\n        if len(embedding) == 0:\n            raise ValueError(\"Embedding cannot be empty\")\n\n        # Convert to numpy for validation\n        vec = np.array(embedding, dtype=np.float32)\n\n        # Check for invalid values using numpy\n        if np.any(np.isnan(vec)):\n            raise ValueError(\"Embedding contains NaN values\")\n\n        if np.any(np.isinf(vec)):\n            raise ValueError(\"Embedding contains infinite values\")\n\n        # Check dimension\n        expected_dim = expected_dimension or self.get_dimension()\n        if len(embedding) != expected_dim:\n            raise ValueError(\n                f\"Embedding dimension mismatch: expected {expected_dim}, got {len(embedding)}\"\n            )\n\n    def batch_cosine_similarity(\n        self, embeddings1: list[list[float]], embeddings2: list[list[float]]\n    ) -&gt; np.ndarray:\n        \"\"\"\n        Calculate pairwise cosine similarities between two sets of embeddings.\n\n        Uses numpy for efficient batch computation.\n\n        Args:\n            embeddings1: First set of embeddings\n            embeddings2: Second set of embeddings\n\n        Returns:\n            Similarity matrix as numpy array (len(embeddings1) x len(embeddings2))\n        \"\"\"\n        # Convert to numpy arrays\n        matrix1 = np.array(embeddings1, dtype=np.float32)\n        matrix2 = np.array(embeddings2, dtype=np.float32)\n\n        # Normalize vectors\n        norm1 = np.linalg.norm(matrix1, axis=1, keepdims=True)\n        norm2 = np.linalg.norm(matrix2, axis=1, keepdims=True)\n\n        # Handle zero vectors\n        norm1 = np.where(norm1 == 0, 1, norm1)\n        norm2 = np.where(norm2 == 0, 1, norm2)\n\n        normalized1 = matrix1 / norm1\n        normalized2 = matrix2 / norm2\n\n        # Calculate cosine similarity matrix\n        similarity_matrix = np.dot(normalized1, normalized2.T)\n\n        return similarity_matrix\n\n    def find_most_similar(\n        self,\n        query_embedding: list[float],\n        candidate_embeddings: list[list[float]],\n        top_k: Optional[int] = None,\n    ) -&gt; list[tuple[int, float]]:\n        \"\"\"\n        Find most similar embeddings to a query using numpy.\n\n        Args:\n            query_embedding: Query embedding vector\n            candidate_embeddings: List of candidate embeddings\n            top_k: Number of top results (None = all)\n\n        Returns:\n            List of (index, similarity) tuples sorted by similarity (descending)\n        \"\"\"\n        if not candidate_embeddings:\n            return []\n\n        # Calculate similarities using numpy\n        query_vec = np.array(query_embedding, dtype=np.float32).reshape(1, -1)\n        candidates_matrix = np.array(candidate_embeddings, dtype=np.float32)\n\n        # Use batch similarity calculation\n        similarities = self.batch_cosine_similarity(query_vec.tolist(), candidates_matrix.tolist())[\n            0\n        ]  # Get first (and only) row\n\n        # Create index-similarity pairs\n        indexed_similarities = list(enumerate(similarities.tolist()))\n\n        # Sort by similarity (descending)\n        indexed_similarities.sort(key=lambda x: x[1], reverse=True)\n\n        # Return top-k results\n        if top_k is not None:\n            indexed_similarities = indexed_similarities[:top_k]\n\n        return indexed_similarities\n\n    def average_embeddings(self, embeddings: list[list[float]]) -&gt; list[float]:\n        \"\"\"\n        Calculate average embedding using numpy.\n\n        Args:\n            embeddings: List of embedding vectors\n\n        Returns:\n            Average embedding vector\n\n        Raises:\n            ValueError: If embeddings list is empty\n        \"\"\"\n        if not embeddings:\n            raise ValueError(\"Cannot average empty list of embeddings\")\n\n        # Use numpy for efficient averaging\n        matrix = np.array(embeddings, dtype=np.float32)\n        average = np.mean(matrix, axis=0)\n\n        return average.tolist()\n\n    def weighted_average_embeddings(\n        self,\n        embeddings: list[list[float]],\n        weights: list[float],\n    ) -&gt; list[float]:\n        \"\"\"\n        Calculate weighted average embedding using numpy.\n\n        Args:\n            embeddings: List of embedding vectors\n            weights: List of weights (must sum to 1.0)\n\n        Returns:\n            Weighted average embedding\n\n        Raises:\n            ValueError: If inputs are invalid\n        \"\"\"\n        if len(embeddings) != len(weights):\n            raise ValueError(\n                f\"Number of embeddings ({len(embeddings)}) must match \"\n                f\"number of weights ({len(weights)})\"\n            )\n\n        if not embeddings:\n            raise ValueError(\"Cannot average empty list of embeddings\")\n\n        # Validate weights sum using numpy\n        weights_array = np.array(weights, dtype=np.float32)\n        if not np.isclose(weights_array.sum(), 1.0, atol=1e-6):\n            raise ValueError(f\"Weights must sum to 1.0, got {weights_array.sum()}\")\n\n        # Calculate weighted average using numpy\n        embeddings_matrix = np.array(embeddings, dtype=np.float32)\n        weighted_avg = np.average(embeddings_matrix, axis=0, weights=weights_array)\n\n        return weighted_avg.tolist()\n</code></pre>"},{"location":"api/base/#bruno_llm.base.BaseEmbeddingProvider.embed_text","title":"<code>embed_text(text)</code>  <code>abstractmethod</code> <code>async</code>","text":"<p>Generate embedding for a single text.</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>str</code> <p>Input text to embed</p> required <p>Returns:</p> Type Description <code>list[float]</code> <p>Embedding vector as list of floats</p> <p>Raises:</p> Type Description <code>LLMError</code> <p>If embedding generation fails</p> Source code in <code>bruno_llm/base/embedding_interface.py</code> <pre><code>@abstractmethod\nasync def embed_text(self, text: str) -&gt; list[float]:\n    \"\"\"\n    Generate embedding for a single text.\n\n    Args:\n        text: Input text to embed\n\n    Returns:\n        Embedding vector as list of floats\n\n    Raises:\n        LLMError: If embedding generation fails\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/base/#bruno_llm.base.BaseEmbeddingProvider.embed_texts","title":"<code>embed_texts(texts)</code>  <code>async</code>","text":"<p>Generate embeddings for multiple texts.</p> <p>Default implementation calls embed_text for each text. Override for providers with native batch support.</p> <p>Parameters:</p> Name Type Description Default <code>texts</code> <code>list[str]</code> <p>List of texts to embed</p> required <p>Returns:</p> Type Description <code>list[list[float]]</code> <p>List of embedding vectors</p> <p>Raises:</p> Type Description <code>LLMError</code> <p>If any embedding generation fails</p> Source code in <code>bruno_llm/base/embedding_interface.py</code> <pre><code>async def embed_texts(self, texts: list[str]) -&gt; list[list[float]]:\n    \"\"\"\n    Generate embeddings for multiple texts.\n\n    Default implementation calls embed_text for each text.\n    Override for providers with native batch support.\n\n    Args:\n        texts: List of texts to embed\n\n    Returns:\n        List of embedding vectors\n\n    Raises:\n        LLMError: If any embedding generation fails\n    \"\"\"\n    embeddings = []\n    for text in texts:\n        embedding = await self.embed_text(text)\n        embeddings.append(embedding)\n    return embeddings\n</code></pre>"},{"location":"api/base/#bruno_llm.base.BaseEmbeddingProvider.embed_message","title":"<code>embed_message(message)</code>  <code>async</code>","text":"<p>Generate embedding for a message.</p> <p>Parameters:</p> Name Type Description Default <code>message</code> <code>Message</code> <p>Message object to embed</p> required <p>Returns:</p> Type Description <code>list[float]</code> <p>Embedding vector for the message content</p> Source code in <code>bruno_llm/base/embedding_interface.py</code> <pre><code>async def embed_message(self, message: Message) -&gt; list[float]:\n    \"\"\"\n    Generate embedding for a message.\n\n    Args:\n        message: Message object to embed\n\n    Returns:\n        Embedding vector for the message content\n    \"\"\"\n    return await self.embed_text(message.content)\n</code></pre>"},{"location":"api/base/#bruno_llm.base.BaseEmbeddingProvider.get_dimension","title":"<code>get_dimension()</code>  <code>abstractmethod</code>","text":"<p>Get the embedding dimension for this provider.</p> <p>Returns:</p> Type Description <code>int</code> <p>Number of dimensions in embeddings</p> Source code in <code>bruno_llm/base/embedding_interface.py</code> <pre><code>@abstractmethod\ndef get_dimension(self) -&gt; int:\n    \"\"\"\n    Get the embedding dimension for this provider.\n\n    Returns:\n        Number of dimensions in embeddings\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/base/#bruno_llm.base.BaseEmbeddingProvider.get_model_name","title":"<code>get_model_name()</code>","text":"<p>Get the model name used by this provider.</p> <p>Returns:</p> Type Description <code>str</code> <p>Model name string</p> Source code in <code>bruno_llm/base/embedding_interface.py</code> <pre><code>def get_model_name(self) -&gt; str:\n    \"\"\"\n    Get the model name used by this provider.\n\n    Returns:\n        Model name string\n    \"\"\"\n    return self.model\n</code></pre>"},{"location":"api/base/#bruno_llm.base.BaseEmbeddingProvider.calculate_similarity","title":"<code>calculate_similarity(embedding1, embedding2)</code>","text":"<p>Calculate cosine similarity between two embeddings using numpy.</p> <p>Uses numpy's optimized dot product and norm calculations for efficiency.</p> <p>Parameters:</p> Name Type Description Default <code>embedding1</code> <code>list[float]</code> <p>First embedding vector</p> required <code>embedding2</code> <code>list[float]</code> <p>Second embedding vector</p> required <p>Returns:</p> Type Description <code>float</code> <p>Cosine similarity (-1.0 to 1.0)</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If vectors have different dimensions</p> Source code in <code>bruno_llm/base/embedding_interface.py</code> <pre><code>def calculate_similarity(self, embedding1: list[float], embedding2: list[float]) -&gt; float:\n    \"\"\"\n    Calculate cosine similarity between two embeddings using numpy.\n\n    Uses numpy's optimized dot product and norm calculations for efficiency.\n\n    Args:\n        embedding1: First embedding vector\n        embedding2: Second embedding vector\n\n    Returns:\n        Cosine similarity (-1.0 to 1.0)\n\n    Raises:\n        ValueError: If vectors have different dimensions\n    \"\"\"\n    # Convert to numpy arrays for efficient computation\n    vec1 = np.array(embedding1, dtype=np.float32)\n    vec2 = np.array(embedding2, dtype=np.float32)\n\n    if vec1.shape != vec2.shape:\n        raise ValueError(f\"Vector dimensions don't match: {vec1.shape} != {vec2.shape}\")\n\n    # Handle edge cases\n    if vec1.size == 0 or vec2.size == 0:\n        return 0.0\n\n    # Calculate cosine similarity using numpy\n    dot_product = np.dot(vec1, vec2)\n    norm1 = np.linalg.norm(vec1)\n    norm2 = np.linalg.norm(vec2)\n\n    # Handle zero vectors\n    if norm1 == 0.0 or norm2 == 0.0:\n        return 0.0\n\n    similarity = dot_product / (norm1 * norm2)\n\n    # Ensure result is in valid range due to floating point precision\n    return float(np.clip(similarity, -1.0, 1.0))\n</code></pre>"},{"location":"api/base/#bruno_llm.base.BaseEmbeddingProvider.check_connection","title":"<code>check_connection()</code>  <code>abstractmethod</code> <code>async</code>","text":"<p>Check if the provider is accessible.</p> <p>Returns:</p> Type Description <code>bool</code> <p>True if provider can be used, False otherwise</p> Source code in <code>bruno_llm/base/embedding_interface.py</code> <pre><code>@abstractmethod\nasync def check_connection(self) -&gt; bool:\n    \"\"\"\n    Check if the provider is accessible.\n\n    Returns:\n        True if provider can be used, False otherwise\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/base/#bruno_llm.base.BaseEmbeddingProvider.validate_embedding","title":"<code>validate_embedding(embedding, expected_dimension=None)</code>","text":"<p>Validate an embedding vector using numpy.</p> <p>Parameters:</p> Name Type Description Default <code>embedding</code> <code>list[float]</code> <p>Embedding to validate</p> required <code>expected_dimension</code> <code>Optional[int]</code> <p>Expected dimension (uses get_dimension() if None)</p> <code>None</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>If embedding is invalid</p> Source code in <code>bruno_llm/base/embedding_interface.py</code> <pre><code>def validate_embedding(\n    self, embedding: list[float], expected_dimension: Optional[int] = None\n) -&gt; None:\n    \"\"\"\n    Validate an embedding vector using numpy.\n\n    Args:\n        embedding: Embedding to validate\n        expected_dimension: Expected dimension (uses get_dimension() if None)\n\n    Raises:\n        ValueError: If embedding is invalid\n    \"\"\"\n    if not isinstance(embedding, list):\n        raise ValueError(f\"Embedding must be a list, got {type(embedding)}\")\n\n    if len(embedding) == 0:\n        raise ValueError(\"Embedding cannot be empty\")\n\n    # Convert to numpy for validation\n    vec = np.array(embedding, dtype=np.float32)\n\n    # Check for invalid values using numpy\n    if np.any(np.isnan(vec)):\n        raise ValueError(\"Embedding contains NaN values\")\n\n    if np.any(np.isinf(vec)):\n        raise ValueError(\"Embedding contains infinite values\")\n\n    # Check dimension\n    expected_dim = expected_dimension or self.get_dimension()\n    if len(embedding) != expected_dim:\n        raise ValueError(\n            f\"Embedding dimension mismatch: expected {expected_dim}, got {len(embedding)}\"\n        )\n</code></pre>"},{"location":"api/base/#bruno_llm.base.BaseEmbeddingProvider.batch_cosine_similarity","title":"<code>batch_cosine_similarity(embeddings1, embeddings2)</code>","text":"<p>Calculate pairwise cosine similarities between two sets of embeddings.</p> <p>Uses numpy for efficient batch computation.</p> <p>Parameters:</p> Name Type Description Default <code>embeddings1</code> <code>list[list[float]]</code> <p>First set of embeddings</p> required <code>embeddings2</code> <code>list[list[float]]</code> <p>Second set of embeddings</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>Similarity matrix as numpy array (len(embeddings1) x len(embeddings2))</p> Source code in <code>bruno_llm/base/embedding_interface.py</code> <pre><code>def batch_cosine_similarity(\n    self, embeddings1: list[list[float]], embeddings2: list[list[float]]\n) -&gt; np.ndarray:\n    \"\"\"\n    Calculate pairwise cosine similarities between two sets of embeddings.\n\n    Uses numpy for efficient batch computation.\n\n    Args:\n        embeddings1: First set of embeddings\n        embeddings2: Second set of embeddings\n\n    Returns:\n        Similarity matrix as numpy array (len(embeddings1) x len(embeddings2))\n    \"\"\"\n    # Convert to numpy arrays\n    matrix1 = np.array(embeddings1, dtype=np.float32)\n    matrix2 = np.array(embeddings2, dtype=np.float32)\n\n    # Normalize vectors\n    norm1 = np.linalg.norm(matrix1, axis=1, keepdims=True)\n    norm2 = np.linalg.norm(matrix2, axis=1, keepdims=True)\n\n    # Handle zero vectors\n    norm1 = np.where(norm1 == 0, 1, norm1)\n    norm2 = np.where(norm2 == 0, 1, norm2)\n\n    normalized1 = matrix1 / norm1\n    normalized2 = matrix2 / norm2\n\n    # Calculate cosine similarity matrix\n    similarity_matrix = np.dot(normalized1, normalized2.T)\n\n    return similarity_matrix\n</code></pre>"},{"location":"api/base/#bruno_llm.base.BaseEmbeddingProvider.find_most_similar","title":"<code>find_most_similar(query_embedding, candidate_embeddings, top_k=None)</code>","text":"<p>Find most similar embeddings to a query using numpy.</p> <p>Parameters:</p> Name Type Description Default <code>query_embedding</code> <code>list[float]</code> <p>Query embedding vector</p> required <code>candidate_embeddings</code> <code>list[list[float]]</code> <p>List of candidate embeddings</p> required <code>top_k</code> <code>Optional[int]</code> <p>Number of top results (None = all)</p> <code>None</code> <p>Returns:</p> Type Description <code>list[tuple[int, float]]</code> <p>List of (index, similarity) tuples sorted by similarity (descending)</p> Source code in <code>bruno_llm/base/embedding_interface.py</code> <pre><code>def find_most_similar(\n    self,\n    query_embedding: list[float],\n    candidate_embeddings: list[list[float]],\n    top_k: Optional[int] = None,\n) -&gt; list[tuple[int, float]]:\n    \"\"\"\n    Find most similar embeddings to a query using numpy.\n\n    Args:\n        query_embedding: Query embedding vector\n        candidate_embeddings: List of candidate embeddings\n        top_k: Number of top results (None = all)\n\n    Returns:\n        List of (index, similarity) tuples sorted by similarity (descending)\n    \"\"\"\n    if not candidate_embeddings:\n        return []\n\n    # Calculate similarities using numpy\n    query_vec = np.array(query_embedding, dtype=np.float32).reshape(1, -1)\n    candidates_matrix = np.array(candidate_embeddings, dtype=np.float32)\n\n    # Use batch similarity calculation\n    similarities = self.batch_cosine_similarity(query_vec.tolist(), candidates_matrix.tolist())[\n        0\n    ]  # Get first (and only) row\n\n    # Create index-similarity pairs\n    indexed_similarities = list(enumerate(similarities.tolist()))\n\n    # Sort by similarity (descending)\n    indexed_similarities.sort(key=lambda x: x[1], reverse=True)\n\n    # Return top-k results\n    if top_k is not None:\n        indexed_similarities = indexed_similarities[:top_k]\n\n    return indexed_similarities\n</code></pre>"},{"location":"api/base/#bruno_llm.base.BaseEmbeddingProvider.average_embeddings","title":"<code>average_embeddings(embeddings)</code>","text":"<p>Calculate average embedding using numpy.</p> <p>Parameters:</p> Name Type Description Default <code>embeddings</code> <code>list[list[float]]</code> <p>List of embedding vectors</p> required <p>Returns:</p> Type Description <code>list[float]</code> <p>Average embedding vector</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If embeddings list is empty</p> Source code in <code>bruno_llm/base/embedding_interface.py</code> <pre><code>def average_embeddings(self, embeddings: list[list[float]]) -&gt; list[float]:\n    \"\"\"\n    Calculate average embedding using numpy.\n\n    Args:\n        embeddings: List of embedding vectors\n\n    Returns:\n        Average embedding vector\n\n    Raises:\n        ValueError: If embeddings list is empty\n    \"\"\"\n    if not embeddings:\n        raise ValueError(\"Cannot average empty list of embeddings\")\n\n    # Use numpy for efficient averaging\n    matrix = np.array(embeddings, dtype=np.float32)\n    average = np.mean(matrix, axis=0)\n\n    return average.tolist()\n</code></pre>"},{"location":"api/base/#bruno_llm.base.BaseEmbeddingProvider.weighted_average_embeddings","title":"<code>weighted_average_embeddings(embeddings, weights)</code>","text":"<p>Calculate weighted average embedding using numpy.</p> <p>Parameters:</p> Name Type Description Default <code>embeddings</code> <code>list[list[float]]</code> <p>List of embedding vectors</p> required <code>weights</code> <code>list[float]</code> <p>List of weights (must sum to 1.0)</p> required <p>Returns:</p> Type Description <code>list[float]</code> <p>Weighted average embedding</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If inputs are invalid</p> Source code in <code>bruno_llm/base/embedding_interface.py</code> <pre><code>def weighted_average_embeddings(\n    self,\n    embeddings: list[list[float]],\n    weights: list[float],\n) -&gt; list[float]:\n    \"\"\"\n    Calculate weighted average embedding using numpy.\n\n    Args:\n        embeddings: List of embedding vectors\n        weights: List of weights (must sum to 1.0)\n\n    Returns:\n        Weighted average embedding\n\n    Raises:\n        ValueError: If inputs are invalid\n    \"\"\"\n    if len(embeddings) != len(weights):\n        raise ValueError(\n            f\"Number of embeddings ({len(embeddings)}) must match \"\n            f\"number of weights ({len(weights)})\"\n        )\n\n    if not embeddings:\n        raise ValueError(\"Cannot average empty list of embeddings\")\n\n    # Validate weights sum using numpy\n    weights_array = np.array(weights, dtype=np.float32)\n    if not np.isclose(weights_array.sum(), 1.0, atol=1e-6):\n        raise ValueError(f\"Weights must sum to 1.0, got {weights_array.sum()}\")\n\n    # Calculate weighted average using numpy\n    embeddings_matrix = np.array(embeddings, dtype=np.float32)\n    weighted_avg = np.average(embeddings_matrix, axis=0, weights=weights_array)\n\n    return weighted_avg.tolist()\n</code></pre>"},{"location":"api/base/#bruno_llm.base.CachingMiddleware","title":"<code>CachingMiddleware</code>","text":"<p>               Bases: <code>Middleware</code></p> <p>Cache responses using ResponseCache.</p> <p>Parameters:</p> Name Type Description Default <code>cache</code> <p>ResponseCache instance</p> required <code>cache_streaming</code> <code>bool</code> <p>Whether to cache streaming responses</p> <code>True</code> Example <p>from bruno_llm.base.cache import ResponseCache cache = ResponseCache(max_size=100, ttl=300) middleware = CachingMiddleware(cache)</p> Source code in <code>bruno_llm/base/middleware.py</code> <pre><code>class CachingMiddleware(Middleware):\n    \"\"\"\n    Cache responses using ResponseCache.\n\n    Args:\n        cache: ResponseCache instance\n        cache_streaming: Whether to cache streaming responses\n\n    Example:\n        &gt;&gt;&gt; from bruno_llm.base.cache import ResponseCache\n        &gt;&gt;&gt; cache = ResponseCache(max_size=100, ttl=300)\n        &gt;&gt;&gt; middleware = CachingMiddleware(cache)\n    \"\"\"\n\n    def __init__(self, cache, cache_streaming: bool = True):\n        \"\"\"\n        Initialize caching middleware.\n\n        Args:\n            cache: ResponseCache instance\n            cache_streaming: Whether to cache streaming responses\n        \"\"\"\n        self.cache = cache\n        self.cache_streaming = cache_streaming\n        self._current_stream_chunks: Optional[list[str]] = None\n\n    async def before_request(\n        self, messages: list[Message], **kwargs: Any\n    ) -&gt; tuple[list[Message], dict[str, Any]]:\n        \"\"\"Check cache before request.\"\"\"\n        # Cache lookup is handled externally\n        return messages, kwargs\n\n    async def after_response(self, messages: list[Message], response: str, **kwargs: Any) -&gt; str:\n        \"\"\"Cache response after receiving.\"\"\"\n        self.cache.set(messages, response, **kwargs)\n        return response\n\n    async def on_stream_chunk(self, chunk: str, **kwargs: Any) -&gt; str:\n        \"\"\"Collect stream chunks for caching.\"\"\"\n        if self.cache_streaming:\n            if self._current_stream_chunks is None:\n                self._current_stream_chunks = []\n            self._current_stream_chunks.append(chunk)\n\n        return chunk\n</code></pre>"},{"location":"api/base/#bruno_llm.base.CachingMiddleware.__init__","title":"<code>__init__(cache, cache_streaming=True)</code>","text":"<p>Initialize caching middleware.</p> <p>Parameters:</p> Name Type Description Default <code>cache</code> <p>ResponseCache instance</p> required <code>cache_streaming</code> <code>bool</code> <p>Whether to cache streaming responses</p> <code>True</code> Source code in <code>bruno_llm/base/middleware.py</code> <pre><code>def __init__(self, cache, cache_streaming: bool = True):\n    \"\"\"\n    Initialize caching middleware.\n\n    Args:\n        cache: ResponseCache instance\n        cache_streaming: Whether to cache streaming responses\n    \"\"\"\n    self.cache = cache\n    self.cache_streaming = cache_streaming\n    self._current_stream_chunks: Optional[list[str]] = None\n</code></pre>"},{"location":"api/base/#bruno_llm.base.CachingMiddleware.before_request","title":"<code>before_request(messages, **kwargs)</code>  <code>async</code>","text":"<p>Check cache before request.</p> Source code in <code>bruno_llm/base/middleware.py</code> <pre><code>async def before_request(\n    self, messages: list[Message], **kwargs: Any\n) -&gt; tuple[list[Message], dict[str, Any]]:\n    \"\"\"Check cache before request.\"\"\"\n    # Cache lookup is handled externally\n    return messages, kwargs\n</code></pre>"},{"location":"api/base/#bruno_llm.base.CachingMiddleware.after_response","title":"<code>after_response(messages, response, **kwargs)</code>  <code>async</code>","text":"<p>Cache response after receiving.</p> Source code in <code>bruno_llm/base/middleware.py</code> <pre><code>async def after_response(self, messages: list[Message], response: str, **kwargs: Any) -&gt; str:\n    \"\"\"Cache response after receiving.\"\"\"\n    self.cache.set(messages, response, **kwargs)\n    return response\n</code></pre>"},{"location":"api/base/#bruno_llm.base.CachingMiddleware.on_stream_chunk","title":"<code>on_stream_chunk(chunk, **kwargs)</code>  <code>async</code>","text":"<p>Collect stream chunks for caching.</p> Source code in <code>bruno_llm/base/middleware.py</code> <pre><code>async def on_stream_chunk(self, chunk: str, **kwargs: Any) -&gt; str:\n    \"\"\"Collect stream chunks for caching.\"\"\"\n    if self.cache_streaming:\n        if self._current_stream_chunks is None:\n            self._current_stream_chunks = []\n        self._current_stream_chunks.append(chunk)\n\n    return chunk\n</code></pre>"},{"location":"api/base/#bruno_llm.base.LoggingMiddleware","title":"<code>LoggingMiddleware</code>","text":"<p>               Bases: <code>Middleware</code></p> <p>Log all requests and responses.</p> <p>Parameters:</p> Name Type Description Default <code>logger</code> <p>Logger instance (defaults to structlog)</p> <code>None</code> <code>log_messages</code> <code>bool</code> <p>Whether to log full message content</p> <code>False</code> Example <p>middleware = LoggingMiddleware(log_messages=False) provider = MiddlewareProvider(base_provider, [middleware])</p> Source code in <code>bruno_llm/base/middleware.py</code> <pre><code>class LoggingMiddleware(Middleware):\n    \"\"\"\n    Log all requests and responses.\n\n    Args:\n        logger: Logger instance (defaults to structlog)\n        log_messages: Whether to log full message content\n\n    Example:\n        &gt;&gt;&gt; middleware = LoggingMiddleware(log_messages=False)\n        &gt;&gt;&gt; provider = MiddlewareProvider(base_provider, [middleware])\n    \"\"\"\n\n    def __init__(self, logger=None, log_messages: bool = False):\n        \"\"\"\n        Initialize logging middleware.\n\n        Args:\n            logger: Logger instance\n            log_messages: Whether to log message content\n        \"\"\"\n        self.log_messages = log_messages\n\n        if logger is None:\n            import structlog\n\n            self.logger = structlog.get_logger(__name__)\n        else:\n            self.logger = logger\n\n    async def before_request(\n        self, messages: list[Message], **kwargs: Any\n    ) -&gt; tuple[list[Message], dict[str, Any]]:\n        \"\"\"Log before request.\"\"\"\n        log_data = {\n            \"event\": \"llm_request\",\n            \"message_count\": len(messages),\n            \"params\": {k: v for k, v in kwargs.items() if k not in [\"api_key\"]},\n        }\n\n        if self.log_messages:\n            log_data[\"messages\"] = [{\"role\": m.role.value, \"content\": m.content} for m in messages]\n\n        self.logger.info(\"LLM request\", **log_data)\n        return messages, kwargs\n\n    async def after_response(self, messages: list[Message], response: str, **kwargs: Any) -&gt; str:\n        \"\"\"Log after response.\"\"\"\n        log_data = {\n            \"event\": \"llm_response\",\n            \"response_length\": len(response),\n        }\n\n        if self.log_messages:\n            log_data[\"response\"] = response\n\n        self.logger.info(\"LLM response\", **log_data)\n        return response\n\n    async def on_error(self, error: Exception, messages: list[Message], **kwargs: Any) -&gt; None:\n        \"\"\"Log errors.\"\"\"\n        self.logger.error(\n            \"LLM error\",\n            event=\"llm_error\",\n            error=str(error),\n            error_type=type(error).__name__,\n            message_count=len(messages),\n        )\n</code></pre>"},{"location":"api/base/#bruno_llm.base.LoggingMiddleware.__init__","title":"<code>__init__(logger=None, log_messages=False)</code>","text":"<p>Initialize logging middleware.</p> <p>Parameters:</p> Name Type Description Default <code>logger</code> <p>Logger instance</p> <code>None</code> <code>log_messages</code> <code>bool</code> <p>Whether to log message content</p> <code>False</code> Source code in <code>bruno_llm/base/middleware.py</code> <pre><code>def __init__(self, logger=None, log_messages: bool = False):\n    \"\"\"\n    Initialize logging middleware.\n\n    Args:\n        logger: Logger instance\n        log_messages: Whether to log message content\n    \"\"\"\n    self.log_messages = log_messages\n\n    if logger is None:\n        import structlog\n\n        self.logger = structlog.get_logger(__name__)\n    else:\n        self.logger = logger\n</code></pre>"},{"location":"api/base/#bruno_llm.base.LoggingMiddleware.before_request","title":"<code>before_request(messages, **kwargs)</code>  <code>async</code>","text":"<p>Log before request.</p> Source code in <code>bruno_llm/base/middleware.py</code> <pre><code>async def before_request(\n    self, messages: list[Message], **kwargs: Any\n) -&gt; tuple[list[Message], dict[str, Any]]:\n    \"\"\"Log before request.\"\"\"\n    log_data = {\n        \"event\": \"llm_request\",\n        \"message_count\": len(messages),\n        \"params\": {k: v for k, v in kwargs.items() if k not in [\"api_key\"]},\n    }\n\n    if self.log_messages:\n        log_data[\"messages\"] = [{\"role\": m.role.value, \"content\": m.content} for m in messages]\n\n    self.logger.info(\"LLM request\", **log_data)\n    return messages, kwargs\n</code></pre>"},{"location":"api/base/#bruno_llm.base.LoggingMiddleware.after_response","title":"<code>after_response(messages, response, **kwargs)</code>  <code>async</code>","text":"<p>Log after response.</p> Source code in <code>bruno_llm/base/middleware.py</code> <pre><code>async def after_response(self, messages: list[Message], response: str, **kwargs: Any) -&gt; str:\n    \"\"\"Log after response.\"\"\"\n    log_data = {\n        \"event\": \"llm_response\",\n        \"response_length\": len(response),\n    }\n\n    if self.log_messages:\n        log_data[\"response\"] = response\n\n    self.logger.info(\"LLM response\", **log_data)\n    return response\n</code></pre>"},{"location":"api/base/#bruno_llm.base.LoggingMiddleware.on_error","title":"<code>on_error(error, messages, **kwargs)</code>  <code>async</code>","text":"<p>Log errors.</p> Source code in <code>bruno_llm/base/middleware.py</code> <pre><code>async def on_error(self, error: Exception, messages: list[Message], **kwargs: Any) -&gt; None:\n    \"\"\"Log errors.\"\"\"\n    self.logger.error(\n        \"LLM error\",\n        event=\"llm_error\",\n        error=str(error),\n        error_type=type(error).__name__,\n        message_count=len(messages),\n    )\n</code></pre>"},{"location":"api/base/#bruno_llm.base.Middleware","title":"<code>Middleware</code>","text":"<p>               Bases: <code>ABC</code></p> <p>Base class for provider middleware.</p> <p>Middleware can intercept and modify: - Request messages before sending to provider - Response text after receiving from provider - Streaming chunks as they arrive - Request parameters (temperature, max_tokens, etc.)</p> Example <p>class LoggingMiddleware(Middleware): ...     async def before_request(self, messages, kwargs): ...         print(f\"Sending {len(messages)} messages\") ...         return messages, kwargs ... ...     async def after_response(self, messages, response, kwargs): ...         print(f\"Received {len(response)} chars\") ...         return response</p> Source code in <code>bruno_llm/base/middleware.py</code> <pre><code>class Middleware(ABC):\n    \"\"\"\n    Base class for provider middleware.\n\n    Middleware can intercept and modify:\n    - Request messages before sending to provider\n    - Response text after receiving from provider\n    - Streaming chunks as they arrive\n    - Request parameters (temperature, max_tokens, etc.)\n\n    Example:\n        &gt;&gt;&gt; class LoggingMiddleware(Middleware):\n        ...     async def before_request(self, messages, **kwargs):\n        ...         print(f\"Sending {len(messages)} messages\")\n        ...         return messages, kwargs\n        ...\n        ...     async def after_response(self, messages, response, **kwargs):\n        ...         print(f\"Received {len(response)} chars\")\n        ...         return response\n    \"\"\"\n\n    @abstractmethod\n    async def before_request(\n        self, messages: list[Message], **kwargs: Any\n    ) -&gt; tuple[list[Message], dict[str, Any]]:\n        \"\"\"\n        Process messages and parameters before request.\n\n        Args:\n            messages: Input messages\n            **kwargs: Request parameters\n\n        Returns:\n            Tuple of (modified_messages, modified_kwargs)\n        \"\"\"\n        pass\n\n    @abstractmethod\n    async def after_response(self, messages: list[Message], response: str, **kwargs: Any) -&gt; str:\n        \"\"\"\n        Process response after receiving.\n\n        Args:\n            messages: Original input messages\n            response: Provider response\n            **kwargs: Request parameters used\n\n        Returns:\n            Modified response\n        \"\"\"\n        pass\n\n    async def on_stream_chunk(self, chunk: str, **kwargs: Any) -&gt; str:\n        \"\"\"\n        Process individual stream chunks.\n\n        Args:\n            chunk: Stream chunk\n            **kwargs: Request parameters\n\n        Returns:\n            Modified chunk\n        \"\"\"\n        return chunk\n\n    async def on_error(self, error: Exception, messages: list[Message], **kwargs: Any) -&gt; None:  # noqa: B027\n        \"\"\"\n        Handle errors during request.\n\n        Args:\n            error: The exception that occurred\n            messages: Messages that were being processed\n            **kwargs: Request parameters\n        \"\"\"\n        pass\n</code></pre>"},{"location":"api/base/#bruno_llm.base.Middleware.before_request","title":"<code>before_request(messages, **kwargs)</code>  <code>abstractmethod</code> <code>async</code>","text":"<p>Process messages and parameters before request.</p> <p>Parameters:</p> Name Type Description Default <code>messages</code> <code>list[Message]</code> <p>Input messages</p> required <code>**kwargs</code> <code>Any</code> <p>Request parameters</p> <code>{}</code> <p>Returns:</p> Type Description <code>tuple[list[Message], dict[str, Any]]</code> <p>Tuple of (modified_messages, modified_kwargs)</p> Source code in <code>bruno_llm/base/middleware.py</code> <pre><code>@abstractmethod\nasync def before_request(\n    self, messages: list[Message], **kwargs: Any\n) -&gt; tuple[list[Message], dict[str, Any]]:\n    \"\"\"\n    Process messages and parameters before request.\n\n    Args:\n        messages: Input messages\n        **kwargs: Request parameters\n\n    Returns:\n        Tuple of (modified_messages, modified_kwargs)\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/base/#bruno_llm.base.Middleware.after_response","title":"<code>after_response(messages, response, **kwargs)</code>  <code>abstractmethod</code> <code>async</code>","text":"<p>Process response after receiving.</p> <p>Parameters:</p> Name Type Description Default <code>messages</code> <code>list[Message]</code> <p>Original input messages</p> required <code>response</code> <code>str</code> <p>Provider response</p> required <code>**kwargs</code> <code>Any</code> <p>Request parameters used</p> <code>{}</code> <p>Returns:</p> Type Description <code>str</code> <p>Modified response</p> Source code in <code>bruno_llm/base/middleware.py</code> <pre><code>@abstractmethod\nasync def after_response(self, messages: list[Message], response: str, **kwargs: Any) -&gt; str:\n    \"\"\"\n    Process response after receiving.\n\n    Args:\n        messages: Original input messages\n        response: Provider response\n        **kwargs: Request parameters used\n\n    Returns:\n        Modified response\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/base/#bruno_llm.base.Middleware.on_stream_chunk","title":"<code>on_stream_chunk(chunk, **kwargs)</code>  <code>async</code>","text":"<p>Process individual stream chunks.</p> <p>Parameters:</p> Name Type Description Default <code>chunk</code> <code>str</code> <p>Stream chunk</p> required <code>**kwargs</code> <code>Any</code> <p>Request parameters</p> <code>{}</code> <p>Returns:</p> Type Description <code>str</code> <p>Modified chunk</p> Source code in <code>bruno_llm/base/middleware.py</code> <pre><code>async def on_stream_chunk(self, chunk: str, **kwargs: Any) -&gt; str:\n    \"\"\"\n    Process individual stream chunks.\n\n    Args:\n        chunk: Stream chunk\n        **kwargs: Request parameters\n\n    Returns:\n        Modified chunk\n    \"\"\"\n    return chunk\n</code></pre>"},{"location":"api/base/#bruno_llm.base.Middleware.on_error","title":"<code>on_error(error, messages, **kwargs)</code>  <code>async</code>","text":"<p>Handle errors during request.</p> <p>Parameters:</p> Name Type Description Default <code>error</code> <code>Exception</code> <p>The exception that occurred</p> required <code>messages</code> <code>list[Message]</code> <p>Messages that were being processed</p> required <code>**kwargs</code> <code>Any</code> <p>Request parameters</p> <code>{}</code> Source code in <code>bruno_llm/base/middleware.py</code> <pre><code>async def on_error(self, error: Exception, messages: list[Message], **kwargs: Any) -&gt; None:  # noqa: B027\n    \"\"\"\n    Handle errors during request.\n\n    Args:\n        error: The exception that occurred\n        messages: Messages that were being processed\n        **kwargs: Request parameters\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/base/#bruno_llm.base.MiddlewareChain","title":"<code>MiddlewareChain</code>","text":"<p>Chain multiple middleware together.</p> <p>Executes middleware in order for before_request, and in reverse order for after_response.</p> <p>Parameters:</p> Name Type Description Default <code>middlewares</code> <code>list[Middleware]</code> <p>List of middleware instances</p> required Example <p>chain = MiddlewareChain([ ...     LoggingMiddleware(), ...     ValidationMiddleware(), ...     CachingMiddleware(cache), ... ]) messages, kwargs = await chain.before_request(messages, **kwargs)</p> Source code in <code>bruno_llm/base/middleware.py</code> <pre><code>class MiddlewareChain:\n    \"\"\"\n    Chain multiple middleware together.\n\n    Executes middleware in order for before_request,\n    and in reverse order for after_response.\n\n    Args:\n        middlewares: List of middleware instances\n\n    Example:\n        &gt;&gt;&gt; chain = MiddlewareChain([\n        ...     LoggingMiddleware(),\n        ...     ValidationMiddleware(),\n        ...     CachingMiddleware(cache),\n        ... ])\n        &gt;&gt;&gt; messages, kwargs = await chain.before_request(messages, **kwargs)\n    \"\"\"\n\n    def __init__(self, middlewares: list[Middleware]):\n        \"\"\"\n        Initialize middleware chain.\n\n        Args:\n            middlewares: List of middleware to chain\n        \"\"\"\n        self.middlewares = middlewares\n\n    async def before_request(\n        self, messages: list[Message], **kwargs: Any\n    ) -&gt; tuple[list[Message], dict[str, Any]]:\n        \"\"\"Execute all middleware before_request in order.\"\"\"\n        for middleware in self.middlewares:\n            messages, kwargs = await middleware.before_request(messages, **kwargs)\n        return messages, kwargs\n\n    async def after_response(self, messages: list[Message], response: str, **kwargs: Any) -&gt; str:\n        \"\"\"Execute all middleware after_response in reverse order.\"\"\"\n        for middleware in reversed(self.middlewares):\n            response = await middleware.after_response(messages, response, **kwargs)\n        return response\n\n    async def on_stream_chunk(self, chunk: str, **kwargs: Any) -&gt; str:\n        \"\"\"Execute all middleware on_stream_chunk in order.\"\"\"\n        for middleware in self.middlewares:\n            chunk = await middleware.on_stream_chunk(chunk, **kwargs)\n        return chunk\n\n    async def on_error(self, error: Exception, messages: list[Message], **kwargs: Any) -&gt; None:\n        \"\"\"Execute all middleware on_error.\"\"\"\n        for middleware in self.middlewares:\n            await middleware.on_error(error, messages, **kwargs)\n</code></pre>"},{"location":"api/base/#bruno_llm.base.MiddlewareChain.__init__","title":"<code>__init__(middlewares)</code>","text":"<p>Initialize middleware chain.</p> <p>Parameters:</p> Name Type Description Default <code>middlewares</code> <code>list[Middleware]</code> <p>List of middleware to chain</p> required Source code in <code>bruno_llm/base/middleware.py</code> <pre><code>def __init__(self, middlewares: list[Middleware]):\n    \"\"\"\n    Initialize middleware chain.\n\n    Args:\n        middlewares: List of middleware to chain\n    \"\"\"\n    self.middlewares = middlewares\n</code></pre>"},{"location":"api/base/#bruno_llm.base.MiddlewareChain.before_request","title":"<code>before_request(messages, **kwargs)</code>  <code>async</code>","text":"<p>Execute all middleware before_request in order.</p> Source code in <code>bruno_llm/base/middleware.py</code> <pre><code>async def before_request(\n    self, messages: list[Message], **kwargs: Any\n) -&gt; tuple[list[Message], dict[str, Any]]:\n    \"\"\"Execute all middleware before_request in order.\"\"\"\n    for middleware in self.middlewares:\n        messages, kwargs = await middleware.before_request(messages, **kwargs)\n    return messages, kwargs\n</code></pre>"},{"location":"api/base/#bruno_llm.base.MiddlewareChain.after_response","title":"<code>after_response(messages, response, **kwargs)</code>  <code>async</code>","text":"<p>Execute all middleware after_response in reverse order.</p> Source code in <code>bruno_llm/base/middleware.py</code> <pre><code>async def after_response(self, messages: list[Message], response: str, **kwargs: Any) -&gt; str:\n    \"\"\"Execute all middleware after_response in reverse order.\"\"\"\n    for middleware in reversed(self.middlewares):\n        response = await middleware.after_response(messages, response, **kwargs)\n    return response\n</code></pre>"},{"location":"api/base/#bruno_llm.base.MiddlewareChain.on_stream_chunk","title":"<code>on_stream_chunk(chunk, **kwargs)</code>  <code>async</code>","text":"<p>Execute all middleware on_stream_chunk in order.</p> Source code in <code>bruno_llm/base/middleware.py</code> <pre><code>async def on_stream_chunk(self, chunk: str, **kwargs: Any) -&gt; str:\n    \"\"\"Execute all middleware on_stream_chunk in order.\"\"\"\n    for middleware in self.middlewares:\n        chunk = await middleware.on_stream_chunk(chunk, **kwargs)\n    return chunk\n</code></pre>"},{"location":"api/base/#bruno_llm.base.MiddlewareChain.on_error","title":"<code>on_error(error, messages, **kwargs)</code>  <code>async</code>","text":"<p>Execute all middleware on_error.</p> Source code in <code>bruno_llm/base/middleware.py</code> <pre><code>async def on_error(self, error: Exception, messages: list[Message], **kwargs: Any) -&gt; None:\n    \"\"\"Execute all middleware on_error.\"\"\"\n    for middleware in self.middlewares:\n        await middleware.on_error(error, messages, **kwargs)\n</code></pre>"},{"location":"api/base/#bruno_llm.base.RetryMiddleware","title":"<code>RetryMiddleware</code>","text":"<p>               Bases: <code>Middleware</code></p> <p>Add retry logic with exponential backoff.</p> <p>Note: This is typically handled by BaseProvider's retry logic, but can be used for additional retry layers.</p> <p>Parameters:</p> Name Type Description Default <code>max_retries</code> <code>int</code> <p>Maximum number of retries</p> <code>3</code> <code>base_delay</code> <code>float</code> <p>Base delay in seconds</p> <code>1.0</code> Example <p>middleware = RetryMiddleware(max_retries=3, base_delay=1.0)</p> Source code in <code>bruno_llm/base/middleware.py</code> <pre><code>class RetryMiddleware(Middleware):\n    \"\"\"\n    Add retry logic with exponential backoff.\n\n    Note: This is typically handled by BaseProvider's retry logic,\n    but can be used for additional retry layers.\n\n    Args:\n        max_retries: Maximum number of retries\n        base_delay: Base delay in seconds\n\n    Example:\n        &gt;&gt;&gt; middleware = RetryMiddleware(max_retries=3, base_delay=1.0)\n    \"\"\"\n\n    def __init__(self, max_retries: int = 3, base_delay: float = 1.0):\n        \"\"\"\n        Initialize retry middleware.\n\n        Args:\n            max_retries: Maximum retry attempts\n            base_delay: Base delay for exponential backoff\n        \"\"\"\n        self.max_retries = max_retries\n        self.base_delay = base_delay\n        self.retry_count = 0\n\n    async def before_request(\n        self, messages: list[Message], **kwargs: Any\n    ) -&gt; tuple[list[Message], dict[str, Any]]:\n        \"\"\"Reset retry count before request.\"\"\"\n        self.retry_count = 0\n        return messages, kwargs\n\n    async def after_response(self, messages: list[Message], response: str, **kwargs: Any) -&gt; str:\n        \"\"\"No processing after successful response.\"\"\"\n        return response\n\n    async def on_error(self, error: Exception, messages: list[Message], **kwargs: Any) -&gt; None:\n        \"\"\"Handle retry logic on error.\"\"\"\n        import asyncio\n\n        self.retry_count += 1\n\n        if self.retry_count &lt;= self.max_retries:\n            delay = self.base_delay * (2 ** (self.retry_count - 1))\n            await asyncio.sleep(delay)\n</code></pre>"},{"location":"api/base/#bruno_llm.base.RetryMiddleware.__init__","title":"<code>__init__(max_retries=3, base_delay=1.0)</code>","text":"<p>Initialize retry middleware.</p> <p>Parameters:</p> Name Type Description Default <code>max_retries</code> <code>int</code> <p>Maximum retry attempts</p> <code>3</code> <code>base_delay</code> <code>float</code> <p>Base delay for exponential backoff</p> <code>1.0</code> Source code in <code>bruno_llm/base/middleware.py</code> <pre><code>def __init__(self, max_retries: int = 3, base_delay: float = 1.0):\n    \"\"\"\n    Initialize retry middleware.\n\n    Args:\n        max_retries: Maximum retry attempts\n        base_delay: Base delay for exponential backoff\n    \"\"\"\n    self.max_retries = max_retries\n    self.base_delay = base_delay\n    self.retry_count = 0\n</code></pre>"},{"location":"api/base/#bruno_llm.base.RetryMiddleware.before_request","title":"<code>before_request(messages, **kwargs)</code>  <code>async</code>","text":"<p>Reset retry count before request.</p> Source code in <code>bruno_llm/base/middleware.py</code> <pre><code>async def before_request(\n    self, messages: list[Message], **kwargs: Any\n) -&gt; tuple[list[Message], dict[str, Any]]:\n    \"\"\"Reset retry count before request.\"\"\"\n    self.retry_count = 0\n    return messages, kwargs\n</code></pre>"},{"location":"api/base/#bruno_llm.base.RetryMiddleware.after_response","title":"<code>after_response(messages, response, **kwargs)</code>  <code>async</code>","text":"<p>No processing after successful response.</p> Source code in <code>bruno_llm/base/middleware.py</code> <pre><code>async def after_response(self, messages: list[Message], response: str, **kwargs: Any) -&gt; str:\n    \"\"\"No processing after successful response.\"\"\"\n    return response\n</code></pre>"},{"location":"api/base/#bruno_llm.base.RetryMiddleware.on_error","title":"<code>on_error(error, messages, **kwargs)</code>  <code>async</code>","text":"<p>Handle retry logic on error.</p> Source code in <code>bruno_llm/base/middleware.py</code> <pre><code>async def on_error(self, error: Exception, messages: list[Message], **kwargs: Any) -&gt; None:\n    \"\"\"Handle retry logic on error.\"\"\"\n    import asyncio\n\n    self.retry_count += 1\n\n    if self.retry_count &lt;= self.max_retries:\n        delay = self.base_delay * (2 ** (self.retry_count - 1))\n        await asyncio.sleep(delay)\n</code></pre>"},{"location":"api/base/#bruno_llm.base.ValidationMiddleware","title":"<code>ValidationMiddleware</code>","text":"<p>               Bases: <code>Middleware</code></p> <p>Validate messages and parameters.</p> <p>Parameters:</p> Name Type Description Default <code>max_message_length</code> <code>Optional[int]</code> <p>Max length for individual messages</p> <code>None</code> <code>allowed_roles</code> <code>Optional[list[str]]</code> <p>Allowed message roles</p> <code>None</code> <code>required_params</code> <code>Optional[list[str]]</code> <p>Required parameter names</p> <code>None</code> Example <p>middleware = ValidationMiddleware( ...     max_message_length=10000, ...     allowed_roles=[\"user\", \"assistant\", \"system\"] ... )</p> Source code in <code>bruno_llm/base/middleware.py</code> <pre><code>class ValidationMiddleware(Middleware):\n    \"\"\"\n    Validate messages and parameters.\n\n    Args:\n        max_message_length: Max length for individual messages\n        allowed_roles: Allowed message roles\n        required_params: Required parameter names\n\n    Example:\n        &gt;&gt;&gt; middleware = ValidationMiddleware(\n        ...     max_message_length=10000,\n        ...     allowed_roles=[\"user\", \"assistant\", \"system\"]\n        ... )\n    \"\"\"\n\n    def __init__(\n        self,\n        max_message_length: Optional[int] = None,\n        allowed_roles: Optional[list[str]] = None,\n        required_params: Optional[list[str]] = None,\n    ):\n        \"\"\"\n        Initialize validation middleware.\n\n        Args:\n            max_message_length: Maximum message length\n            allowed_roles: Allowed message roles\n            required_params: Required parameters\n        \"\"\"\n        self.max_message_length = max_message_length\n        self.allowed_roles = allowed_roles\n        self.required_params = required_params or []\n\n    async def before_request(\n        self, messages: list[Message], **kwargs: Any\n    ) -&gt; tuple[list[Message], dict[str, Any]]:\n        \"\"\"Validate before request.\"\"\"\n        # Validate message lengths\n        if self.max_message_length:\n            for msg in messages:\n                if len(msg.content) &gt; self.max_message_length:\n                    raise ValueError(\n                        f\"Message content exceeds max length \"\n                        f\"({len(msg.content)} &gt; {self.max_message_length})\"\n                    )\n\n        # Validate roles\n        if self.allowed_roles:\n            for msg in messages:\n                if msg.role.value not in self.allowed_roles:\n                    raise ValueError(\n                        f\"Invalid message role: {msg.role.value}. Allowed: {self.allowed_roles}\"\n                    )\n\n        # Validate required parameters\n        for param in self.required_params:\n            if param not in kwargs:\n                raise ValueError(f\"Required parameter missing: {param}\")\n\n        return messages, kwargs\n\n    async def after_response(self, messages: list[Message], response: str, **kwargs: Any) -&gt; str:\n        \"\"\"No validation after response.\"\"\"\n        return response\n</code></pre>"},{"location":"api/base/#bruno_llm.base.ValidationMiddleware.__init__","title":"<code>__init__(max_message_length=None, allowed_roles=None, required_params=None)</code>","text":"<p>Initialize validation middleware.</p> <p>Parameters:</p> Name Type Description Default <code>max_message_length</code> <code>Optional[int]</code> <p>Maximum message length</p> <code>None</code> <code>allowed_roles</code> <code>Optional[list[str]]</code> <p>Allowed message roles</p> <code>None</code> <code>required_params</code> <code>Optional[list[str]]</code> <p>Required parameters</p> <code>None</code> Source code in <code>bruno_llm/base/middleware.py</code> <pre><code>def __init__(\n    self,\n    max_message_length: Optional[int] = None,\n    allowed_roles: Optional[list[str]] = None,\n    required_params: Optional[list[str]] = None,\n):\n    \"\"\"\n    Initialize validation middleware.\n\n    Args:\n        max_message_length: Maximum message length\n        allowed_roles: Allowed message roles\n        required_params: Required parameters\n    \"\"\"\n    self.max_message_length = max_message_length\n    self.allowed_roles = allowed_roles\n    self.required_params = required_params or []\n</code></pre>"},{"location":"api/base/#bruno_llm.base.ValidationMiddleware.before_request","title":"<code>before_request(messages, **kwargs)</code>  <code>async</code>","text":"<p>Validate before request.</p> Source code in <code>bruno_llm/base/middleware.py</code> <pre><code>async def before_request(\n    self, messages: list[Message], **kwargs: Any\n) -&gt; tuple[list[Message], dict[str, Any]]:\n    \"\"\"Validate before request.\"\"\"\n    # Validate message lengths\n    if self.max_message_length:\n        for msg in messages:\n            if len(msg.content) &gt; self.max_message_length:\n                raise ValueError(\n                    f\"Message content exceeds max length \"\n                    f\"({len(msg.content)} &gt; {self.max_message_length})\"\n                )\n\n    # Validate roles\n    if self.allowed_roles:\n        for msg in messages:\n            if msg.role.value not in self.allowed_roles:\n                raise ValueError(\n                    f\"Invalid message role: {msg.role.value}. Allowed: {self.allowed_roles}\"\n                )\n\n    # Validate required parameters\n    for param in self.required_params:\n        if param not in kwargs:\n            raise ValueError(f\"Required parameter missing: {param}\")\n\n    return messages, kwargs\n</code></pre>"},{"location":"api/base/#bruno_llm.base.ValidationMiddleware.after_response","title":"<code>after_response(messages, response, **kwargs)</code>  <code>async</code>","text":"<p>No validation after response.</p> Source code in <code>bruno_llm/base/middleware.py</code> <pre><code>async def after_response(self, messages: list[Message], response: str, **kwargs: Any) -&gt; str:\n    \"\"\"No validation after response.\"\"\"\n    return response\n</code></pre>"},{"location":"api/base/#bruno_llm.base.RateLimiter","title":"<code>RateLimiter</code>","text":"<p>Async rate limiter using token bucket algorithm.</p> <p>Controls the rate of API calls to prevent exceeding provider limits. Thread-safe and supports multiple concurrent requests.</p> Example <p>limiter = RateLimiter(requests_per_minute=60) async with limiter: ...     # Make API call ...     response = await api_call()</p> Source code in <code>bruno_llm/base/rate_limiter.py</code> <pre><code>class RateLimiter:\n    \"\"\"\n    Async rate limiter using token bucket algorithm.\n\n    Controls the rate of API calls to prevent exceeding provider limits.\n    Thread-safe and supports multiple concurrent requests.\n\n    Example:\n        &gt;&gt;&gt; limiter = RateLimiter(requests_per_minute=60)\n        &gt;&gt;&gt; async with limiter:\n        ...     # Make API call\n        ...     response = await api_call()\n    \"\"\"\n\n    def __init__(\n        self,\n        requests_per_minute: int = 60,\n        tokens_per_minute: Optional[int] = None,\n    ):\n        \"\"\"\n        Initialize rate limiter.\n\n        Args:\n            requests_per_minute: Maximum requests allowed per minute\n            tokens_per_minute: Maximum tokens allowed per minute (optional)\n        \"\"\"\n        self.requests_per_minute = requests_per_minute\n        self.tokens_per_minute = tokens_per_minute\n\n        # Calculate minimum interval between requests\n        self.min_interval = 60.0 / requests_per_minute if requests_per_minute &gt; 0 else 0\n\n        # Token bucket for requests\n        self._request_tokens = float(requests_per_minute)\n        self._max_request_tokens = float(requests_per_minute)\n        self._last_update = time.time()\n\n        # Token bucket for API tokens (if specified)\n        self._api_tokens = float(tokens_per_minute) if tokens_per_minute else None\n        self._max_api_tokens = float(tokens_per_minute) if tokens_per_minute else None\n\n        # Lock for thread safety\n        self._lock = asyncio.Lock()\n\n    async def _refill_tokens(self) -&gt; None:\n        \"\"\"Refill token buckets based on elapsed time.\"\"\"\n        now = time.time()\n        elapsed = now - self._last_update\n\n        if elapsed &lt;= 0:\n            return\n\n        # Refill request tokens\n        tokens_to_add = (elapsed * self.requests_per_minute) / 60.0\n        self._request_tokens = min(self._max_request_tokens, self._request_tokens + tokens_to_add)\n\n        # Refill API tokens if applicable\n        if self._api_tokens is not None and self.tokens_per_minute:\n            api_tokens_to_add = (elapsed * self.tokens_per_minute) / 60.0\n            self._api_tokens = min(self._max_api_tokens or 0, self._api_tokens + api_tokens_to_add)\n\n        self._last_update = now\n\n    async def acquire(self, api_tokens: int = 0) -&gt; None:\n        \"\"\"\n        Acquire permission to make a request.\n\n        Blocks until rate limit allows the request.\n\n        Args:\n            api_tokens: Number of API tokens the request will consume\n        \"\"\"\n        async with self._lock:\n            while True:\n                await self._refill_tokens()\n\n                # Check if we have enough request tokens\n                if self._request_tokens &lt; 1:\n                    # Calculate wait time\n                    wait_time = (1 - self._request_tokens) * (60.0 / self.requests_per_minute)\n                    await asyncio.sleep(wait_time)\n                    continue\n\n                # Check if we have enough API tokens (if applicable)\n                if self._api_tokens is not None and api_tokens &gt; 0:\n                    if self._api_tokens &lt; api_tokens:\n                        wait_time = (api_tokens - self._api_tokens) * (\n                            60.0 / (self.tokens_per_minute or 1)\n                        )\n                        await asyncio.sleep(wait_time)\n                        continue\n\n                # Consume tokens\n                self._request_tokens -= 1\n                if self._api_tokens is not None and api_tokens &gt; 0:\n                    self._api_tokens -= api_tokens\n\n                break\n\n    async def __aenter__(self) -&gt; \"RateLimiter\":\n        \"\"\"Context manager entry.\"\"\"\n        await self.acquire()\n        return self\n\n    async def __aexit__(self, exc_type, exc_val, exc_tb) -&gt; None:\n        \"\"\"Context manager exit.\"\"\"\n        pass\n\n    def get_stats(self) -&gt; dict:\n        \"\"\"\n        Get current rate limiter statistics.\n\n        Returns:\n            Dict with current token levels and limits\n        \"\"\"\n        return {\n            \"requests_per_minute\": self.requests_per_minute,\n            \"tokens_per_minute\": self.tokens_per_minute,\n            \"available_request_tokens\": self._request_tokens,\n            \"available_api_tokens\": self._api_tokens,\n            \"last_update\": self._last_update,\n        }\n</code></pre>"},{"location":"api/base/#bruno_llm.base.RateLimiter.__init__","title":"<code>__init__(requests_per_minute=60, tokens_per_minute=None)</code>","text":"<p>Initialize rate limiter.</p> <p>Parameters:</p> Name Type Description Default <code>requests_per_minute</code> <code>int</code> <p>Maximum requests allowed per minute</p> <code>60</code> <code>tokens_per_minute</code> <code>Optional[int]</code> <p>Maximum tokens allowed per minute (optional)</p> <code>None</code> Source code in <code>bruno_llm/base/rate_limiter.py</code> <pre><code>def __init__(\n    self,\n    requests_per_minute: int = 60,\n    tokens_per_minute: Optional[int] = None,\n):\n    \"\"\"\n    Initialize rate limiter.\n\n    Args:\n        requests_per_minute: Maximum requests allowed per minute\n        tokens_per_minute: Maximum tokens allowed per minute (optional)\n    \"\"\"\n    self.requests_per_minute = requests_per_minute\n    self.tokens_per_minute = tokens_per_minute\n\n    # Calculate minimum interval between requests\n    self.min_interval = 60.0 / requests_per_minute if requests_per_minute &gt; 0 else 0\n\n    # Token bucket for requests\n    self._request_tokens = float(requests_per_minute)\n    self._max_request_tokens = float(requests_per_minute)\n    self._last_update = time.time()\n\n    # Token bucket for API tokens (if specified)\n    self._api_tokens = float(tokens_per_minute) if tokens_per_minute else None\n    self._max_api_tokens = float(tokens_per_minute) if tokens_per_minute else None\n\n    # Lock for thread safety\n    self._lock = asyncio.Lock()\n</code></pre>"},{"location":"api/base/#bruno_llm.base.RateLimiter.acquire","title":"<code>acquire(api_tokens=0)</code>  <code>async</code>","text":"<p>Acquire permission to make a request.</p> <p>Blocks until rate limit allows the request.</p> <p>Parameters:</p> Name Type Description Default <code>api_tokens</code> <code>int</code> <p>Number of API tokens the request will consume</p> <code>0</code> Source code in <code>bruno_llm/base/rate_limiter.py</code> <pre><code>async def acquire(self, api_tokens: int = 0) -&gt; None:\n    \"\"\"\n    Acquire permission to make a request.\n\n    Blocks until rate limit allows the request.\n\n    Args:\n        api_tokens: Number of API tokens the request will consume\n    \"\"\"\n    async with self._lock:\n        while True:\n            await self._refill_tokens()\n\n            # Check if we have enough request tokens\n            if self._request_tokens &lt; 1:\n                # Calculate wait time\n                wait_time = (1 - self._request_tokens) * (60.0 / self.requests_per_minute)\n                await asyncio.sleep(wait_time)\n                continue\n\n            # Check if we have enough API tokens (if applicable)\n            if self._api_tokens is not None and api_tokens &gt; 0:\n                if self._api_tokens &lt; api_tokens:\n                    wait_time = (api_tokens - self._api_tokens) * (\n                        60.0 / (self.tokens_per_minute or 1)\n                    )\n                    await asyncio.sleep(wait_time)\n                    continue\n\n            # Consume tokens\n            self._request_tokens -= 1\n            if self._api_tokens is not None and api_tokens &gt; 0:\n                self._api_tokens -= api_tokens\n\n            break\n</code></pre>"},{"location":"api/base/#bruno_llm.base.RateLimiter.__aenter__","title":"<code>__aenter__()</code>  <code>async</code>","text":"<p>Context manager entry.</p> Source code in <code>bruno_llm/base/rate_limiter.py</code> <pre><code>async def __aenter__(self) -&gt; \"RateLimiter\":\n    \"\"\"Context manager entry.\"\"\"\n    await self.acquire()\n    return self\n</code></pre>"},{"location":"api/base/#bruno_llm.base.RateLimiter.__aexit__","title":"<code>__aexit__(exc_type, exc_val, exc_tb)</code>  <code>async</code>","text":"<p>Context manager exit.</p> Source code in <code>bruno_llm/base/rate_limiter.py</code> <pre><code>async def __aexit__(self, exc_type, exc_val, exc_tb) -&gt; None:\n    \"\"\"Context manager exit.\"\"\"\n    pass\n</code></pre>"},{"location":"api/base/#bruno_llm.base.RateLimiter.get_stats","title":"<code>get_stats()</code>","text":"<p>Get current rate limiter statistics.</p> <p>Returns:</p> Type Description <code>dict</code> <p>Dict with current token levels and limits</p> Source code in <code>bruno_llm/base/rate_limiter.py</code> <pre><code>def get_stats(self) -&gt; dict:\n    \"\"\"\n    Get current rate limiter statistics.\n\n    Returns:\n        Dict with current token levels and limits\n    \"\"\"\n    return {\n        \"requests_per_minute\": self.requests_per_minute,\n        \"tokens_per_minute\": self.tokens_per_minute,\n        \"available_request_tokens\": self._request_tokens,\n        \"available_api_tokens\": self._api_tokens,\n        \"last_update\": self._last_update,\n    }\n</code></pre>"},{"location":"api/base/#bruno_llm.base.RetryConfig","title":"<code>RetryConfig</code>","text":"<p>Configuration for retry behavior.</p> Example <p>config = RetryConfig( ...     max_retries=5, ...     initial_delay=1.0, ...     max_delay=60.0, ...     exponential_base=2.0 ... )</p> Source code in <code>bruno_llm/base/retry.py</code> <pre><code>class RetryConfig:\n    \"\"\"\n    Configuration for retry behavior.\n\n    Example:\n        &gt;&gt;&gt; config = RetryConfig(\n        ...     max_retries=5,\n        ...     initial_delay=1.0,\n        ...     max_delay=60.0,\n        ...     exponential_base=2.0\n        ... )\n    \"\"\"\n\n    def __init__(\n        self,\n        max_retries: int = 3,\n        initial_delay: float = 1.0,\n        max_delay: float = 60.0,\n        exponential_base: float = 2.0,\n        jitter: bool = True,\n        retry_on: Optional[tuple[type[Exception], ...]] = None,\n    ):\n        \"\"\"\n        Initialize retry configuration.\n\n        Args:\n            max_retries: Maximum number of retry attempts\n            initial_delay: Initial delay between retries in seconds\n            max_delay: Maximum delay between retries in seconds\n            exponential_base: Base for exponential backoff\n            jitter: Whether to add random jitter to delays\n            retry_on: Tuple of exception types to retry on (None = all)\n        \"\"\"\n        self.max_retries = max_retries\n        self.initial_delay = initial_delay\n        self.max_delay = max_delay\n        self.exponential_base = exponential_base\n        self.jitter = jitter\n        self.retry_on = retry_on or (Exception,)\n\n    def calculate_delay(self, attempt: int) -&gt; float:\n        \"\"\"\n        Calculate delay for given retry attempt.\n\n        Uses exponential backoff with optional jitter.\n\n        Args:\n            attempt: Retry attempt number (0-indexed)\n\n        Returns:\n            Delay in seconds\n        \"\"\"\n        # Exponential backoff\n        delay = min(self.initial_delay * (self.exponential_base**attempt), self.max_delay)\n\n        # Add jitter if enabled\n        if self.jitter:\n            jitter_amount = delay * 0.1  # 10% jitter\n            delay += random.uniform(-jitter_amount, jitter_amount)\n\n        return max(0, delay)\n\n    def should_retry(self, exception: Exception, attempt: int) -&gt; bool:\n        \"\"\"\n        Determine if retry should be attempted.\n\n        Args:\n            exception: Exception that was raised\n            attempt: Current attempt number (0-indexed)\n\n        Returns:\n            True if should retry, False otherwise\n        \"\"\"\n        # Check if we've exhausted retries\n        if attempt &gt;= self.max_retries:\n            return False\n\n        # Check if exception type is retryable\n        if not isinstance(exception, self.retry_on):\n            return False\n\n        # Special handling for rate limit errors\n        if isinstance(exception, RateLimitError):\n            return True\n\n        return True\n</code></pre>"},{"location":"api/base/#bruno_llm.base.RetryConfig.__init__","title":"<code>__init__(max_retries=3, initial_delay=1.0, max_delay=60.0, exponential_base=2.0, jitter=True, retry_on=None)</code>","text":"<p>Initialize retry configuration.</p> <p>Parameters:</p> Name Type Description Default <code>max_retries</code> <code>int</code> <p>Maximum number of retry attempts</p> <code>3</code> <code>initial_delay</code> <code>float</code> <p>Initial delay between retries in seconds</p> <code>1.0</code> <code>max_delay</code> <code>float</code> <p>Maximum delay between retries in seconds</p> <code>60.0</code> <code>exponential_base</code> <code>float</code> <p>Base for exponential backoff</p> <code>2.0</code> <code>jitter</code> <code>bool</code> <p>Whether to add random jitter to delays</p> <code>True</code> <code>retry_on</code> <code>Optional[tuple[type[Exception], ...]]</code> <p>Tuple of exception types to retry on (None = all)</p> <code>None</code> Source code in <code>bruno_llm/base/retry.py</code> <pre><code>def __init__(\n    self,\n    max_retries: int = 3,\n    initial_delay: float = 1.0,\n    max_delay: float = 60.0,\n    exponential_base: float = 2.0,\n    jitter: bool = True,\n    retry_on: Optional[tuple[type[Exception], ...]] = None,\n):\n    \"\"\"\n    Initialize retry configuration.\n\n    Args:\n        max_retries: Maximum number of retry attempts\n        initial_delay: Initial delay between retries in seconds\n        max_delay: Maximum delay between retries in seconds\n        exponential_base: Base for exponential backoff\n        jitter: Whether to add random jitter to delays\n        retry_on: Tuple of exception types to retry on (None = all)\n    \"\"\"\n    self.max_retries = max_retries\n    self.initial_delay = initial_delay\n    self.max_delay = max_delay\n    self.exponential_base = exponential_base\n    self.jitter = jitter\n    self.retry_on = retry_on or (Exception,)\n</code></pre>"},{"location":"api/base/#bruno_llm.base.RetryConfig.calculate_delay","title":"<code>calculate_delay(attempt)</code>","text":"<p>Calculate delay for given retry attempt.</p> <p>Uses exponential backoff with optional jitter.</p> <p>Parameters:</p> Name Type Description Default <code>attempt</code> <code>int</code> <p>Retry attempt number (0-indexed)</p> required <p>Returns:</p> Type Description <code>float</code> <p>Delay in seconds</p> Source code in <code>bruno_llm/base/retry.py</code> <pre><code>def calculate_delay(self, attempt: int) -&gt; float:\n    \"\"\"\n    Calculate delay for given retry attempt.\n\n    Uses exponential backoff with optional jitter.\n\n    Args:\n        attempt: Retry attempt number (0-indexed)\n\n    Returns:\n        Delay in seconds\n    \"\"\"\n    # Exponential backoff\n    delay = min(self.initial_delay * (self.exponential_base**attempt), self.max_delay)\n\n    # Add jitter if enabled\n    if self.jitter:\n        jitter_amount = delay * 0.1  # 10% jitter\n        delay += random.uniform(-jitter_amount, jitter_amount)\n\n    return max(0, delay)\n</code></pre>"},{"location":"api/base/#bruno_llm.base.RetryConfig.should_retry","title":"<code>should_retry(exception, attempt)</code>","text":"<p>Determine if retry should be attempted.</p> <p>Parameters:</p> Name Type Description Default <code>exception</code> <code>Exception</code> <p>Exception that was raised</p> required <code>attempt</code> <code>int</code> <p>Current attempt number (0-indexed)</p> required <p>Returns:</p> Type Description <code>bool</code> <p>True if should retry, False otherwise</p> Source code in <code>bruno_llm/base/retry.py</code> <pre><code>def should_retry(self, exception: Exception, attempt: int) -&gt; bool:\n    \"\"\"\n    Determine if retry should be attempted.\n\n    Args:\n        exception: Exception that was raised\n        attempt: Current attempt number (0-indexed)\n\n    Returns:\n        True if should retry, False otherwise\n    \"\"\"\n    # Check if we've exhausted retries\n    if attempt &gt;= self.max_retries:\n        return False\n\n    # Check if exception type is retryable\n    if not isinstance(exception, self.retry_on):\n        return False\n\n    # Special handling for rate limit errors\n    if isinstance(exception, RateLimitError):\n        return True\n\n    return True\n</code></pre>"},{"location":"api/base/#bruno_llm.base.RetryDecorator","title":"<code>RetryDecorator</code>","text":"<p>Decorator for adding retry logic to async functions.</p> Example <p>@RetryDecorator(max_retries=5) ... async def api_call(): ...     return await external_api()</p> Source code in <code>bruno_llm/base/retry.py</code> <pre><code>class RetryDecorator:\n    \"\"\"\n    Decorator for adding retry logic to async functions.\n\n    Example:\n        &gt;&gt;&gt; @RetryDecorator(max_retries=5)\n        ... async def api_call():\n        ...     return await external_api()\n    \"\"\"\n\n    def __init__(\n        self,\n        max_retries: int = 3,\n        initial_delay: float = 1.0,\n        max_delay: float = 60.0,\n        exponential_base: float = 2.0,\n        jitter: bool = True,\n    ):\n        \"\"\"\n        Initialize retry decorator.\n\n        Args:\n            max_retries: Maximum number of retry attempts\n            initial_delay: Initial delay between retries\n            max_delay: Maximum delay between retries\n            exponential_base: Base for exponential backoff\n            jitter: Whether to add jitter\n        \"\"\"\n        self.config = RetryConfig(\n            max_retries=max_retries,\n            initial_delay=initial_delay,\n            max_delay=max_delay,\n            exponential_base=exponential_base,\n            jitter=jitter,\n        )\n\n    def __call__(self, func: Callable[..., T]) -&gt; Callable[..., T]:\n        \"\"\"\n        Wrap function with retry logic.\n\n        Args:\n            func: Function to wrap\n\n        Returns:\n            Wrapped function\n        \"\"\"\n\n        async def wrapper(*args: Any, **kwargs: Any) -&gt; T:\n            return await retry_async(func, *args, config=self.config, **kwargs)\n\n        return wrapper\n</code></pre>"},{"location":"api/base/#bruno_llm.base.RetryDecorator.__init__","title":"<code>__init__(max_retries=3, initial_delay=1.0, max_delay=60.0, exponential_base=2.0, jitter=True)</code>","text":"<p>Initialize retry decorator.</p> <p>Parameters:</p> Name Type Description Default <code>max_retries</code> <code>int</code> <p>Maximum number of retry attempts</p> <code>3</code> <code>initial_delay</code> <code>float</code> <p>Initial delay between retries</p> <code>1.0</code> <code>max_delay</code> <code>float</code> <p>Maximum delay between retries</p> <code>60.0</code> <code>exponential_base</code> <code>float</code> <p>Base for exponential backoff</p> <code>2.0</code> <code>jitter</code> <code>bool</code> <p>Whether to add jitter</p> <code>True</code> Source code in <code>bruno_llm/base/retry.py</code> <pre><code>def __init__(\n    self,\n    max_retries: int = 3,\n    initial_delay: float = 1.0,\n    max_delay: float = 60.0,\n    exponential_base: float = 2.0,\n    jitter: bool = True,\n):\n    \"\"\"\n    Initialize retry decorator.\n\n    Args:\n        max_retries: Maximum number of retry attempts\n        initial_delay: Initial delay between retries\n        max_delay: Maximum delay between retries\n        exponential_base: Base for exponential backoff\n        jitter: Whether to add jitter\n    \"\"\"\n    self.config = RetryConfig(\n        max_retries=max_retries,\n        initial_delay=initial_delay,\n        max_delay=max_delay,\n        exponential_base=exponential_base,\n        jitter=jitter,\n    )\n</code></pre>"},{"location":"api/base/#bruno_llm.base.RetryDecorator.__call__","title":"<code>__call__(func)</code>","text":"<p>Wrap function with retry logic.</p> <p>Parameters:</p> Name Type Description Default <code>func</code> <code>Callable[..., T]</code> <p>Function to wrap</p> required <p>Returns:</p> Type Description <code>Callable[..., T]</code> <p>Wrapped function</p> Source code in <code>bruno_llm/base/retry.py</code> <pre><code>def __call__(self, func: Callable[..., T]) -&gt; Callable[..., T]:\n    \"\"\"\n    Wrap function with retry logic.\n\n    Args:\n        func: Function to wrap\n\n    Returns:\n        Wrapped function\n    \"\"\"\n\n    async def wrapper(*args: Any, **kwargs: Any) -&gt; T:\n        return await retry_async(func, *args, config=self.config, **kwargs)\n\n    return wrapper\n</code></pre>"},{"location":"api/base/#bruno_llm.base.StreamAggregator","title":"<code>StreamAggregator</code>","text":"<p>Aggregate streaming chunks with various strategies.</p> <p>Provides different aggregation strategies for streaming responses: - Word-by-word: Buffer until complete words - Sentence-by-sentence: Buffer until sentence boundaries - Fixed-size: Buffer until fixed character count - Time-based: Buffer for fixed time intervals</p> <p>Parameters:</p> Name Type Description Default <code>strategy</code> <code>str</code> <p>Aggregation strategy ('word', 'sentence', 'fixed', 'time')</p> <code>'word'</code> <code>size</code> <code>int</code> <p>Size parameter (chars for 'fixed', seconds for 'time')</p> <code>10</code> Example <p>aggregator = StreamAggregator(strategy='word') async for chunk in aggregator.aggregate(stream): ...     print(chunk)  # Prints complete words</p> Source code in <code>bruno_llm/base/streaming.py</code> <pre><code>class StreamAggregator:\n    \"\"\"\n    Aggregate streaming chunks with various strategies.\n\n    Provides different aggregation strategies for streaming responses:\n    - Word-by-word: Buffer until complete words\n    - Sentence-by-sentence: Buffer until sentence boundaries\n    - Fixed-size: Buffer until fixed character count\n    - Time-based: Buffer for fixed time intervals\n\n    Args:\n        strategy: Aggregation strategy ('word', 'sentence', 'fixed', 'time')\n        size: Size parameter (chars for 'fixed', seconds for 'time')\n\n    Example:\n        &gt;&gt;&gt; aggregator = StreamAggregator(strategy='word')\n        &gt;&gt;&gt; async for chunk in aggregator.aggregate(stream):\n        ...     print(chunk)  # Prints complete words\n    \"\"\"\n\n    def __init__(\n        self,\n        strategy: str = \"word\",\n        size: int = 10,\n    ):\n        \"\"\"\n        Initialize stream aggregator.\n\n        Args:\n            strategy: Aggregation strategy\n            size: Size parameter for aggregation\n        \"\"\"\n        self.strategy = strategy\n        self.size = size\n        self._buffer = \"\"\n\n    async def aggregate(self, stream: AsyncIterator[str]) -&gt; AsyncIterator[str]:\n        \"\"\"\n        Aggregate stream chunks according to strategy.\n\n        Args:\n            stream: Input stream to aggregate\n\n        Yields:\n            Aggregated chunks\n        \"\"\"\n        if self.strategy == \"word\":\n            async for chunk in self._aggregate_words(stream):\n                yield chunk\n        elif self.strategy == \"sentence\":\n            async for chunk in self._aggregate_sentences(stream):\n                yield chunk\n        elif self.strategy == \"fixed\":\n            async for chunk in self._aggregate_fixed(stream):\n                yield chunk\n        elif self.strategy == \"time\":\n            async for chunk in self._aggregate_time(stream):\n                yield chunk\n        else:\n            # No aggregation, pass through\n            async for chunk in stream:\n                yield chunk\n\n    async def _aggregate_words(self, stream: AsyncIterator[str]) -&gt; AsyncIterator[str]:\n        \"\"\"Aggregate chunks into complete words.\"\"\"\n        async for chunk in stream:\n            self._buffer += chunk\n\n            # Split on whitespace while keeping incomplete words\n            parts = self._buffer.split()\n\n            if len(parts) &gt; 1:\n                # Yield all complete words\n                for word in parts[:-1]:\n                    yield word + \" \"\n\n                # Keep the last part as buffer (might be incomplete)\n                self._buffer = parts[-1]\n            elif self._buffer.endswith((\" \", \"\\n\", \"\\t\")):\n                # Buffer ends with whitespace, yield it\n                yield self._buffer\n                self._buffer = \"\"\n\n        # Flush remaining buffer\n        if self._buffer:\n            yield self._buffer\n            self._buffer = \"\"\n\n    async def _aggregate_sentences(self, stream: AsyncIterator[str]) -&gt; AsyncIterator[str]:\n        \"\"\"Aggregate chunks into complete sentences.\"\"\"\n        sentence_endings = (\".\", \"!\", \"?\", \"\\n\")\n\n        async for chunk in stream:\n            self._buffer += chunk\n\n            # Check if buffer contains sentence ending\n            while any(self._buffer.endswith(end) for end in sentence_endings):\n                # Find last sentence ending\n                last_idx = -1\n                for ending in sentence_endings:\n                    idx = self._buffer.rfind(ending)\n                    if idx &gt; last_idx:\n                        last_idx = idx\n\n                if last_idx &gt;= 0:\n                    # Yield complete sentence(s)\n                    sentence = self._buffer[: last_idx + 1]\n                    yield sentence\n                    self._buffer = self._buffer[last_idx + 1 :]\n                else:\n                    break\n\n        # Flush remaining buffer\n        if self._buffer:\n            yield self._buffer\n            self._buffer = \"\"\n\n    async def _aggregate_fixed(self, stream: AsyncIterator[str]) -&gt; AsyncIterator[str]:\n        \"\"\"Aggregate chunks into fixed-size chunks.\"\"\"\n        async for chunk in stream:\n            self._buffer += chunk\n\n            # Yield chunks of fixed size\n            while len(self._buffer) &gt;= self.size:\n                yield self._buffer[: self.size]\n                self._buffer = self._buffer[self.size :]\n\n        # Flush remaining buffer\n        if self._buffer:\n            yield self._buffer\n            self._buffer = \"\"\n\n    async def _aggregate_time(self, stream: AsyncIterator[str]) -&gt; AsyncIterator[str]:\n        \"\"\"Aggregate chunks based on time intervals.\"\"\"\n        import time\n\n        last_yield = time.time()\n\n        async for chunk in stream:\n            self._buffer += chunk\n\n            current_time = time.time()\n            if current_time - last_yield &gt;= self.size:\n                if self._buffer:\n                    yield self._buffer\n                    self._buffer = \"\"\n                    last_yield = current_time\n\n        # Flush remaining buffer\n        if self._buffer:\n            yield self._buffer\n            self._buffer = \"\"\n</code></pre>"},{"location":"api/base/#bruno_llm.base.StreamAggregator.__init__","title":"<code>__init__(strategy='word', size=10)</code>","text":"<p>Initialize stream aggregator.</p> <p>Parameters:</p> Name Type Description Default <code>strategy</code> <code>str</code> <p>Aggregation strategy</p> <code>'word'</code> <code>size</code> <code>int</code> <p>Size parameter for aggregation</p> <code>10</code> Source code in <code>bruno_llm/base/streaming.py</code> <pre><code>def __init__(\n    self,\n    strategy: str = \"word\",\n    size: int = 10,\n):\n    \"\"\"\n    Initialize stream aggregator.\n\n    Args:\n        strategy: Aggregation strategy\n        size: Size parameter for aggregation\n    \"\"\"\n    self.strategy = strategy\n    self.size = size\n    self._buffer = \"\"\n</code></pre>"},{"location":"api/base/#bruno_llm.base.StreamAggregator.aggregate","title":"<code>aggregate(stream)</code>  <code>async</code>","text":"<p>Aggregate stream chunks according to strategy.</p> <p>Parameters:</p> Name Type Description Default <code>stream</code> <code>AsyncIterator[str]</code> <p>Input stream to aggregate</p> required <p>Yields:</p> Type Description <code>AsyncIterator[str]</code> <p>Aggregated chunks</p> Source code in <code>bruno_llm/base/streaming.py</code> <pre><code>async def aggregate(self, stream: AsyncIterator[str]) -&gt; AsyncIterator[str]:\n    \"\"\"\n    Aggregate stream chunks according to strategy.\n\n    Args:\n        stream: Input stream to aggregate\n\n    Yields:\n        Aggregated chunks\n    \"\"\"\n    if self.strategy == \"word\":\n        async for chunk in self._aggregate_words(stream):\n            yield chunk\n    elif self.strategy == \"sentence\":\n        async for chunk in self._aggregate_sentences(stream):\n            yield chunk\n    elif self.strategy == \"fixed\":\n        async for chunk in self._aggregate_fixed(stream):\n            yield chunk\n    elif self.strategy == \"time\":\n        async for chunk in self._aggregate_time(stream):\n            yield chunk\n    else:\n        # No aggregation, pass through\n        async for chunk in stream:\n            yield chunk\n</code></pre>"},{"location":"api/base/#bruno_llm.base.StreamBuffer","title":"<code>StreamBuffer</code>  <code>dataclass</code>","text":"<p>Buffer for managing streaming chunks.</p> <p>Provides buffering, batching, and aggregation of stream chunks.</p> <p>Attributes:</p> Name Type Description <code>buffer</code> <code>deque[str]</code> <p>Internal deque for storing chunks</p> <code>max_size</code> <code>int</code> <p>Maximum buffer size in characters</p> <code>batch_size</code> <code>int</code> <p>Number of chunks to batch before yielding</p> <code>stats</code> <code>StreamStats</code> <p>Stream statistics</p> Source code in <code>bruno_llm/base/streaming.py</code> <pre><code>@dataclass\nclass StreamBuffer:\n    \"\"\"\n    Buffer for managing streaming chunks.\n\n    Provides buffering, batching, and aggregation of stream chunks.\n\n    Attributes:\n        buffer: Internal deque for storing chunks\n        max_size: Maximum buffer size in characters\n        batch_size: Number of chunks to batch before yielding\n        stats: Stream statistics\n    \"\"\"\n\n    buffer: deque[str] = field(default_factory=deque)\n    max_size: int = 10000\n    batch_size: int = 1\n    stats: StreamStats = field(default_factory=StreamStats)\n\n    def add(self, chunk: str) -&gt; None:\n        \"\"\"\n        Add a chunk to the buffer.\n\n        Args:\n            chunk: Text chunk to buffer\n\n        Raises:\n            StreamError: If buffer is full\n        \"\"\"\n        current_size = sum(len(c) for c in self.buffer)\n\n        if current_size + len(chunk) &gt; self.max_size:\n            raise StreamError(\n                f\"Stream buffer full ({current_size} chars). \"\n                f\"Consider increasing max_size or consuming buffer faster.\"\n            )\n\n        self.buffer.append(chunk)\n        self.stats.chunks_received += 1\n        self.stats.total_chars += len(chunk)\n\n    def get_batch(self) -&gt; Optional[str]:\n        \"\"\"\n        Get a batch of chunks.\n\n        Returns:\n            Concatenated batch or None if not enough chunks\n        \"\"\"\n        if len(self.buffer) &lt; self.batch_size:\n            return None\n\n        chunks = []\n        for _ in range(min(self.batch_size, len(self.buffer))):\n            chunks.append(self.buffer.popleft())\n\n        return \"\".join(chunks)\n\n    def flush(self) -&gt; str:\n        \"\"\"\n        Flush all remaining chunks.\n\n        Returns:\n            All remaining chunks concatenated\n        \"\"\"\n        chunks = list(self.buffer)\n        self.buffer.clear()\n        return \"\".join(chunks)\n\n    def is_empty(self) -&gt; bool:\n        \"\"\"Check if buffer is empty.\"\"\"\n        return len(self.buffer) == 0\n\n    def clear(self) -&gt; None:\n        \"\"\"Clear buffer and reset stats.\"\"\"\n        self.buffer.clear()\n        self.stats = StreamStats()\n</code></pre>"},{"location":"api/base/#bruno_llm.base.StreamBuffer.add","title":"<code>add(chunk)</code>","text":"<p>Add a chunk to the buffer.</p> <p>Parameters:</p> Name Type Description Default <code>chunk</code> <code>str</code> <p>Text chunk to buffer</p> required <p>Raises:</p> Type Description <code>StreamError</code> <p>If buffer is full</p> Source code in <code>bruno_llm/base/streaming.py</code> <pre><code>def add(self, chunk: str) -&gt; None:\n    \"\"\"\n    Add a chunk to the buffer.\n\n    Args:\n        chunk: Text chunk to buffer\n\n    Raises:\n        StreamError: If buffer is full\n    \"\"\"\n    current_size = sum(len(c) for c in self.buffer)\n\n    if current_size + len(chunk) &gt; self.max_size:\n        raise StreamError(\n            f\"Stream buffer full ({current_size} chars). \"\n            f\"Consider increasing max_size or consuming buffer faster.\"\n        )\n\n    self.buffer.append(chunk)\n    self.stats.chunks_received += 1\n    self.stats.total_chars += len(chunk)\n</code></pre>"},{"location":"api/base/#bruno_llm.base.StreamBuffer.get_batch","title":"<code>get_batch()</code>","text":"<p>Get a batch of chunks.</p> <p>Returns:</p> Type Description <code>Optional[str]</code> <p>Concatenated batch or None if not enough chunks</p> Source code in <code>bruno_llm/base/streaming.py</code> <pre><code>def get_batch(self) -&gt; Optional[str]:\n    \"\"\"\n    Get a batch of chunks.\n\n    Returns:\n        Concatenated batch or None if not enough chunks\n    \"\"\"\n    if len(self.buffer) &lt; self.batch_size:\n        return None\n\n    chunks = []\n    for _ in range(min(self.batch_size, len(self.buffer))):\n        chunks.append(self.buffer.popleft())\n\n    return \"\".join(chunks)\n</code></pre>"},{"location":"api/base/#bruno_llm.base.StreamBuffer.flush","title":"<code>flush()</code>","text":"<p>Flush all remaining chunks.</p> <p>Returns:</p> Type Description <code>str</code> <p>All remaining chunks concatenated</p> Source code in <code>bruno_llm/base/streaming.py</code> <pre><code>def flush(self) -&gt; str:\n    \"\"\"\n    Flush all remaining chunks.\n\n    Returns:\n        All remaining chunks concatenated\n    \"\"\"\n    chunks = list(self.buffer)\n    self.buffer.clear()\n    return \"\".join(chunks)\n</code></pre>"},{"location":"api/base/#bruno_llm.base.StreamBuffer.is_empty","title":"<code>is_empty()</code>","text":"<p>Check if buffer is empty.</p> Source code in <code>bruno_llm/base/streaming.py</code> <pre><code>def is_empty(self) -&gt; bool:\n    \"\"\"Check if buffer is empty.\"\"\"\n    return len(self.buffer) == 0\n</code></pre>"},{"location":"api/base/#bruno_llm.base.StreamBuffer.clear","title":"<code>clear()</code>","text":"<p>Clear buffer and reset stats.</p> Source code in <code>bruno_llm/base/streaming.py</code> <pre><code>def clear(self) -&gt; None:\n    \"\"\"Clear buffer and reset stats.\"\"\"\n    self.buffer.clear()\n    self.stats = StreamStats()\n</code></pre>"},{"location":"api/base/#bruno_llm.base.StreamProcessor","title":"<code>StreamProcessor</code>","text":"<p>Process streaming responses with callbacks and error handling.</p> <p>Provides a framework for processing streams with: - Progress callbacks - Error recovery - Automatic retry on connection loss - Statistics tracking</p> <p>Parameters:</p> Name Type Description Default <code>on_chunk</code> <code>Optional[Callable[[str], None]]</code> <p>Callback for each chunk (chunk: str) -&gt; None</p> <code>None</code> <code>on_error</code> <code>Optional[Callable[[Exception], None]]</code> <p>Callback for errors (error: Exception) -&gt; None</p> <code>None</code> <code>on_complete</code> <code>Optional[Callable[[StreamStats], None]]</code> <p>Callback when stream completes (stats: StreamStats) -&gt; None</p> <code>None</code> <code>max_retries</code> <code>int</code> <p>Maximum number of retries on error</p> <code>3</code> Example <p>processor = StreamProcessor( ...     on_chunk=lambda chunk: print(chunk, end=\"\"), ...     on_error=lambda e: print(f\"Error: {e}\"), ...     on_complete=lambda stats: print(f\"\\nReceived {stats.chunks_received} chunks\") ... ) await processor.process(stream)</p> Source code in <code>bruno_llm/base/streaming.py</code> <pre><code>class StreamProcessor:\n    \"\"\"\n    Process streaming responses with callbacks and error handling.\n\n    Provides a framework for processing streams with:\n    - Progress callbacks\n    - Error recovery\n    - Automatic retry on connection loss\n    - Statistics tracking\n\n    Args:\n        on_chunk: Callback for each chunk (chunk: str) -&gt; None\n        on_error: Callback for errors (error: Exception) -&gt; None\n        on_complete: Callback when stream completes (stats: StreamStats) -&gt; None\n        max_retries: Maximum number of retries on error\n\n    Example:\n        &gt;&gt;&gt; processor = StreamProcessor(\n        ...     on_chunk=lambda chunk: print(chunk, end=\"\"),\n        ...     on_error=lambda e: print(f\"Error: {e}\"),\n        ...     on_complete=lambda stats: print(f\"\\\\nReceived {stats.chunks_received} chunks\")\n        ... )\n        &gt;&gt;&gt; await processor.process(stream)\n    \"\"\"\n\n    def __init__(\n        self,\n        on_chunk: Optional[Callable[[str], None]] = None,\n        on_error: Optional[Callable[[Exception], None]] = None,\n        on_complete: Optional[Callable[[StreamStats], None]] = None,\n        max_retries: int = 3,\n    ):\n        \"\"\"\n        Initialize stream processor.\n\n        Args:\n            on_chunk: Callback for each chunk\n            on_error: Callback for errors\n            on_complete: Callback when stream completes\n            max_retries: Maximum number of retries on error\n        \"\"\"\n        self.on_chunk = on_chunk\n        self.on_error = on_error\n        self.on_complete = on_complete\n        self.max_retries = max_retries\n        self.stats = StreamStats()\n\n    async def process(\n        self,\n        stream: AsyncIterator[str],\n        retry_on_error: bool = True,\n    ) -&gt; list[str]:\n        \"\"\"\n        Process a stream with callbacks and error handling.\n\n        Args:\n            stream: Input stream to process\n            retry_on_error: Whether to retry on errors\n\n        Returns:\n            List of all chunks received\n\n        Raises:\n            StreamError: If max retries exceeded\n        \"\"\"\n        import time\n\n        chunks = []\n        start_time = time.time()\n        retries = 0\n\n        try:\n            async for chunk in stream:\n                chunks.append(chunk)\n                self.stats.chunks_received += 1\n                self.stats.total_chars += len(chunk)\n\n                if self.on_chunk:\n                    self.on_chunk(chunk)\n\n            # Calculate duration after stream completes\n            self.stats.duration = time.time() - start_time\n\n            if self.on_complete:\n                self.on_complete(self.stats)\n\n            return chunks\n\n        except Exception as e:\n            self.stats.errors += 1\n            self.stats.duration = time.time() - start_time\n\n            if self.on_error:\n                self.on_error(e)\n\n            if retry_on_error and retries &lt; self.max_retries:\n                retries += 1\n                await asyncio.sleep(2**retries)  # Exponential backoff\n                return await self.process(stream, retry_on_error)\n\n            raise StreamError(f\"Stream processing failed: {e}\") from e\n</code></pre>"},{"location":"api/base/#bruno_llm.base.StreamProcessor.__init__","title":"<code>__init__(on_chunk=None, on_error=None, on_complete=None, max_retries=3)</code>","text":"<p>Initialize stream processor.</p> <p>Parameters:</p> Name Type Description Default <code>on_chunk</code> <code>Optional[Callable[[str], None]]</code> <p>Callback for each chunk</p> <code>None</code> <code>on_error</code> <code>Optional[Callable[[Exception], None]]</code> <p>Callback for errors</p> <code>None</code> <code>on_complete</code> <code>Optional[Callable[[StreamStats], None]]</code> <p>Callback when stream completes</p> <code>None</code> <code>max_retries</code> <code>int</code> <p>Maximum number of retries on error</p> <code>3</code> Source code in <code>bruno_llm/base/streaming.py</code> <pre><code>def __init__(\n    self,\n    on_chunk: Optional[Callable[[str], None]] = None,\n    on_error: Optional[Callable[[Exception], None]] = None,\n    on_complete: Optional[Callable[[StreamStats], None]] = None,\n    max_retries: int = 3,\n):\n    \"\"\"\n    Initialize stream processor.\n\n    Args:\n        on_chunk: Callback for each chunk\n        on_error: Callback for errors\n        on_complete: Callback when stream completes\n        max_retries: Maximum number of retries on error\n    \"\"\"\n    self.on_chunk = on_chunk\n    self.on_error = on_error\n    self.on_complete = on_complete\n    self.max_retries = max_retries\n    self.stats = StreamStats()\n</code></pre>"},{"location":"api/base/#bruno_llm.base.StreamProcessor.process","title":"<code>process(stream, retry_on_error=True)</code>  <code>async</code>","text":"<p>Process a stream with callbacks and error handling.</p> <p>Parameters:</p> Name Type Description Default <code>stream</code> <code>AsyncIterator[str]</code> <p>Input stream to process</p> required <code>retry_on_error</code> <code>bool</code> <p>Whether to retry on errors</p> <code>True</code> <p>Returns:</p> Type Description <code>list[str]</code> <p>List of all chunks received</p> <p>Raises:</p> Type Description <code>StreamError</code> <p>If max retries exceeded</p> Source code in <code>bruno_llm/base/streaming.py</code> <pre><code>async def process(\n    self,\n    stream: AsyncIterator[str],\n    retry_on_error: bool = True,\n) -&gt; list[str]:\n    \"\"\"\n    Process a stream with callbacks and error handling.\n\n    Args:\n        stream: Input stream to process\n        retry_on_error: Whether to retry on errors\n\n    Returns:\n        List of all chunks received\n\n    Raises:\n        StreamError: If max retries exceeded\n    \"\"\"\n    import time\n\n    chunks = []\n    start_time = time.time()\n    retries = 0\n\n    try:\n        async for chunk in stream:\n            chunks.append(chunk)\n            self.stats.chunks_received += 1\n            self.stats.total_chars += len(chunk)\n\n            if self.on_chunk:\n                self.on_chunk(chunk)\n\n        # Calculate duration after stream completes\n        self.stats.duration = time.time() - start_time\n\n        if self.on_complete:\n            self.on_complete(self.stats)\n\n        return chunks\n\n    except Exception as e:\n        self.stats.errors += 1\n        self.stats.duration = time.time() - start_time\n\n        if self.on_error:\n            self.on_error(e)\n\n        if retry_on_error and retries &lt; self.max_retries:\n            retries += 1\n            await asyncio.sleep(2**retries)  # Exponential backoff\n            return await self.process(stream, retry_on_error)\n\n        raise StreamError(f\"Stream processing failed: {e}\") from e\n</code></pre>"},{"location":"api/base/#bruno_llm.base.StreamStats","title":"<code>StreamStats</code>  <code>dataclass</code>","text":"<p>Statistics for a streaming session.</p> <p>Attributes:</p> Name Type Description <code>chunks_received</code> <code>int</code> <p>Number of chunks received</p> <code>total_chars</code> <code>int</code> <p>Total characters streamed</p> <code>total_tokens</code> <code>int</code> <p>Estimated token count</p> <code>duration</code> <code>float</code> <p>Duration of stream in seconds</p> <code>errors</code> <code>int</code> <p>Number of errors encountered</p> Source code in <code>bruno_llm/base/streaming.py</code> <pre><code>@dataclass\nclass StreamStats:\n    \"\"\"\n    Statistics for a streaming session.\n\n    Attributes:\n        chunks_received: Number of chunks received\n        total_chars: Total characters streamed\n        total_tokens: Estimated token count\n        duration: Duration of stream in seconds\n        errors: Number of errors encountered\n    \"\"\"\n\n    chunks_received: int = 0\n    total_chars: int = 0\n    total_tokens: int = 0\n    duration: float = 0.0\n    errors: int = 0\n</code></pre>"},{"location":"api/base/#bruno_llm.base.SimpleTokenCounter","title":"<code>SimpleTokenCounter</code>","text":"<p>               Bases: <code>TokenCounter</code></p> <p>Simple token counter using word splitting.</p> <p>This is a fallback implementation that approximates token count by counting words. Not as accurate as provider-specific tokenizers but works universally.</p> Example <p>counter = SimpleTokenCounter() tokens = counter.count_tokens(\"Hello world!\") print(tokens)  # Approximately 2-3</p> Source code in <code>bruno_llm/base/token_counter.py</code> <pre><code>class SimpleTokenCounter(TokenCounter):\n    \"\"\"\n    Simple token counter using word splitting.\n\n    This is a fallback implementation that approximates token count\n    by counting words. Not as accurate as provider-specific tokenizers\n    but works universally.\n\n    Example:\n        &gt;&gt;&gt; counter = SimpleTokenCounter()\n        &gt;&gt;&gt; tokens = counter.count_tokens(\"Hello world!\")\n        &gt;&gt;&gt; print(tokens)  # Approximately 2-3\n    \"\"\"\n\n    def __init__(self, chars_per_token: float = 4.0):\n        \"\"\"\n        Initialize simple token counter.\n\n        Args:\n            chars_per_token: Average characters per token (default: 4)\n        \"\"\"\n        self.chars_per_token = chars_per_token\n\n    def count_tokens(self, text: str) -&gt; int:\n        \"\"\"\n        Count tokens using character-based estimation.\n\n        Uses the common approximation that 1 token \u2248 4 characters\n        in English text.\n\n        Args:\n            text: Text to count tokens for\n\n        Returns:\n            Estimated token count\n        \"\"\"\n        if not text:\n            return 0\n        return max(1, int(len(text) / self.chars_per_token))\n</code></pre>"},{"location":"api/base/#bruno_llm.base.SimpleTokenCounter.__init__","title":"<code>__init__(chars_per_token=4.0)</code>","text":"<p>Initialize simple token counter.</p> <p>Parameters:</p> Name Type Description Default <code>chars_per_token</code> <code>float</code> <p>Average characters per token (default: 4)</p> <code>4.0</code> Source code in <code>bruno_llm/base/token_counter.py</code> <pre><code>def __init__(self, chars_per_token: float = 4.0):\n    \"\"\"\n    Initialize simple token counter.\n\n    Args:\n        chars_per_token: Average characters per token (default: 4)\n    \"\"\"\n    self.chars_per_token = chars_per_token\n</code></pre>"},{"location":"api/base/#bruno_llm.base.SimpleTokenCounter.count_tokens","title":"<code>count_tokens(text)</code>","text":"<p>Count tokens using character-based estimation.</p> <p>Uses the common approximation that 1 token \u2248 4 characters in English text.</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>str</code> <p>Text to count tokens for</p> required <p>Returns:</p> Type Description <code>int</code> <p>Estimated token count</p> Source code in <code>bruno_llm/base/token_counter.py</code> <pre><code>def count_tokens(self, text: str) -&gt; int:\n    \"\"\"\n    Count tokens using character-based estimation.\n\n    Uses the common approximation that 1 token \u2248 4 characters\n    in English text.\n\n    Args:\n        text: Text to count tokens for\n\n    Returns:\n        Estimated token count\n    \"\"\"\n    if not text:\n        return 0\n    return max(1, int(len(text) / self.chars_per_token))\n</code></pre>"},{"location":"api/base/#bruno_llm.base.TikTokenCounter","title":"<code>TikTokenCounter</code>","text":"<p>               Bases: <code>TokenCounter</code></p> <p>Token counter using OpenAI's tiktoken library.</p> <p>Provides accurate token counting for OpenAI models. Falls back to SimpleTokenCounter if tiktoken is not available.</p> Example <p>counter = TikTokenCounter(model=\"gpt-4\") tokens = counter.count_tokens(\"Hello world!\") print(tokens)</p> Source code in <code>bruno_llm/base/token_counter.py</code> <pre><code>class TikTokenCounter(TokenCounter):\n    \"\"\"\n    Token counter using OpenAI's tiktoken library.\n\n    Provides accurate token counting for OpenAI models.\n    Falls back to SimpleTokenCounter if tiktoken is not available.\n\n    Example:\n        &gt;&gt;&gt; counter = TikTokenCounter(model=\"gpt-4\")\n        &gt;&gt;&gt; tokens = counter.count_tokens(\"Hello world!\")\n        &gt;&gt;&gt; print(tokens)\n    \"\"\"\n\n    def __init__(self, model: str = \"gpt-4\"):\n        \"\"\"\n        Initialize tiktoken-based counter.\n\n        Args:\n            model: Model name for tiktoken encoding\n        \"\"\"\n        self.model = model\n        self._encoding = None\n        self._fallback = SimpleTokenCounter()\n\n        try:\n            import tiktoken\n\n            self._encoding = tiktoken.encoding_for_model(model)\n        except ImportError:\n            # tiktoken not available, will use fallback\n            pass\n        except Exception:\n            # Model not found or other error, use fallback\n            pass\n\n    def count_tokens(self, text: str) -&gt; int:\n        \"\"\"\n        Count tokens using tiktoken or fallback.\n\n        Args:\n            text: Text to count tokens for\n\n        Returns:\n            Accurate token count (if tiktoken available) or estimate\n        \"\"\"\n        if not text:\n            return 0\n\n        if self._encoding is not None:\n            try:\n                return len(self._encoding.encode(text))\n            except Exception:\n                pass\n\n        # Fallback to simple counting\n        return self._fallback.count_tokens(text)\n</code></pre>"},{"location":"api/base/#bruno_llm.base.TikTokenCounter.__init__","title":"<code>__init__(model='gpt-4')</code>","text":"<p>Initialize tiktoken-based counter.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>str</code> <p>Model name for tiktoken encoding</p> <code>'gpt-4'</code> Source code in <code>bruno_llm/base/token_counter.py</code> <pre><code>def __init__(self, model: str = \"gpt-4\"):\n    \"\"\"\n    Initialize tiktoken-based counter.\n\n    Args:\n        model: Model name for tiktoken encoding\n    \"\"\"\n    self.model = model\n    self._encoding = None\n    self._fallback = SimpleTokenCounter()\n\n    try:\n        import tiktoken\n\n        self._encoding = tiktoken.encoding_for_model(model)\n    except ImportError:\n        # tiktoken not available, will use fallback\n        pass\n    except Exception:\n        # Model not found or other error, use fallback\n        pass\n</code></pre>"},{"location":"api/base/#bruno_llm.base.TikTokenCounter.count_tokens","title":"<code>count_tokens(text)</code>","text":"<p>Count tokens using tiktoken or fallback.</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>str</code> <p>Text to count tokens for</p> required <p>Returns:</p> Type Description <code>int</code> <p>Accurate token count (if tiktoken available) or estimate</p> Source code in <code>bruno_llm/base/token_counter.py</code> <pre><code>def count_tokens(self, text: str) -&gt; int:\n    \"\"\"\n    Count tokens using tiktoken or fallback.\n\n    Args:\n        text: Text to count tokens for\n\n    Returns:\n        Accurate token count (if tiktoken available) or estimate\n    \"\"\"\n    if not text:\n        return 0\n\n    if self._encoding is not None:\n        try:\n            return len(self._encoding.encode(text))\n        except Exception:\n            pass\n\n    # Fallback to simple counting\n    return self._fallback.count_tokens(text)\n</code></pre>"},{"location":"api/base/#bruno_llm.base.TokenCounter","title":"<code>TokenCounter</code>","text":"<p>               Bases: <code>ABC</code></p> <p>Abstract base class for token counting.</p> <p>Different providers may have different tokenization methods. Subclasses should implement provider-specific counting logic.</p> Source code in <code>bruno_llm/base/token_counter.py</code> <pre><code>class TokenCounter(ABC):\n    \"\"\"\n    Abstract base class for token counting.\n\n    Different providers may have different tokenization methods.\n    Subclasses should implement provider-specific counting logic.\n    \"\"\"\n\n    @abstractmethod\n    def count_tokens(self, text: str) -&gt; int:\n        \"\"\"\n        Count tokens in text.\n\n        Args:\n            text: Text to count tokens for\n\n        Returns:\n            Number of tokens\n        \"\"\"\n        pass\n\n    def count_message_tokens(self, message: Message) -&gt; int:\n        \"\"\"\n        Count tokens in a message.\n\n        Args:\n            message: Message to count tokens for\n\n        Returns:\n            Number of tokens\n        \"\"\"\n        return self.count_tokens(message.content)\n\n    def count_messages_tokens(self, messages: list[Message]) -&gt; int:\n        \"\"\"\n        Count tokens in multiple messages.\n\n        Args:\n            messages: List of messages\n\n        Returns:\n            Total number of tokens\n        \"\"\"\n        total = 0\n        for message in messages:\n            total += self.count_message_tokens(message)\n            # Add overhead for message formatting (role, etc.)\n            total += 4  # Approximate overhead per message\n        return total\n</code></pre>"},{"location":"api/base/#bruno_llm.base.TokenCounter.count_tokens","title":"<code>count_tokens(text)</code>  <code>abstractmethod</code>","text":"<p>Count tokens in text.</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>str</code> <p>Text to count tokens for</p> required <p>Returns:</p> Type Description <code>int</code> <p>Number of tokens</p> Source code in <code>bruno_llm/base/token_counter.py</code> <pre><code>@abstractmethod\ndef count_tokens(self, text: str) -&gt; int:\n    \"\"\"\n    Count tokens in text.\n\n    Args:\n        text: Text to count tokens for\n\n    Returns:\n        Number of tokens\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/base/#bruno_llm.base.TokenCounter.count_message_tokens","title":"<code>count_message_tokens(message)</code>","text":"<p>Count tokens in a message.</p> <p>Parameters:</p> Name Type Description Default <code>message</code> <code>Message</code> <p>Message to count tokens for</p> required <p>Returns:</p> Type Description <code>int</code> <p>Number of tokens</p> Source code in <code>bruno_llm/base/token_counter.py</code> <pre><code>def count_message_tokens(self, message: Message) -&gt; int:\n    \"\"\"\n    Count tokens in a message.\n\n    Args:\n        message: Message to count tokens for\n\n    Returns:\n        Number of tokens\n    \"\"\"\n    return self.count_tokens(message.content)\n</code></pre>"},{"location":"api/base/#bruno_llm.base.TokenCounter.count_messages_tokens","title":"<code>count_messages_tokens(messages)</code>","text":"<p>Count tokens in multiple messages.</p> <p>Parameters:</p> Name Type Description Default <code>messages</code> <code>list[Message]</code> <p>List of messages</p> required <p>Returns:</p> Type Description <code>int</code> <p>Total number of tokens</p> Source code in <code>bruno_llm/base/token_counter.py</code> <pre><code>def count_messages_tokens(self, messages: list[Message]) -&gt; int:\n    \"\"\"\n    Count tokens in multiple messages.\n\n    Args:\n        messages: List of messages\n\n    Returns:\n        Total number of tokens\n    \"\"\"\n    total = 0\n    for message in messages:\n        total += self.count_message_tokens(message)\n        # Add overhead for message formatting (role, etc.)\n        total += 4  # Approximate overhead per message\n    return total\n</code></pre>"},{"location":"api/base/#bruno_llm.base.retry_async","title":"<code>retry_async(func, *args, config=None, **kwargs)</code>  <code>async</code>","text":"<p>Execute async function with retry logic.</p> <p>Parameters:</p> Name Type Description Default <code>func</code> <code>Callable[..., T]</code> <p>Async function to execute</p> required <code>*args</code> <code>Any</code> <p>Positional arguments for func</p> <code>()</code> <code>config</code> <code>Optional[RetryConfig]</code> <p>Retry configuration (uses defaults if None)</p> <code>None</code> <code>**kwargs</code> <code>Any</code> <p>Keyword arguments for func</p> <code>{}</code> <p>Returns:</p> Type Description <code>T</code> <p>Result from func</p> <p>Raises:</p> Type Description <code>Exception</code> <p>Last exception if all retries fail</p> Example <p>async def api_call(): ...     # May fail transiently ...     return await external_api() result = await retry_async(api_call, config=RetryConfig(max_retries=5))</p> Source code in <code>bruno_llm/base/retry.py</code> <pre><code>async def retry_async(\n    func: Callable[..., T],\n    *args: Any,\n    config: Optional[RetryConfig] = None,\n    **kwargs: Any,\n) -&gt; T:\n    \"\"\"\n    Execute async function with retry logic.\n\n    Args:\n        func: Async function to execute\n        *args: Positional arguments for func\n        config: Retry configuration (uses defaults if None)\n        **kwargs: Keyword arguments for func\n\n    Returns:\n        Result from func\n\n    Raises:\n        Exception: Last exception if all retries fail\n\n    Example:\n        &gt;&gt;&gt; async def api_call():\n        ...     # May fail transiently\n        ...     return await external_api()\n        &gt;&gt;&gt; result = await retry_async(api_call, config=RetryConfig(max_retries=5))\n    \"\"\"\n    if config is None:\n        config = RetryConfig()\n\n    last_exception = None\n\n    for attempt in range(config.max_retries + 1):\n        try:\n            return await func(*args, **kwargs)\n        except Exception as e:\n            last_exception = e\n\n            # Check if we should retry\n            if not config.should_retry(e, attempt):\n                raise\n\n            # Calculate and wait\n            if attempt &lt; config.max_retries:\n                delay = config.calculate_delay(attempt)\n\n                # Special handling for rate limit with retry_after\n                if isinstance(e, RateLimitError) and e.retry_after:\n                    delay = max(delay, e.retry_after)\n\n                await asyncio.sleep(delay)\n\n    # Should not reach here, but for safety\n    if last_exception:\n        raise last_exception\n    raise LLMError(\"Retry loop exited unexpectedly\")\n</code></pre>"},{"location":"api/base/#bruno_llm.base.stream_with_timeout","title":"<code>stream_with_timeout(stream, timeout=30.0)</code>  <code>async</code>","text":"<p>Wrap a stream with timeout protection.</p> <p>Raises TimeoutError if no chunk is received within timeout.</p> <p>Parameters:</p> Name Type Description Default <code>stream</code> <code>AsyncIterator[str]</code> <p>Input stream to wrap</p> required <code>timeout</code> <code>float</code> <p>Timeout in seconds for each chunk</p> <code>30.0</code> <p>Yields:</p> Type Description <code>AsyncIterator[str]</code> <p>Stream chunks</p> <p>Raises:</p> Type Description <code>TimeoutError</code> <p>If timeout is exceeded</p> Example <p>async for chunk in stream_with_timeout(stream, timeout=10.0): ...     print(chunk)</p> Source code in <code>bruno_llm/base/streaming.py</code> <pre><code>async def stream_with_timeout(\n    stream: AsyncIterator[str],\n    timeout: float = 30.0,\n) -&gt; AsyncIterator[str]:\n    \"\"\"\n    Wrap a stream with timeout protection.\n\n    Raises TimeoutError if no chunk is received within timeout.\n\n    Args:\n        stream: Input stream to wrap\n        timeout: Timeout in seconds for each chunk\n\n    Yields:\n        Stream chunks\n\n    Raises:\n        TimeoutError: If timeout is exceeded\n\n    Example:\n        &gt;&gt;&gt; async for chunk in stream_with_timeout(stream, timeout=10.0):\n        ...     print(chunk)\n    \"\"\"\n    async for chunk in stream:\n        try:\n            yield await asyncio.wait_for(_async_identity(chunk), timeout=timeout)\n        except asyncio.TimeoutError as e:\n            raise TimeoutError(f\"No chunk received within {timeout} seconds\") from e\n</code></pre>"},{"location":"api/base/#bruno_llm.base.create_token_counter","title":"<code>create_token_counter(provider='simple', model=None)</code>","text":"<p>Factory function to create appropriate token counter.</p> <p>Parameters:</p> Name Type Description Default <code>provider</code> <code>str</code> <p>Provider name (\"simple\", \"openai\", \"tiktoken\")</p> <code>'simple'</code> <code>model</code> <code>Optional[str]</code> <p>Optional model name for provider-specific counting</p> <code>None</code> <p>Returns:</p> Type Description <code>TokenCounter</code> <p>TokenCounter instance</p> Example <p>counter = create_token_counter(\"openai\", model=\"gpt-4\") tokens = counter.count_tokens(\"Hello!\")</p> Source code in <code>bruno_llm/base/token_counter.py</code> <pre><code>def create_token_counter(\n    provider: str = \"simple\",\n    model: Optional[str] = None,\n) -&gt; TokenCounter:\n    \"\"\"\n    Factory function to create appropriate token counter.\n\n    Args:\n        provider: Provider name (\"simple\", \"openai\", \"tiktoken\")\n        model: Optional model name for provider-specific counting\n\n    Returns:\n        TokenCounter instance\n\n    Example:\n        &gt;&gt;&gt; counter = create_token_counter(\"openai\", model=\"gpt-4\")\n        &gt;&gt;&gt; tokens = counter.count_tokens(\"Hello!\")\n    \"\"\"\n    if provider in (\"openai\", \"tiktoken\"):\n        if model:\n            return TikTokenCounter(model=model)\n        return TikTokenCounter()\n\n    return SimpleTokenCounter()\n</code></pre>"},{"location":"api/exceptions/","title":"Exceptions","text":""},{"location":"api/exceptions/#bruno_llm.exceptions","title":"<code>bruno_llm.exceptions</code>","text":"<p>Exception hierarchy for bruno-llm.</p> <p>Defines custom exceptions for LLM provider errors, enabling consistent error handling across all providers.</p>"},{"location":"api/exceptions/#bruno_llm.exceptions.LLMError","title":"<code>LLMError</code>","text":"<p>               Bases: <code>Exception</code></p> <p>Base exception for all LLM-related errors.</p> <p>All custom exceptions in bruno-llm inherit from this class.</p> <p>Parameters:</p> Name Type Description Default <code>message</code> <code>str</code> <p>Error message</p> required <code>provider</code> <code>Optional[str]</code> <p>Name of the provider that raised the error</p> <code>None</code> <code>original_error</code> <code>Optional[Exception]</code> <p>Original exception if available</p> <code>None</code> Source code in <code>bruno_llm/exceptions.py</code> <pre><code>class LLMError(Exception):\n    \"\"\"\n    Base exception for all LLM-related errors.\n\n    All custom exceptions in bruno-llm inherit from this class.\n\n    Args:\n        message: Error message\n        provider: Name of the provider that raised the error\n        original_error: Original exception if available\n    \"\"\"\n\n    def __init__(\n        self,\n        message: str,\n        provider: Optional[str] = None,\n        original_error: Optional[Exception] = None,\n    ):\n        self.message = message\n        self.provider = provider\n        self.original_error = original_error\n        super().__init__(self.format_message())\n\n    def format_message(self) -&gt; str:\n        \"\"\"Format the error message with provider context.\"\"\"\n        parts = []\n        if self.provider:\n            parts.append(f\"[{self.provider}]\")\n        parts.append(self.message)\n        if self.original_error:\n            parts.append(\n                f\"(caused by: {type(self.original_error).__name__}: {self.original_error})\"\n            )\n        return \" \".join(parts)\n</code></pre>"},{"location":"api/exceptions/#bruno_llm.exceptions.LLMError.format_message","title":"<code>format_message()</code>","text":"<p>Format the error message with provider context.</p> Source code in <code>bruno_llm/exceptions.py</code> <pre><code>def format_message(self) -&gt; str:\n    \"\"\"Format the error message with provider context.\"\"\"\n    parts = []\n    if self.provider:\n        parts.append(f\"[{self.provider}]\")\n    parts.append(self.message)\n    if self.original_error:\n        parts.append(\n            f\"(caused by: {type(self.original_error).__name__}: {self.original_error})\"\n        )\n    return \" \".join(parts)\n</code></pre>"},{"location":"api/exceptions/#bruno_llm.exceptions.AuthenticationError","title":"<code>AuthenticationError</code>","text":"<p>               Bases: <code>LLMError</code></p> <p>Raised when authentication with LLM provider fails.</p> <p>Common causes: - Invalid API key - Expired API key - Missing API key - Invalid organization ID</p> Example <p>raise AuthenticationError(\"Invalid API key\", provider=\"openai\")</p> Source code in <code>bruno_llm/exceptions.py</code> <pre><code>class AuthenticationError(LLMError):\n    \"\"\"\n    Raised when authentication with LLM provider fails.\n\n    Common causes:\n    - Invalid API key\n    - Expired API key\n    - Missing API key\n    - Invalid organization ID\n\n    Example:\n        &gt;&gt;&gt; raise AuthenticationError(\"Invalid API key\", provider=\"openai\")\n    \"\"\"\n\n    pass\n</code></pre>"},{"location":"api/exceptions/#bruno_llm.exceptions.RateLimitError","title":"<code>RateLimitError</code>","text":"<p>               Bases: <code>LLMError</code></p> <p>Raised when API rate limits are exceeded.</p> <p>Attributes:</p> Name Type Description <code>retry_after</code> <p>Seconds to wait before retrying (if provided)</p> Example <p>raise RateLimitError(\"Rate limit exceeded\", provider=\"openai\", retry_after=60)</p> Source code in <code>bruno_llm/exceptions.py</code> <pre><code>class RateLimitError(LLMError):\n    \"\"\"\n    Raised when API rate limits are exceeded.\n\n    Attributes:\n        retry_after: Seconds to wait before retrying (if provided)\n\n    Example:\n        &gt;&gt;&gt; raise RateLimitError(\"Rate limit exceeded\", provider=\"openai\", retry_after=60)\n    \"\"\"\n\n    def __init__(\n        self,\n        message: str,\n        provider: Optional[str] = None,\n        original_error: Optional[Exception] = None,\n        retry_after: Optional[int] = None,\n    ):\n        super().__init__(message, provider, original_error)\n        self.retry_after = retry_after\n</code></pre>"},{"location":"api/exceptions/#bruno_llm.exceptions.ModelNotFoundError","title":"<code>ModelNotFoundError</code>","text":"<p>               Bases: <code>LLMError</code></p> <p>Raised when requested model is not found or not available.</p> <p>Common causes: - Model name misspelled - Model not available in region - Model access not granted - Model has been deprecated</p> Example <p>raise ModelNotFoundError(\"Model 'gpt-5' not found\", provider=\"openai\")</p> Source code in <code>bruno_llm/exceptions.py</code> <pre><code>class ModelNotFoundError(LLMError):\n    \"\"\"\n    Raised when requested model is not found or not available.\n\n    Common causes:\n    - Model name misspelled\n    - Model not available in region\n    - Model access not granted\n    - Model has been deprecated\n\n    Example:\n        &gt;&gt;&gt; raise ModelNotFoundError(\"Model 'gpt-5' not found\", provider=\"openai\")\n    \"\"\"\n\n    pass\n</code></pre>"},{"location":"api/exceptions/#bruno_llm.exceptions.ContextLengthExceededError","title":"<code>ContextLengthExceededError</code>","text":"<p>               Bases: <code>LLMError</code></p> <p>Raised when input exceeds model's context length.</p> <p>Attributes:</p> Name Type Description <code>max_tokens</code> <p>Maximum tokens allowed</p> <code>actual_tokens</code> <p>Actual token count in request</p> Example <p>raise ContextLengthExceededError( ...     \"Context length exceeded\", ...     provider=\"openai\", ...     max_tokens=8192, ...     actual_tokens=10000 ... )</p> Source code in <code>bruno_llm/exceptions.py</code> <pre><code>class ContextLengthExceededError(LLMError):\n    \"\"\"\n    Raised when input exceeds model's context length.\n\n    Attributes:\n        max_tokens: Maximum tokens allowed\n        actual_tokens: Actual token count in request\n\n    Example:\n        &gt;&gt;&gt; raise ContextLengthExceededError(\n        ...     \"Context length exceeded\",\n        ...     provider=\"openai\",\n        ...     max_tokens=8192,\n        ...     actual_tokens=10000\n        ... )\n    \"\"\"\n\n    def __init__(\n        self,\n        message: str,\n        provider: Optional[str] = None,\n        original_error: Optional[Exception] = None,\n        max_tokens: Optional[int] = None,\n        actual_tokens: Optional[int] = None,\n    ):\n        super().__init__(message, provider, original_error)\n        self.max_tokens = max_tokens\n        self.actual_tokens = actual_tokens\n</code></pre>"},{"location":"api/exceptions/#bruno_llm.exceptions.StreamError","title":"<code>StreamError</code>","text":"<p>               Bases: <code>LLMError</code></p> <p>Raised when streaming response encounters an error.</p> <p>Common causes: - Network connection lost - Server-side error during streaming - Invalid chunk format - Stream interrupted</p> Example <p>raise StreamError(\"Stream connection lost\", provider=\"openai\")</p> Source code in <code>bruno_llm/exceptions.py</code> <pre><code>class StreamError(LLMError):\n    \"\"\"\n    Raised when streaming response encounters an error.\n\n    Common causes:\n    - Network connection lost\n    - Server-side error during streaming\n    - Invalid chunk format\n    - Stream interrupted\n\n    Example:\n        &gt;&gt;&gt; raise StreamError(\"Stream connection lost\", provider=\"openai\")\n    \"\"\"\n\n    pass\n</code></pre>"},{"location":"api/exceptions/#bruno_llm.exceptions.ConfigurationError","title":"<code>ConfigurationError</code>","text":"<p>               Bases: <code>LLMError</code></p> <p>Raised when provider configuration is invalid.</p> <p>Common causes: - Missing required configuration - Invalid configuration values - Conflicting configuration options</p> Example <p>raise ConfigurationError(\"Missing base_url\", provider=\"ollama\")</p> Source code in <code>bruno_llm/exceptions.py</code> <pre><code>class ConfigurationError(LLMError):\n    \"\"\"\n    Raised when provider configuration is invalid.\n\n    Common causes:\n    - Missing required configuration\n    - Invalid configuration values\n    - Conflicting configuration options\n\n    Example:\n        &gt;&gt;&gt; raise ConfigurationError(\"Missing base_url\", provider=\"ollama\")\n    \"\"\"\n\n    pass\n</code></pre>"},{"location":"api/exceptions/#bruno_llm.exceptions.TimeoutError","title":"<code>TimeoutError</code>","text":"<p>               Bases: <code>LLMError</code></p> <p>Raised when request times out.</p> <p>Attributes:</p> Name Type Description <code>timeout</code> <p>Timeout value in seconds</p> Example <p>raise TimeoutError(\"Request timed out after 30s\", provider=\"openai\", timeout=30)</p> Source code in <code>bruno_llm/exceptions.py</code> <pre><code>class TimeoutError(LLMError):\n    \"\"\"\n    Raised when request times out.\n\n    Attributes:\n        timeout: Timeout value in seconds\n\n    Example:\n        &gt;&gt;&gt; raise TimeoutError(\"Request timed out after 30s\", provider=\"openai\", timeout=30)\n    \"\"\"\n\n    def __init__(\n        self,\n        message: str,\n        provider: Optional[str] = None,\n        original_error: Optional[Exception] = None,\n        timeout: Optional[float] = None,\n    ):\n        super().__init__(message, provider, original_error)\n        self.timeout = timeout\n</code></pre>"},{"location":"api/exceptions/#bruno_llm.exceptions.InvalidResponseError","title":"<code>InvalidResponseError</code>","text":"<p>               Bases: <code>LLMError</code></p> <p>Raised when provider returns invalid or unexpected response.</p> <p>Common causes: - Malformed JSON response - Missing required fields - Unexpected response structure</p> Example <p>raise InvalidResponseError(\"Missing 'content' field\", provider=\"openai\")</p> Source code in <code>bruno_llm/exceptions.py</code> <pre><code>class InvalidResponseError(LLMError):\n    \"\"\"\n    Raised when provider returns invalid or unexpected response.\n\n    Common causes:\n    - Malformed JSON response\n    - Missing required fields\n    - Unexpected response structure\n\n    Example:\n        &gt;&gt;&gt; raise InvalidResponseError(\"Missing 'content' field\", provider=\"openai\")\n    \"\"\"\n\n    pass\n</code></pre>"},{"location":"api/exceptions/#bruno_llm.exceptions.ProviderNotFoundError","title":"<code>ProviderNotFoundError</code>","text":"<p>               Bases: <code>LLMError</code></p> <p>Raised when requested provider is not registered.</p> Example <p>raise ProviderNotFoundError(\"Provider 'unknown' not found\")</p> Source code in <code>bruno_llm/exceptions.py</code> <pre><code>class ProviderNotFoundError(LLMError):\n    \"\"\"\n    Raised when requested provider is not registered.\n\n    Example:\n        &gt;&gt;&gt; raise ProviderNotFoundError(\"Provider 'unknown' not found\")\n    \"\"\"\n\n    pass\n</code></pre>"},{"location":"api/factory/","title":"Factory","text":""},{"location":"api/factory/#bruno_llm.factory.LLMFactory","title":"<code>bruno_llm.factory.LLMFactory</code>","text":"<p>Factory for creating LLM provider instances.</p> <p>Provides multiple ways to instantiate providers: - Direct creation with configuration - Environment-based configuration - Fallback chain for resilience</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; # Direct creation\n&gt;&gt;&gt; llm = LLMFactory.create(\n...     provider=\"ollama\",\n...     config={\"model\": \"llama2\"}\n... )\n</code></pre> <pre><code>&gt;&gt;&gt; # From environment variables\n&gt;&gt;&gt; llm = LLMFactory.create_from_env(provider=\"openai\")\n</code></pre> <pre><code>&gt;&gt;&gt; # With fallback\n&gt;&gt;&gt; llm = LLMFactory.create_with_fallback(\n...     providers=[\"openai\", \"ollama\"],\n...     configs=[openai_config, ollama_config]\n... )\n</code></pre> Source code in <code>bruno_llm/factory.py</code> <pre><code>class LLMFactory:\n    \"\"\"\n    Factory for creating LLM provider instances.\n\n    Provides multiple ways to instantiate providers:\n    - Direct creation with configuration\n    - Environment-based configuration\n    - Fallback chain for resilience\n\n    Examples:\n        &gt;&gt;&gt; # Direct creation\n        &gt;&gt;&gt; llm = LLMFactory.create(\n        ...     provider=\"ollama\",\n        ...     config={\"model\": \"llama2\"}\n        ... )\n\n        &gt;&gt;&gt; # From environment variables\n        &gt;&gt;&gt; llm = LLMFactory.create_from_env(provider=\"openai\")\n\n        &gt;&gt;&gt; # With fallback\n        &gt;&gt;&gt; llm = LLMFactory.create_with_fallback(\n        ...     providers=[\"openai\", \"ollama\"],\n        ...     configs=[openai_config, ollama_config]\n        ... )\n    \"\"\"\n\n    _providers: dict[str, Callable[..., LLMInterface]] = {}\n\n    @classmethod\n    def register(cls, name: str, provider_class: Callable[..., LLMInterface]) -&gt; None:\n        \"\"\"\n        Register a provider class with the factory.\n\n        Args:\n            name: Provider name (e.g., \"ollama\", \"openai\")\n            provider_class: Provider class or factory function\n        \"\"\"\n        cls._providers[name.lower()] = provider_class\n\n    @classmethod\n    def create(\n        cls, provider: str, config: Optional[dict[str, Any]] = None, **kwargs: Any\n    ) -&gt; LLMInterface:\n        \"\"\"\n        Create a provider instance.\n\n        Args:\n            provider: Provider name (\"ollama\", \"openai\", etc.)\n            config: Configuration dictionary (optional)\n            **kwargs: Additional arguments passed to provider\n\n        Returns:\n            Configured LLMInterface instance\n\n        Raises:\n            ConfigurationError: If provider not found or config invalid\n\n        Examples:\n            &gt;&gt;&gt; llm = LLMFactory.create(\"ollama\", {\"model\": \"llama2\"})\n            &gt;&gt;&gt; llm = LLMFactory.create(\"openai\", api_key=\"sk-...\", model=\"gpt-4\")\n        \"\"\"\n        provider_lower = provider.lower()\n\n        if provider_lower not in cls._providers:\n            available = \", \".join(cls._providers.keys())\n            raise ConfigurationError(\n                f\"Provider '{provider}' not found. Available providers: {available}\"\n            )\n\n        provider_class = cls._providers[provider_lower]\n\n        # Merge config dict and kwargs\n        final_config = config.copy() if config else {}\n        final_config.update(kwargs)\n\n        try:\n            return provider_class(**final_config)\n        except TypeError as e:\n            raise ConfigurationError(f\"Invalid configuration for provider '{provider}': {e}\") from e\n\n    @classmethod\n    def create_from_env(cls, provider: str, prefix: Optional[str] = None) -&gt; LLMInterface:\n        \"\"\"\n        Create provider from environment variables.\n\n        Environment variables are read using the pattern:\n        {PREFIX}_{PROVIDER}_{SETTING}\n\n        Args:\n            provider: Provider name\n            prefix: Environment variable prefix (default: \"BRUNO_LLM\")\n\n        Returns:\n            Configured LLMInterface instance\n\n        Raises:\n            ConfigurationError: If required env vars missing\n\n        Examples:\n            &gt;&gt;&gt; # With BRUNO_LLM_OPENAI_API_KEY=sk-...\n            &gt;&gt;&gt; # and BRUNO_LLM_OPENAI_MODEL=gpt-4\n            &gt;&gt;&gt; llm = LLMFactory.create_from_env(\"openai\")\n\n            &gt;&gt;&gt; # Custom prefix\n            &gt;&gt;&gt; # MY_APP_OLLAMA_MODEL=llama2\n            &gt;&gt;&gt; llm = LLMFactory.create_from_env(\"ollama\", prefix=\"MY_APP\")\n        \"\"\"\n        prefix = prefix or \"BRUNO_LLM\"\n        provider_upper = provider.upper()\n        env_prefix = f\"{prefix}_{provider_upper}_\"\n\n        # Collect all matching environment variables\n        config: dict[str, Any] = {}\n        for key, value in os.environ.items():\n            if key.startswith(env_prefix):\n                # Remove prefix and convert to lowercase\n                setting_name = key[len(env_prefix) :].lower()\n                config[setting_name] = value\n\n        if not config:\n            raise ConfigurationError(\n                f\"No environment variables found for provider '{provider}'. \"\n                f\"Expected variables starting with {env_prefix}\"\n            )\n\n        return cls.create(provider, config)\n\n    @classmethod\n    async def create_with_fallback(\n        cls, providers: list[str], configs: Optional[list[dict[str, Any]]] = None\n    ) -&gt; LLMInterface:\n        \"\"\"\n        Create provider with fallback chain.\n\n        Tries each provider in order until one connects successfully.\n\n        Args:\n            providers: List of provider names in priority order\n            configs: Optional list of configurations (must match providers length)\n\n        Returns:\n            First successfully connected LLMInterface instance\n\n        Raises:\n            LLMError: If all providers fail to connect\n\n        Examples:\n            &gt;&gt;&gt; llm = await LLMFactory.create_with_fallback(\n            ...     providers=[\"openai\", \"ollama\"],\n            ...     configs=[\n            ...         {\"api_key\": \"sk-...\", \"model\": \"gpt-4\"},\n            ...         {\"model\": \"llama2\"}\n            ...     ]\n            ... )\n        \"\"\"\n        if not providers:\n            raise ConfigurationError(\"No providers specified for fallback\")\n\n        if configs and len(configs) != len(providers):\n            raise ConfigurationError(\n                f\"configs length ({len(configs)}) must match providers length ({len(providers)})\"\n            )\n\n        errors = []\n\n        for i, provider_name in enumerate(providers):\n            try:\n                config = configs[i] if configs else {}\n                provider = cls.create(provider_name, config)\n\n                # Test connection\n                if await provider.check_connection():\n                    return provider\n                else:\n                    errors.append(f\"{provider_name}: Connection check failed\")\n\n            except Exception as e:\n                errors.append(f\"{provider_name}: {e}\")\n                continue\n\n        # All providers failed\n        error_details = \"; \".join(errors)\n        raise LLMError(\n            f\"All providers failed to connect. Tried: {', '.join(providers)}. \"\n            f\"Errors: {error_details}\"\n        )\n\n    @classmethod\n    def list_providers(cls) -&gt; list[str]:\n        \"\"\"\n        List all registered providers.\n\n        Returns:\n            List of provider names\n        \"\"\"\n        return sorted(cls._providers.keys())\n\n    @classmethod\n    def is_registered(cls, provider: str) -&gt; bool:\n        \"\"\"\n        Check if a provider is registered.\n\n        Args:\n            provider: Provider name\n\n        Returns:\n            True if provider is registered\n        \"\"\"\n        return provider.lower() in cls._providers\n</code></pre>"},{"location":"api/factory/#bruno_llm.factory.LLMFactory.register","title":"<code>register(name, provider_class)</code>  <code>classmethod</code>","text":"<p>Register a provider class with the factory.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Provider name (e.g., \"ollama\", \"openai\")</p> required <code>provider_class</code> <code>Callable[..., LLMInterface]</code> <p>Provider class or factory function</p> required Source code in <code>bruno_llm/factory.py</code> <pre><code>@classmethod\ndef register(cls, name: str, provider_class: Callable[..., LLMInterface]) -&gt; None:\n    \"\"\"\n    Register a provider class with the factory.\n\n    Args:\n        name: Provider name (e.g., \"ollama\", \"openai\")\n        provider_class: Provider class or factory function\n    \"\"\"\n    cls._providers[name.lower()] = provider_class\n</code></pre>"},{"location":"api/factory/#bruno_llm.factory.LLMFactory.create","title":"<code>create(provider, config=None, **kwargs)</code>  <code>classmethod</code>","text":"<p>Create a provider instance.</p> <p>Parameters:</p> Name Type Description Default <code>provider</code> <code>str</code> <p>Provider name (\"ollama\", \"openai\", etc.)</p> required <code>config</code> <code>Optional[dict[str, Any]]</code> <p>Configuration dictionary (optional)</p> <code>None</code> <code>**kwargs</code> <code>Any</code> <p>Additional arguments passed to provider</p> <code>{}</code> <p>Returns:</p> Type Description <code>LLMInterface</code> <p>Configured LLMInterface instance</p> <p>Raises:</p> Type Description <code>ConfigurationError</code> <p>If provider not found or config invalid</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; llm = LLMFactory.create(\"ollama\", {\"model\": \"llama2\"})\n&gt;&gt;&gt; llm = LLMFactory.create(\"openai\", api_key=\"sk-...\", model=\"gpt-4\")\n</code></pre> Source code in <code>bruno_llm/factory.py</code> <pre><code>@classmethod\ndef create(\n    cls, provider: str, config: Optional[dict[str, Any]] = None, **kwargs: Any\n) -&gt; LLMInterface:\n    \"\"\"\n    Create a provider instance.\n\n    Args:\n        provider: Provider name (\"ollama\", \"openai\", etc.)\n        config: Configuration dictionary (optional)\n        **kwargs: Additional arguments passed to provider\n\n    Returns:\n        Configured LLMInterface instance\n\n    Raises:\n        ConfigurationError: If provider not found or config invalid\n\n    Examples:\n        &gt;&gt;&gt; llm = LLMFactory.create(\"ollama\", {\"model\": \"llama2\"})\n        &gt;&gt;&gt; llm = LLMFactory.create(\"openai\", api_key=\"sk-...\", model=\"gpt-4\")\n    \"\"\"\n    provider_lower = provider.lower()\n\n    if provider_lower not in cls._providers:\n        available = \", \".join(cls._providers.keys())\n        raise ConfigurationError(\n            f\"Provider '{provider}' not found. Available providers: {available}\"\n        )\n\n    provider_class = cls._providers[provider_lower]\n\n    # Merge config dict and kwargs\n    final_config = config.copy() if config else {}\n    final_config.update(kwargs)\n\n    try:\n        return provider_class(**final_config)\n    except TypeError as e:\n        raise ConfigurationError(f\"Invalid configuration for provider '{provider}': {e}\") from e\n</code></pre>"},{"location":"api/factory/#bruno_llm.factory.LLMFactory.create_from_env","title":"<code>create_from_env(provider, prefix=None)</code>  <code>classmethod</code>","text":"<p>Create provider from environment variables.</p> <p>Environment variables are read using the pattern: {PREFIX}{PROVIDER}{SETTING}</p> <p>Parameters:</p> Name Type Description Default <code>provider</code> <code>str</code> <p>Provider name</p> required <code>prefix</code> <code>Optional[str]</code> <p>Environment variable prefix (default: \"BRUNO_LLM\")</p> <code>None</code> <p>Returns:</p> Type Description <code>LLMInterface</code> <p>Configured LLMInterface instance</p> <p>Raises:</p> Type Description <code>ConfigurationError</code> <p>If required env vars missing</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; # With BRUNO_LLM_OPENAI_API_KEY=sk-...\n&gt;&gt;&gt; # and BRUNO_LLM_OPENAI_MODEL=gpt-4\n&gt;&gt;&gt; llm = LLMFactory.create_from_env(\"openai\")\n</code></pre> <pre><code>&gt;&gt;&gt; # Custom prefix\n&gt;&gt;&gt; # MY_APP_OLLAMA_MODEL=llama2\n&gt;&gt;&gt; llm = LLMFactory.create_from_env(\"ollama\", prefix=\"MY_APP\")\n</code></pre> Source code in <code>bruno_llm/factory.py</code> <pre><code>@classmethod\ndef create_from_env(cls, provider: str, prefix: Optional[str] = None) -&gt; LLMInterface:\n    \"\"\"\n    Create provider from environment variables.\n\n    Environment variables are read using the pattern:\n    {PREFIX}_{PROVIDER}_{SETTING}\n\n    Args:\n        provider: Provider name\n        prefix: Environment variable prefix (default: \"BRUNO_LLM\")\n\n    Returns:\n        Configured LLMInterface instance\n\n    Raises:\n        ConfigurationError: If required env vars missing\n\n    Examples:\n        &gt;&gt;&gt; # With BRUNO_LLM_OPENAI_API_KEY=sk-...\n        &gt;&gt;&gt; # and BRUNO_LLM_OPENAI_MODEL=gpt-4\n        &gt;&gt;&gt; llm = LLMFactory.create_from_env(\"openai\")\n\n        &gt;&gt;&gt; # Custom prefix\n        &gt;&gt;&gt; # MY_APP_OLLAMA_MODEL=llama2\n        &gt;&gt;&gt; llm = LLMFactory.create_from_env(\"ollama\", prefix=\"MY_APP\")\n    \"\"\"\n    prefix = prefix or \"BRUNO_LLM\"\n    provider_upper = provider.upper()\n    env_prefix = f\"{prefix}_{provider_upper}_\"\n\n    # Collect all matching environment variables\n    config: dict[str, Any] = {}\n    for key, value in os.environ.items():\n        if key.startswith(env_prefix):\n            # Remove prefix and convert to lowercase\n            setting_name = key[len(env_prefix) :].lower()\n            config[setting_name] = value\n\n    if not config:\n        raise ConfigurationError(\n            f\"No environment variables found for provider '{provider}'. \"\n            f\"Expected variables starting with {env_prefix}\"\n        )\n\n    return cls.create(provider, config)\n</code></pre>"},{"location":"api/factory/#bruno_llm.factory.LLMFactory.create_with_fallback","title":"<code>create_with_fallback(providers, configs=None)</code>  <code>async</code> <code>classmethod</code>","text":"<p>Create provider with fallback chain.</p> <p>Tries each provider in order until one connects successfully.</p> <p>Parameters:</p> Name Type Description Default <code>providers</code> <code>list[str]</code> <p>List of provider names in priority order</p> required <code>configs</code> <code>Optional[list[dict[str, Any]]]</code> <p>Optional list of configurations (must match providers length)</p> <code>None</code> <p>Returns:</p> Type Description <code>LLMInterface</code> <p>First successfully connected LLMInterface instance</p> <p>Raises:</p> Type Description <code>LLMError</code> <p>If all providers fail to connect</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; llm = await LLMFactory.create_with_fallback(\n...     providers=[\"openai\", \"ollama\"],\n...     configs=[\n...         {\"api_key\": \"sk-...\", \"model\": \"gpt-4\"},\n...         {\"model\": \"llama2\"}\n...     ]\n... )\n</code></pre> Source code in <code>bruno_llm/factory.py</code> <pre><code>@classmethod\nasync def create_with_fallback(\n    cls, providers: list[str], configs: Optional[list[dict[str, Any]]] = None\n) -&gt; LLMInterface:\n    \"\"\"\n    Create provider with fallback chain.\n\n    Tries each provider in order until one connects successfully.\n\n    Args:\n        providers: List of provider names in priority order\n        configs: Optional list of configurations (must match providers length)\n\n    Returns:\n        First successfully connected LLMInterface instance\n\n    Raises:\n        LLMError: If all providers fail to connect\n\n    Examples:\n        &gt;&gt;&gt; llm = await LLMFactory.create_with_fallback(\n        ...     providers=[\"openai\", \"ollama\"],\n        ...     configs=[\n        ...         {\"api_key\": \"sk-...\", \"model\": \"gpt-4\"},\n        ...         {\"model\": \"llama2\"}\n        ...     ]\n        ... )\n    \"\"\"\n    if not providers:\n        raise ConfigurationError(\"No providers specified for fallback\")\n\n    if configs and len(configs) != len(providers):\n        raise ConfigurationError(\n            f\"configs length ({len(configs)}) must match providers length ({len(providers)})\"\n        )\n\n    errors = []\n\n    for i, provider_name in enumerate(providers):\n        try:\n            config = configs[i] if configs else {}\n            provider = cls.create(provider_name, config)\n\n            # Test connection\n            if await provider.check_connection():\n                return provider\n            else:\n                errors.append(f\"{provider_name}: Connection check failed\")\n\n        except Exception as e:\n            errors.append(f\"{provider_name}: {e}\")\n            continue\n\n    # All providers failed\n    error_details = \"; \".join(errors)\n    raise LLMError(\n        f\"All providers failed to connect. Tried: {', '.join(providers)}. \"\n        f\"Errors: {error_details}\"\n    )\n</code></pre>"},{"location":"api/factory/#bruno_llm.factory.LLMFactory.list_providers","title":"<code>list_providers()</code>  <code>classmethod</code>","text":"<p>List all registered providers.</p> <p>Returns:</p> Type Description <code>list[str]</code> <p>List of provider names</p> Source code in <code>bruno_llm/factory.py</code> <pre><code>@classmethod\ndef list_providers(cls) -&gt; list[str]:\n    \"\"\"\n    List all registered providers.\n\n    Returns:\n        List of provider names\n    \"\"\"\n    return sorted(cls._providers.keys())\n</code></pre>"},{"location":"api/factory/#bruno_llm.factory.LLMFactory.is_registered","title":"<code>is_registered(provider)</code>  <code>classmethod</code>","text":"<p>Check if a provider is registered.</p> <p>Parameters:</p> Name Type Description Default <code>provider</code> <code>str</code> <p>Provider name</p> required <p>Returns:</p> Type Description <code>bool</code> <p>True if provider is registered</p> Source code in <code>bruno_llm/factory.py</code> <pre><code>@classmethod\ndef is_registered(cls, provider: str) -&gt; bool:\n    \"\"\"\n    Check if a provider is registered.\n\n    Args:\n        provider: Provider name\n\n    Returns:\n        True if provider is registered\n    \"\"\"\n    return provider.lower() in cls._providers\n</code></pre>"},{"location":"api/providers/","title":"Providers","text":""},{"location":"api/providers/#bruno_llm.providers.ollama.OllamaProvider","title":"<code>bruno_llm.providers.ollama.OllamaProvider</code>","text":"<p>               Bases: <code>BaseProvider</code>, <code>LLMInterface</code></p> <p>Ollama provider for local LLM inference.</p> <p>Ollama runs models locally without API keys. Requires Ollama to be installed and running on the specified base_url.</p> <p>Parameters:</p> Name Type Description Default <code>base_url</code> <code>str</code> <p>Ollama API endpoint (default: http://localhost:11434)</p> <code>'http://localhost:11434'</code> <code>model</code> <code>str</code> <p>Model name (default: llama2)</p> <code>'llama2'</code> <code>timeout</code> <code>float</code> <p>Request timeout in seconds (default: 30.0)</p> <code>30.0</code> <code>**kwargs</code> <code>Any</code> <p>Additional configuration parameters</p> <code>{}</code> <p>Examples:</p> <pre><code>&gt;&gt;&gt; provider = OllamaProvider(model=\"llama2\")\n&gt;&gt;&gt; response = await provider.generate([\n...     Message(role=MessageRole.USER, content=\"Hello\")\n... ])\n</code></pre> <pre><code>&gt;&gt;&gt; # Streaming\n&gt;&gt;&gt; async for chunk in provider.stream([\n...     Message(role=MessageRole.USER, content=\"Tell me a story\")\n... ]):\n...     print(chunk, end=\"\")\n</code></pre> See Also <ul> <li>https://ollama.ai/</li> <li>bruno-core LLMInterface documentation</li> </ul> Source code in <code>bruno_llm/providers/ollama/provider.py</code> <pre><code>class OllamaProvider(BaseProvider, LLMInterface):\n    \"\"\"\n    Ollama provider for local LLM inference.\n\n    Ollama runs models locally without API keys. Requires Ollama\n    to be installed and running on the specified base_url.\n\n    Args:\n        base_url: Ollama API endpoint (default: http://localhost:11434)\n        model: Model name (default: llama2)\n        timeout: Request timeout in seconds (default: 30.0)\n        **kwargs: Additional configuration parameters\n\n    Examples:\n        &gt;&gt;&gt; provider = OllamaProvider(model=\"llama2\")\n        &gt;&gt;&gt; response = await provider.generate([\n        ...     Message(role=MessageRole.USER, content=\"Hello\")\n        ... ])\n\n        &gt;&gt;&gt; # Streaming\n        &gt;&gt;&gt; async for chunk in provider.stream([\n        ...     Message(role=MessageRole.USER, content=\"Tell me a story\")\n        ... ]):\n        ...     print(chunk, end=\"\")\n\n    See Also:\n        - https://ollama.ai/\n        - bruno-core LLMInterface documentation\n    \"\"\"\n\n    def __init__(\n        self,\n        base_url: str = \"http://localhost:11434\",\n        model: str = \"llama2\",\n        timeout: float = 30.0,\n        **kwargs: Any,\n    ):\n        \"\"\"Initialize Ollama provider.\"\"\"\n        # Create config\n        config = OllamaConfig(base_url=base_url, model=model, timeout=timeout, **kwargs)\n\n        # Initialize base provider\n        super().__init__(\n            provider_name=\"ollama\",\n            max_retries=3,\n            timeout=timeout,\n        )\n\n        # Store config\n        self._config = config\n        self._model = config.model\n\n        # Create HTTP client\n        self._client = httpx.AsyncClient(\n            base_url=config.base_url,\n            timeout=httpx.Timeout(config.timeout),\n        )\n\n        # Token counter (simple estimation for local models)\n        self._token_counter = SimpleTokenCounter()\n\n    @property\n    def model(self) -&gt; str:\n        \"\"\"Get current model name.\"\"\"\n        return self._model\n\n    @property\n    def config(self) -&gt; OllamaConfig:\n        \"\"\"Get provider configuration.\"\"\"\n        return self._config\n\n    def _format_messages(self, messages: list[Message]) -&gt; list[dict[str, str]]:\n        \"\"\"Convert bruno-core messages to Ollama format.\"\"\"\n        return [{\"role\": msg.role.value, \"content\": msg.content} for msg in messages]\n\n    def _build_request(\n        self, messages: list[Message], stream: bool = False, **kwargs: Any\n    ) -&gt; dict[str, Any]:\n        \"\"\"Build Ollama API request.\"\"\"\n        request = {\n            \"model\": self._model,\n            \"messages\": self._format_messages(messages),\n            \"stream\": stream,\n        }\n\n        # Add optional parameters\n        options: dict[str, Any] = {}\n\n        if self._config.temperature is not None:\n            options[\"temperature\"] = self._config.temperature\n        if self._config.top_p is not None:\n            options[\"top_p\"] = self._config.top_p\n        if self._config.top_k is not None:\n            options[\"top_k\"] = self._config.top_k\n        if self._config.num_predict is not None:\n            options[\"num_predict\"] = self._config.num_predict\n        if self._config.stop is not None:\n            request[\"stop\"] = self._config.stop\n\n        # Override with kwargs\n        options.update(kwargs)\n\n        if options:\n            request[\"options\"] = options\n\n        return request\n\n    async def generate(\n        self,\n        messages: list[Message],\n        temperature: Optional[float] = None,\n        max_tokens: Optional[int] = None,\n        **kwargs: Any,\n    ) -&gt; str:\n        \"\"\"\n        Generate a complete response from Ollama.\n\n        Args:\n            messages: List of conversation messages\n            temperature: Sampling temperature (0.0 to 2.0)\n            max_tokens: Maximum number of tokens to generate\n            **kwargs: Additional generation parameters\n\n        Returns:\n            Generated text response\n\n        Raises:\n            ModelNotFoundError: If model doesn't exist\n            LLMTimeoutError: If request times out\n            LLMError: For other API errors\n        \"\"\"\n        try:\n            # Build parameters including explicit ones\n            generation_params = {}\n            if temperature is not None:\n                generation_params[\"temperature\"] = temperature\n            if max_tokens is not None:\n                generation_params[\"num_predict\"] = (\n                    max_tokens  # Ollama uses num_predict instead of max_tokens\n                )\n            generation_params.update(kwargs)\n\n            request = self._build_request(messages, stream=False, **generation_params)\n\n            response = await self._client.post(\n                \"/api/chat\",\n                json=request,\n            )\n            response.raise_for_status()\n\n            data = response.json()\n\n            # Extract response content\n            if \"message\" not in data:\n                raise InvalidResponseError(\"No 'message' in response\")\n\n            content = data[\"message\"].get(\"content\", \"\")\n            return content\n\n        except httpx.TimeoutException as e:\n            raise LLMTimeoutError(f\"Request timed out: {e}\") from e\n        except httpx.HTTPStatusError as e:\n            if e.response.status_code == 404:\n                raise ModelNotFoundError(\n                    f\"Model '{self._model}' not found. \"\n                    f\"Run 'ollama pull {self._model}' to download it.\"\n                ) from e\n            raise LLMError(f\"HTTP error: {e}\") from e\n        except httpx.RequestError as e:\n            raise LLMError(\n                f\"Failed to connect to Ollama at {self._config.base_url}. \"\n                f\"Make sure Ollama is running: {e}\"\n            ) from e\n        except (KeyError, json.JSONDecodeError) as e:\n            raise InvalidResponseError(f\"Invalid response format: {e}\") from e\n\n    async def stream(\n        self,\n        messages: list[Message],\n        temperature: Optional[float] = None,\n        max_tokens: Optional[int] = None,\n        **kwargs: Any,\n    ) -&gt; AsyncIterator[str]:\n        \"\"\"\n        Stream response tokens from Ollama.\n\n        Args:\n            messages: List of conversation messages\n            temperature: Sampling temperature (0.0 to 2.0)\n            max_tokens: Maximum number of tokens to generate\n            **kwargs: Additional generation parameters\n\n        Yields:\n            Response text chunks\n\n        Raises:\n            StreamError: If streaming fails\n        \"\"\"\n        try:\n            # Build parameters including explicit ones\n            generation_params = {}\n            if temperature is not None:\n                generation_params[\"temperature\"] = temperature\n            if max_tokens is not None:\n                generation_params[\"num_predict\"] = (\n                    max_tokens  # Ollama uses num_predict instead of max_tokens\n                )\n            generation_params.update(kwargs)\n\n            request = self._build_request(messages, stream=True, **generation_params)\n\n            async with self._client.stream(\n                \"POST\",\n                \"/api/chat\",\n                json=request,\n            ) as response:\n                response.raise_for_status()\n\n                async for line in response.aiter_lines():\n                    if not line.strip():\n                        continue\n\n                    try:\n                        data = json.loads(line)\n\n                        # Check if streaming is done\n                        if data.get(\"done\", False):\n                            break\n\n                        # Extract content chunk\n                        if \"message\" in data:\n                            content = data[\"message\"].get(\"content\", \"\")\n                            if content:\n                                yield content\n\n                    except json.JSONDecodeError:\n                        # Skip malformed lines\n                        continue\n\n        except httpx.HTTPStatusError as e:\n            if e.response.status_code == 404:\n                raise ModelNotFoundError(\n                    f\"Model '{self._model}' not found. \"\n                    f\"Run 'ollama pull {self._model}' to download it.\"\n                ) from e\n            raise StreamError(f\"Streaming failed: {e}\") from e\n        except httpx.RequestError as e:\n            raise StreamError(f\"Failed to connect to Ollama: {e}\") from e\n        except Exception as e:\n            raise StreamError(f\"Unexpected streaming error: {e}\") from e\n\n    async def list_models(self) -&gt; list[str]:\n        \"\"\"\n        List available models in Ollama.\n\n        Returns:\n            List of model names\n\n        Raises:\n            LLMError: If request fails\n        \"\"\"\n        try:\n            response = await self._client.get(\"/api/tags\")\n            response.raise_for_status()\n\n            data = response.json()\n            models = data.get(\"models\", [])\n\n            return [model[\"name\"] for model in models]\n\n        except httpx.RequestError as e:\n            raise LLMError(f\"Failed to list models: {e}\") from e\n        except (KeyError, json.JSONDecodeError) as e:\n            raise InvalidResponseError(f\"Invalid response format: {e}\") from e\n\n    async def check_connection(self) -&gt; bool:\n        \"\"\"\n        Check if Ollama is accessible.\n\n        Returns:\n            True if Ollama is running and accessible\n        \"\"\"\n        try:\n            response = await self._client.get(\"/api/tags\")\n            return response.status_code == 200\n        except Exception:\n            return False\n\n    def get_token_count(self, text: str) -&gt; int:\n        \"\"\"\n        Estimate token count for text.\n\n        Args:\n            text: Text to count tokens for\n\n        Returns:\n            Estimated token count\n        \"\"\"\n        return self._token_counter.count_tokens(text)\n\n    def get_model_info(self) -&gt; dict[str, Any]:\n        \"\"\"\n        Get current model information.\n\n        Returns:\n            Dictionary with model details\n        \"\"\"\n        return {\n            \"provider\": \"ollama\",\n            \"model\": self._model,\n            \"base_url\": self._config.base_url,\n            \"temperature\": self._config.temperature,\n            \"max_tokens\": self._config.num_predict,\n        }\n\n    async def close(self) -&gt; None:\n        \"\"\"Close HTTP client and cleanup resources.\"\"\"\n        await self._client.aclose()\n\n    async def __aenter__(self):\n        \"\"\"Context manager entry.\"\"\"\n        return self\n\n    async def __aexit__(self, exc_type, exc_val, exc_tb):\n        \"\"\"Context manager exit.\"\"\"\n        await self.close()\n</code></pre>"},{"location":"api/providers/#bruno_llm.providers.ollama.OllamaProvider.model","title":"<code>model</code>  <code>property</code>","text":"<p>Get current model name.</p>"},{"location":"api/providers/#bruno_llm.providers.ollama.OllamaProvider.config","title":"<code>config</code>  <code>property</code>","text":"<p>Get provider configuration.</p>"},{"location":"api/providers/#bruno_llm.providers.ollama.OllamaProvider.__init__","title":"<code>__init__(base_url='http://localhost:11434', model='llama2', timeout=30.0, **kwargs)</code>","text":"<p>Initialize Ollama provider.</p> Source code in <code>bruno_llm/providers/ollama/provider.py</code> <pre><code>def __init__(\n    self,\n    base_url: str = \"http://localhost:11434\",\n    model: str = \"llama2\",\n    timeout: float = 30.0,\n    **kwargs: Any,\n):\n    \"\"\"Initialize Ollama provider.\"\"\"\n    # Create config\n    config = OllamaConfig(base_url=base_url, model=model, timeout=timeout, **kwargs)\n\n    # Initialize base provider\n    super().__init__(\n        provider_name=\"ollama\",\n        max_retries=3,\n        timeout=timeout,\n    )\n\n    # Store config\n    self._config = config\n    self._model = config.model\n\n    # Create HTTP client\n    self._client = httpx.AsyncClient(\n        base_url=config.base_url,\n        timeout=httpx.Timeout(config.timeout),\n    )\n\n    # Token counter (simple estimation for local models)\n    self._token_counter = SimpleTokenCounter()\n</code></pre>"},{"location":"api/providers/#bruno_llm.providers.ollama.OllamaProvider.generate","title":"<code>generate(messages, temperature=None, max_tokens=None, **kwargs)</code>  <code>async</code>","text":"<p>Generate a complete response from Ollama.</p> <p>Parameters:</p> Name Type Description Default <code>messages</code> <code>list[Message]</code> <p>List of conversation messages</p> required <code>temperature</code> <code>Optional[float]</code> <p>Sampling temperature (0.0 to 2.0)</p> <code>None</code> <code>max_tokens</code> <code>Optional[int]</code> <p>Maximum number of tokens to generate</p> <code>None</code> <code>**kwargs</code> <code>Any</code> <p>Additional generation parameters</p> <code>{}</code> <p>Returns:</p> Type Description <code>str</code> <p>Generated text response</p> <p>Raises:</p> Type Description <code>ModelNotFoundError</code> <p>If model doesn't exist</p> <code>TimeoutError</code> <p>If request times out</p> <code>LLMError</code> <p>For other API errors</p> Source code in <code>bruno_llm/providers/ollama/provider.py</code> <pre><code>async def generate(\n    self,\n    messages: list[Message],\n    temperature: Optional[float] = None,\n    max_tokens: Optional[int] = None,\n    **kwargs: Any,\n) -&gt; str:\n    \"\"\"\n    Generate a complete response from Ollama.\n\n    Args:\n        messages: List of conversation messages\n        temperature: Sampling temperature (0.0 to 2.0)\n        max_tokens: Maximum number of tokens to generate\n        **kwargs: Additional generation parameters\n\n    Returns:\n        Generated text response\n\n    Raises:\n        ModelNotFoundError: If model doesn't exist\n        LLMTimeoutError: If request times out\n        LLMError: For other API errors\n    \"\"\"\n    try:\n        # Build parameters including explicit ones\n        generation_params = {}\n        if temperature is not None:\n            generation_params[\"temperature\"] = temperature\n        if max_tokens is not None:\n            generation_params[\"num_predict\"] = (\n                max_tokens  # Ollama uses num_predict instead of max_tokens\n            )\n        generation_params.update(kwargs)\n\n        request = self._build_request(messages, stream=False, **generation_params)\n\n        response = await self._client.post(\n            \"/api/chat\",\n            json=request,\n        )\n        response.raise_for_status()\n\n        data = response.json()\n\n        # Extract response content\n        if \"message\" not in data:\n            raise InvalidResponseError(\"No 'message' in response\")\n\n        content = data[\"message\"].get(\"content\", \"\")\n        return content\n\n    except httpx.TimeoutException as e:\n        raise LLMTimeoutError(f\"Request timed out: {e}\") from e\n    except httpx.HTTPStatusError as e:\n        if e.response.status_code == 404:\n            raise ModelNotFoundError(\n                f\"Model '{self._model}' not found. \"\n                f\"Run 'ollama pull {self._model}' to download it.\"\n            ) from e\n        raise LLMError(f\"HTTP error: {e}\") from e\n    except httpx.RequestError as e:\n        raise LLMError(\n            f\"Failed to connect to Ollama at {self._config.base_url}. \"\n            f\"Make sure Ollama is running: {e}\"\n        ) from e\n    except (KeyError, json.JSONDecodeError) as e:\n        raise InvalidResponseError(f\"Invalid response format: {e}\") from e\n</code></pre>"},{"location":"api/providers/#bruno_llm.providers.ollama.OllamaProvider.stream","title":"<code>stream(messages, temperature=None, max_tokens=None, **kwargs)</code>  <code>async</code>","text":"<p>Stream response tokens from Ollama.</p> <p>Parameters:</p> Name Type Description Default <code>messages</code> <code>list[Message]</code> <p>List of conversation messages</p> required <code>temperature</code> <code>Optional[float]</code> <p>Sampling temperature (0.0 to 2.0)</p> <code>None</code> <code>max_tokens</code> <code>Optional[int]</code> <p>Maximum number of tokens to generate</p> <code>None</code> <code>**kwargs</code> <code>Any</code> <p>Additional generation parameters</p> <code>{}</code> <p>Yields:</p> Type Description <code>AsyncIterator[str]</code> <p>Response text chunks</p> <p>Raises:</p> Type Description <code>StreamError</code> <p>If streaming fails</p> Source code in <code>bruno_llm/providers/ollama/provider.py</code> <pre><code>async def stream(\n    self,\n    messages: list[Message],\n    temperature: Optional[float] = None,\n    max_tokens: Optional[int] = None,\n    **kwargs: Any,\n) -&gt; AsyncIterator[str]:\n    \"\"\"\n    Stream response tokens from Ollama.\n\n    Args:\n        messages: List of conversation messages\n        temperature: Sampling temperature (0.0 to 2.0)\n        max_tokens: Maximum number of tokens to generate\n        **kwargs: Additional generation parameters\n\n    Yields:\n        Response text chunks\n\n    Raises:\n        StreamError: If streaming fails\n    \"\"\"\n    try:\n        # Build parameters including explicit ones\n        generation_params = {}\n        if temperature is not None:\n            generation_params[\"temperature\"] = temperature\n        if max_tokens is not None:\n            generation_params[\"num_predict\"] = (\n                max_tokens  # Ollama uses num_predict instead of max_tokens\n            )\n        generation_params.update(kwargs)\n\n        request = self._build_request(messages, stream=True, **generation_params)\n\n        async with self._client.stream(\n            \"POST\",\n            \"/api/chat\",\n            json=request,\n        ) as response:\n            response.raise_for_status()\n\n            async for line in response.aiter_lines():\n                if not line.strip():\n                    continue\n\n                try:\n                    data = json.loads(line)\n\n                    # Check if streaming is done\n                    if data.get(\"done\", False):\n                        break\n\n                    # Extract content chunk\n                    if \"message\" in data:\n                        content = data[\"message\"].get(\"content\", \"\")\n                        if content:\n                            yield content\n\n                except json.JSONDecodeError:\n                    # Skip malformed lines\n                    continue\n\n    except httpx.HTTPStatusError as e:\n        if e.response.status_code == 404:\n            raise ModelNotFoundError(\n                f\"Model '{self._model}' not found. \"\n                f\"Run 'ollama pull {self._model}' to download it.\"\n            ) from e\n        raise StreamError(f\"Streaming failed: {e}\") from e\n    except httpx.RequestError as e:\n        raise StreamError(f\"Failed to connect to Ollama: {e}\") from e\n    except Exception as e:\n        raise StreamError(f\"Unexpected streaming error: {e}\") from e\n</code></pre>"},{"location":"api/providers/#bruno_llm.providers.ollama.OllamaProvider.list_models","title":"<code>list_models()</code>  <code>async</code>","text":"<p>List available models in Ollama.</p> <p>Returns:</p> Type Description <code>list[str]</code> <p>List of model names</p> <p>Raises:</p> Type Description <code>LLMError</code> <p>If request fails</p> Source code in <code>bruno_llm/providers/ollama/provider.py</code> <pre><code>async def list_models(self) -&gt; list[str]:\n    \"\"\"\n    List available models in Ollama.\n\n    Returns:\n        List of model names\n\n    Raises:\n        LLMError: If request fails\n    \"\"\"\n    try:\n        response = await self._client.get(\"/api/tags\")\n        response.raise_for_status()\n\n        data = response.json()\n        models = data.get(\"models\", [])\n\n        return [model[\"name\"] for model in models]\n\n    except httpx.RequestError as e:\n        raise LLMError(f\"Failed to list models: {e}\") from e\n    except (KeyError, json.JSONDecodeError) as e:\n        raise InvalidResponseError(f\"Invalid response format: {e}\") from e\n</code></pre>"},{"location":"api/providers/#bruno_llm.providers.ollama.OllamaProvider.check_connection","title":"<code>check_connection()</code>  <code>async</code>","text":"<p>Check if Ollama is accessible.</p> <p>Returns:</p> Type Description <code>bool</code> <p>True if Ollama is running and accessible</p> Source code in <code>bruno_llm/providers/ollama/provider.py</code> <pre><code>async def check_connection(self) -&gt; bool:\n    \"\"\"\n    Check if Ollama is accessible.\n\n    Returns:\n        True if Ollama is running and accessible\n    \"\"\"\n    try:\n        response = await self._client.get(\"/api/tags\")\n        return response.status_code == 200\n    except Exception:\n        return False\n</code></pre>"},{"location":"api/providers/#bruno_llm.providers.ollama.OllamaProvider.get_token_count","title":"<code>get_token_count(text)</code>","text":"<p>Estimate token count for text.</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>str</code> <p>Text to count tokens for</p> required <p>Returns:</p> Type Description <code>int</code> <p>Estimated token count</p> Source code in <code>bruno_llm/providers/ollama/provider.py</code> <pre><code>def get_token_count(self, text: str) -&gt; int:\n    \"\"\"\n    Estimate token count for text.\n\n    Args:\n        text: Text to count tokens for\n\n    Returns:\n        Estimated token count\n    \"\"\"\n    return self._token_counter.count_tokens(text)\n</code></pre>"},{"location":"api/providers/#bruno_llm.providers.ollama.OllamaProvider.get_model_info","title":"<code>get_model_info()</code>","text":"<p>Get current model information.</p> <p>Returns:</p> Type Description <code>dict[str, Any]</code> <p>Dictionary with model details</p> Source code in <code>bruno_llm/providers/ollama/provider.py</code> <pre><code>def get_model_info(self) -&gt; dict[str, Any]:\n    \"\"\"\n    Get current model information.\n\n    Returns:\n        Dictionary with model details\n    \"\"\"\n    return {\n        \"provider\": \"ollama\",\n        \"model\": self._model,\n        \"base_url\": self._config.base_url,\n        \"temperature\": self._config.temperature,\n        \"max_tokens\": self._config.num_predict,\n    }\n</code></pre>"},{"location":"api/providers/#bruno_llm.providers.ollama.OllamaProvider.close","title":"<code>close()</code>  <code>async</code>","text":"<p>Close HTTP client and cleanup resources.</p> Source code in <code>bruno_llm/providers/ollama/provider.py</code> <pre><code>async def close(self) -&gt; None:\n    \"\"\"Close HTTP client and cleanup resources.\"\"\"\n    await self._client.aclose()\n</code></pre>"},{"location":"api/providers/#bruno_llm.providers.ollama.OllamaProvider.__aenter__","title":"<code>__aenter__()</code>  <code>async</code>","text":"<p>Context manager entry.</p> Source code in <code>bruno_llm/providers/ollama/provider.py</code> <pre><code>async def __aenter__(self):\n    \"\"\"Context manager entry.\"\"\"\n    return self\n</code></pre>"},{"location":"api/providers/#bruno_llm.providers.ollama.OllamaProvider.__aexit__","title":"<code>__aexit__(exc_type, exc_val, exc_tb)</code>  <code>async</code>","text":"<p>Context manager exit.</p> Source code in <code>bruno_llm/providers/ollama/provider.py</code> <pre><code>async def __aexit__(self, exc_type, exc_val, exc_tb):\n    \"\"\"Context manager exit.\"\"\"\n    await self.close()\n</code></pre>"},{"location":"api/providers/#bruno_llm.providers.openai.OpenAIProvider","title":"<code>bruno_llm.providers.openai.OpenAIProvider</code>","text":"<p>               Bases: <code>BaseProvider</code>, <code>LLMInterface</code></p> <p>OpenAI provider for GPT models.</p> <p>Provides access to OpenAI's GPT models (GPT-4, GPT-3.5-turbo, etc.) via the official OpenAI API. Requires an API key.</p> <p>Parameters:</p> Name Type Description Default <code>api_key</code> <code>str</code> <p>OpenAI API key (required)</p> required <code>model</code> <code>str</code> <p>Model name (default: gpt-4)</p> <code>'gpt-4'</code> <code>organization</code> <code>Optional[str]</code> <p>Organization ID (optional)</p> <code>None</code> <code>timeout</code> <code>float</code> <p>Request timeout in seconds (default: 30.0)</p> <code>30.0</code> <code>**kwargs</code> <code>Any</code> <p>Additional configuration parameters</p> <code>{}</code> <p>Examples:</p> <pre><code>&gt;&gt;&gt; provider = OpenAIProvider(api_key=\"sk-...\", model=\"gpt-4\")\n&gt;&gt;&gt; response = await provider.generate([\n...     Message(role=MessageRole.USER, content=\"Hello\")\n... ])\n</code></pre> <pre><code>&gt;&gt;&gt; # Streaming\n&gt;&gt;&gt; async for chunk in provider.stream([\n...     Message(role=MessageRole.USER, content=\"Tell me a story\")\n... ]):\n...     print(chunk, end=\"\")\n</code></pre> <pre><code>&gt;&gt;&gt; # With cost tracking\n&gt;&gt;&gt; provider = OpenAIProvider(api_key=\"sk-...\", track_cost=True)\n&gt;&gt;&gt; await provider.generate([...])\n&gt;&gt;&gt; report = provider.cost_tracker.get_usage_report()\n</code></pre> See Also <ul> <li>https://platform.openai.com/docs/api-reference</li> <li>bruno-core LLMInterface documentation</li> </ul> Source code in <code>bruno_llm/providers/openai/provider.py</code> <pre><code>class OpenAIProvider(BaseProvider, LLMInterface):\n    \"\"\"\n    OpenAI provider for GPT models.\n\n    Provides access to OpenAI's GPT models (GPT-4, GPT-3.5-turbo, etc.)\n    via the official OpenAI API. Requires an API key.\n\n    Args:\n        api_key: OpenAI API key (required)\n        model: Model name (default: gpt-4)\n        organization: Organization ID (optional)\n        timeout: Request timeout in seconds (default: 30.0)\n        **kwargs: Additional configuration parameters\n\n    Examples:\n        &gt;&gt;&gt; provider = OpenAIProvider(api_key=\"sk-...\", model=\"gpt-4\")\n        &gt;&gt;&gt; response = await provider.generate([\n        ...     Message(role=MessageRole.USER, content=\"Hello\")\n        ... ])\n\n        &gt;&gt;&gt; # Streaming\n        &gt;&gt;&gt; async for chunk in provider.stream([\n        ...     Message(role=MessageRole.USER, content=\"Tell me a story\")\n        ... ]):\n        ...     print(chunk, end=\"\")\n\n        &gt;&gt;&gt; # With cost tracking\n        &gt;&gt;&gt; provider = OpenAIProvider(api_key=\"sk-...\", track_cost=True)\n        &gt;&gt;&gt; await provider.generate([...])\n        &gt;&gt;&gt; report = provider.cost_tracker.get_usage_report()\n\n    See Also:\n        - https://platform.openai.com/docs/api-reference\n        - bruno-core LLMInterface documentation\n    \"\"\"\n\n    def __init__(\n        self,\n        api_key: str,\n        model: str = \"gpt-4\",\n        organization: Optional[str] = None,\n        timeout: float = 30.0,\n        track_cost: bool = True,\n        **kwargs: Any,\n    ):\n        \"\"\"Initialize OpenAI provider.\"\"\"\n        # Create config\n        config = OpenAIConfig(\n            api_key=api_key, model=model, organization=organization, timeout=timeout, **kwargs\n        )\n\n        # Initialize base provider\n        super().__init__(\n            provider_name=\"openai\",\n            max_retries=config.max_retries,\n            timeout=timeout,\n        )\n\n        # Store config\n        self._config = config\n        self._model = config.model\n\n        # Create OpenAI client\n        self._client = AsyncOpenAI(\n            api_key=config.api_key.get_secret_value(),\n            organization=config.organization,\n            base_url=config.base_url,\n            timeout=config.timeout,\n            max_retries=0,  # We handle retries in BaseProvider\n        )\n\n        # Token counter (tiktoken for accurate counting)\n        self._token_counter = create_token_counter(\"openai\", model=model)\n\n        # Cost tracker\n        self._track_cost = track_cost\n        if track_cost:\n            self.cost_tracker = CostTracker(\n                provider_name=\"openai\",\n                pricing=PRICING_OPENAI,\n            )\n\n    @property\n    def model(self) -&gt; str:\n        \"\"\"Get current model name.\"\"\"\n        return self._model\n\n    @property\n    def config(self) -&gt; OpenAIConfig:\n        \"\"\"Get provider configuration.\"\"\"\n        return self._config\n\n    def _format_messages(self, messages: list[Message]) -&gt; list[dict[str, str]]:\n        \"\"\"Convert bruno-core messages to OpenAI format.\"\"\"\n        return [{\"role\": msg.role.value, \"content\": msg.content} for msg in messages]\n\n    def _build_request_params(self, **kwargs: Any) -&gt; dict[str, Any]:\n        \"\"\"Build OpenAI API request parameters.\"\"\"\n        params: dict[str, Any] = {\n            \"model\": self._model,\n            \"temperature\": self._config.temperature,\n            \"top_p\": self._config.top_p,\n        }\n\n        # Add optional parameters\n        if self._config.max_tokens is not None:\n            params[\"max_tokens\"] = self._config.max_tokens\n        if self._config.presence_penalty != 0.0:\n            params[\"presence_penalty\"] = self._config.presence_penalty\n        if self._config.frequency_penalty != 0.0:\n            params[\"frequency_penalty\"] = self._config.frequency_penalty\n        if self._config.stop is not None:\n            params[\"stop\"] = self._config.stop\n\n        # Override with kwargs\n        params.update(kwargs)\n\n        return params\n\n    def _track_usage(\n        self,\n        messages: list[Message],\n        response_text: str,\n        completion: Optional[ChatCompletion] = None,\n    ) -&gt; None:\n        \"\"\"Track token usage and costs.\"\"\"\n        if not self._track_cost:\n            return\n\n        # Get token counts from response or estimate\n        if completion and hasattr(completion, \"usage\") and completion.usage:\n            input_tokens = completion.usage.prompt_tokens\n            output_tokens = completion.usage.completion_tokens\n        else:\n            # Estimate if usage not available\n            input_text = \" \".join(msg.content for msg in messages)\n            input_tokens = self._token_counter.count_tokens(input_text)\n            output_tokens = self._token_counter.count_tokens(response_text)\n\n        # Track in cost tracker\n        self.cost_tracker.track_request(\n            model=self._model,\n            input_tokens=input_tokens,\n            output_tokens=output_tokens,\n        )\n\n    async def generate(\n        self,\n        messages: list[Message],\n        temperature: Optional[float] = None,\n        max_tokens: Optional[int] = None,\n        **kwargs: Any,\n    ) -&gt; str:\n        \"\"\"\n        Generate a complete response from OpenAI.\n\n        Args:\n            messages: List of conversation messages\n            temperature: Sampling temperature (0.0 to 2.0)\n            max_tokens: Maximum number of tokens to generate\n            **kwargs: Additional generation parameters\n\n        Returns:\n            Generated text response\n\n        Raises:\n            AuthenticationError: If API key is invalid\n            RateLimitError: If rate limit exceeded\n            ModelNotFoundError: If model doesn't exist\n            LLMTimeoutError: If request times out\n            LLMError: For other API errors\n        \"\"\"\n        try:\n            # Build parameters including explicit ones\n            generation_params = {}\n            if temperature is not None:\n                generation_params[\"temperature\"] = temperature\n            if max_tokens is not None:\n                generation_params[\"max_tokens\"] = max_tokens\n            generation_params.update(kwargs)\n\n            params = self._build_request_params(**generation_params)\n\n            completion: ChatCompletion = await self._client.chat.completions.create(\n                messages=self._format_messages(messages), **params\n            )\n\n            # Extract response content\n            if not completion.choices:\n                raise InvalidResponseError(\"No choices in response\")\n\n            content = completion.choices[0].message.content or \"\"\n\n            # Track usage\n            self._track_usage(messages, content, completion)\n\n            return content\n\n        except APITimeoutError as e:\n            raise LLMTimeoutError(f\"Request timed out: {e}\") from e\n        except APIConnectionError as e:\n            raise LLMError(f\"Connection error: {e}\") from e\n        except OpenAIError as e:\n            # Parse OpenAI-specific errors by exception type\n            if isinstance(e, OpenAIAuthError):\n                raise AuthenticationError(f\"Invalid API key: {e}\") from e\n            elif isinstance(e, OpenAIRateLimitError):\n                raise RateLimitError(f\"Rate limit exceeded: {e}\") from e\n            elif isinstance(e, OpenAINotFoundError):\n                raise ModelNotFoundError(f\"Model '{self._model}' not found: {e}\") from e\n            else:\n                raise LLMError(f\"OpenAI API error: {e}\") from e\n\n    async def stream(\n        self,\n        messages: list[Message],\n        temperature: Optional[float] = None,\n        max_tokens: Optional[int] = None,\n        **kwargs: Any,\n    ) -&gt; AsyncIterator[str]:\n        \"\"\"\n        Stream response tokens from OpenAI.\n\n        Args:\n            messages: List of conversation messages\n            temperature: Sampling temperature (0.0 to 2.0)\n            max_tokens: Maximum number of tokens to generate\n            **kwargs: Additional generation parameters\n\n        Yields:\n            Response text chunks\n\n        Raises:\n            StreamError: If streaming fails\n        \"\"\"\n        try:\n            # Build parameters including explicit ones\n            generation_params = {}\n            if temperature is not None:\n                generation_params[\"temperature\"] = temperature\n            if max_tokens is not None:\n                generation_params[\"max_tokens\"] = max_tokens\n            generation_params.update(kwargs)\n\n            params = self._build_request_params(stream=True, **generation_params)\n\n            stream = await self._client.chat.completions.create(\n                messages=self._format_messages(messages), **params\n            )\n\n            full_response = []\n\n            async for chunk in stream:\n                if not chunk.choices:\n                    continue\n\n                delta = chunk.choices[0].delta\n                content = delta.content\n\n                if content:\n                    full_response.append(content)\n                    yield content\n\n            # Track usage after streaming completes\n            response_text = \"\".join(full_response)\n            self._track_usage(messages, response_text)\n\n        except APITimeoutError as e:\n            raise StreamError(f\"Stream timed out: {e}\") from e\n        except APIConnectionError as e:\n            raise StreamError(f\"Connection error: {e}\") from e\n        except OpenAIError as e:\n            # Parse OpenAI-specific errors by exception type\n            if isinstance(e, OpenAIAuthError):\n                raise AuthenticationError(f\"Invalid API key: {e}\") from e\n            elif isinstance(e, OpenAIRateLimitError):\n                raise RateLimitError(f\"Rate limit exceeded: {e}\") from e\n            elif isinstance(e, OpenAINotFoundError):\n                raise ModelNotFoundError(f\"Model '{self._model}' not found: {e}\") from e\n            else:\n                raise StreamError(f\"Streaming failed: {e}\") from e\n        except Exception as e:\n            raise StreamError(f\"Unexpected streaming error: {e}\") from e\n\n    async def list_models(self) -&gt; list[str]:\n        \"\"\"\n        List available OpenAI models.\n\n        Returns:\n            List of model IDs\n\n        Raises:\n            LLMError: If request fails\n        \"\"\"\n        try:\n            models = await self._client.models.list()\n            return [model.id for model in models.data]\n        except OpenAIError as e:\n            raise LLMError(f\"Failed to list models: {e}\") from e\n\n    async def check_connection(self) -&gt; bool:\n        \"\"\"\n        Check if OpenAI API is accessible.\n\n        Returns:\n            True if API is accessible with valid credentials\n        \"\"\"\n        try:\n            await self._client.models.list()\n            return True\n        except Exception:\n            return False\n\n    def get_token_count(self, text: str) -&gt; int:\n        \"\"\"\n        Get accurate token count for text using tiktoken.\n\n        Args:\n            text: Text to count tokens for\n\n        Returns:\n            Exact token count\n        \"\"\"\n        return self._token_counter.count_tokens(text)\n\n    def get_model_info(self) -&gt; dict[str, Any]:\n        \"\"\"\n        Get current model information.\n\n        Returns:\n            Dictionary with model details\n        \"\"\"\n        info = {\n            \"provider\": \"openai\",\n            \"model\": self._model,\n            \"base_url\": self._config.base_url,\n            \"temperature\": self._config.temperature,\n            \"max_tokens\": self._config.max_tokens,\n        }\n\n        # Add cost tracking info if enabled\n        if self._track_cost:\n            info[\"cost_tracking\"] = {\n                \"enabled\": True,\n                \"total_cost\": self.cost_tracker.get_total_cost(),\n                \"total_requests\": self.cost_tracker.get_request_count(),\n            }\n\n        return info\n\n    async def close(self) -&gt; None:\n        \"\"\"Close OpenAI client and cleanup resources.\"\"\"\n        await self._client.close()\n\n    async def __aenter__(self):\n        \"\"\"Context manager entry.\"\"\"\n        return self\n\n    async def __aexit__(self, exc_type, exc_val, exc_tb):\n        \"\"\"Context manager exit.\"\"\"\n        await self.close()\n</code></pre>"},{"location":"api/providers/#bruno_llm.providers.openai.OpenAIProvider.model","title":"<code>model</code>  <code>property</code>","text":"<p>Get current model name.</p>"},{"location":"api/providers/#bruno_llm.providers.openai.OpenAIProvider.config","title":"<code>config</code>  <code>property</code>","text":"<p>Get provider configuration.</p>"},{"location":"api/providers/#bruno_llm.providers.openai.OpenAIProvider.__init__","title":"<code>__init__(api_key, model='gpt-4', organization=None, timeout=30.0, track_cost=True, **kwargs)</code>","text":"<p>Initialize OpenAI provider.</p> Source code in <code>bruno_llm/providers/openai/provider.py</code> <pre><code>def __init__(\n    self,\n    api_key: str,\n    model: str = \"gpt-4\",\n    organization: Optional[str] = None,\n    timeout: float = 30.0,\n    track_cost: bool = True,\n    **kwargs: Any,\n):\n    \"\"\"Initialize OpenAI provider.\"\"\"\n    # Create config\n    config = OpenAIConfig(\n        api_key=api_key, model=model, organization=organization, timeout=timeout, **kwargs\n    )\n\n    # Initialize base provider\n    super().__init__(\n        provider_name=\"openai\",\n        max_retries=config.max_retries,\n        timeout=timeout,\n    )\n\n    # Store config\n    self._config = config\n    self._model = config.model\n\n    # Create OpenAI client\n    self._client = AsyncOpenAI(\n        api_key=config.api_key.get_secret_value(),\n        organization=config.organization,\n        base_url=config.base_url,\n        timeout=config.timeout,\n        max_retries=0,  # We handle retries in BaseProvider\n    )\n\n    # Token counter (tiktoken for accurate counting)\n    self._token_counter = create_token_counter(\"openai\", model=model)\n\n    # Cost tracker\n    self._track_cost = track_cost\n    if track_cost:\n        self.cost_tracker = CostTracker(\n            provider_name=\"openai\",\n            pricing=PRICING_OPENAI,\n        )\n</code></pre>"},{"location":"api/providers/#bruno_llm.providers.openai.OpenAIProvider.generate","title":"<code>generate(messages, temperature=None, max_tokens=None, **kwargs)</code>  <code>async</code>","text":"<p>Generate a complete response from OpenAI.</p> <p>Parameters:</p> Name Type Description Default <code>messages</code> <code>list[Message]</code> <p>List of conversation messages</p> required <code>temperature</code> <code>Optional[float]</code> <p>Sampling temperature (0.0 to 2.0)</p> <code>None</code> <code>max_tokens</code> <code>Optional[int]</code> <p>Maximum number of tokens to generate</p> <code>None</code> <code>**kwargs</code> <code>Any</code> <p>Additional generation parameters</p> <code>{}</code> <p>Returns:</p> Type Description <code>str</code> <p>Generated text response</p> <p>Raises:</p> Type Description <code>AuthenticationError</code> <p>If API key is invalid</p> <code>RateLimitError</code> <p>If rate limit exceeded</p> <code>ModelNotFoundError</code> <p>If model doesn't exist</p> <code>TimeoutError</code> <p>If request times out</p> <code>LLMError</code> <p>For other API errors</p> Source code in <code>bruno_llm/providers/openai/provider.py</code> <pre><code>async def generate(\n    self,\n    messages: list[Message],\n    temperature: Optional[float] = None,\n    max_tokens: Optional[int] = None,\n    **kwargs: Any,\n) -&gt; str:\n    \"\"\"\n    Generate a complete response from OpenAI.\n\n    Args:\n        messages: List of conversation messages\n        temperature: Sampling temperature (0.0 to 2.0)\n        max_tokens: Maximum number of tokens to generate\n        **kwargs: Additional generation parameters\n\n    Returns:\n        Generated text response\n\n    Raises:\n        AuthenticationError: If API key is invalid\n        RateLimitError: If rate limit exceeded\n        ModelNotFoundError: If model doesn't exist\n        LLMTimeoutError: If request times out\n        LLMError: For other API errors\n    \"\"\"\n    try:\n        # Build parameters including explicit ones\n        generation_params = {}\n        if temperature is not None:\n            generation_params[\"temperature\"] = temperature\n        if max_tokens is not None:\n            generation_params[\"max_tokens\"] = max_tokens\n        generation_params.update(kwargs)\n\n        params = self._build_request_params(**generation_params)\n\n        completion: ChatCompletion = await self._client.chat.completions.create(\n            messages=self._format_messages(messages), **params\n        )\n\n        # Extract response content\n        if not completion.choices:\n            raise InvalidResponseError(\"No choices in response\")\n\n        content = completion.choices[0].message.content or \"\"\n\n        # Track usage\n        self._track_usage(messages, content, completion)\n\n        return content\n\n    except APITimeoutError as e:\n        raise LLMTimeoutError(f\"Request timed out: {e}\") from e\n    except APIConnectionError as e:\n        raise LLMError(f\"Connection error: {e}\") from e\n    except OpenAIError as e:\n        # Parse OpenAI-specific errors by exception type\n        if isinstance(e, OpenAIAuthError):\n            raise AuthenticationError(f\"Invalid API key: {e}\") from e\n        elif isinstance(e, OpenAIRateLimitError):\n            raise RateLimitError(f\"Rate limit exceeded: {e}\") from e\n        elif isinstance(e, OpenAINotFoundError):\n            raise ModelNotFoundError(f\"Model '{self._model}' not found: {e}\") from e\n        else:\n            raise LLMError(f\"OpenAI API error: {e}\") from e\n</code></pre>"},{"location":"api/providers/#bruno_llm.providers.openai.OpenAIProvider.stream","title":"<code>stream(messages, temperature=None, max_tokens=None, **kwargs)</code>  <code>async</code>","text":"<p>Stream response tokens from OpenAI.</p> <p>Parameters:</p> Name Type Description Default <code>messages</code> <code>list[Message]</code> <p>List of conversation messages</p> required <code>temperature</code> <code>Optional[float]</code> <p>Sampling temperature (0.0 to 2.0)</p> <code>None</code> <code>max_tokens</code> <code>Optional[int]</code> <p>Maximum number of tokens to generate</p> <code>None</code> <code>**kwargs</code> <code>Any</code> <p>Additional generation parameters</p> <code>{}</code> <p>Yields:</p> Type Description <code>AsyncIterator[str]</code> <p>Response text chunks</p> <p>Raises:</p> Type Description <code>StreamError</code> <p>If streaming fails</p> Source code in <code>bruno_llm/providers/openai/provider.py</code> <pre><code>async def stream(\n    self,\n    messages: list[Message],\n    temperature: Optional[float] = None,\n    max_tokens: Optional[int] = None,\n    **kwargs: Any,\n) -&gt; AsyncIterator[str]:\n    \"\"\"\n    Stream response tokens from OpenAI.\n\n    Args:\n        messages: List of conversation messages\n        temperature: Sampling temperature (0.0 to 2.0)\n        max_tokens: Maximum number of tokens to generate\n        **kwargs: Additional generation parameters\n\n    Yields:\n        Response text chunks\n\n    Raises:\n        StreamError: If streaming fails\n    \"\"\"\n    try:\n        # Build parameters including explicit ones\n        generation_params = {}\n        if temperature is not None:\n            generation_params[\"temperature\"] = temperature\n        if max_tokens is not None:\n            generation_params[\"max_tokens\"] = max_tokens\n        generation_params.update(kwargs)\n\n        params = self._build_request_params(stream=True, **generation_params)\n\n        stream = await self._client.chat.completions.create(\n            messages=self._format_messages(messages), **params\n        )\n\n        full_response = []\n\n        async for chunk in stream:\n            if not chunk.choices:\n                continue\n\n            delta = chunk.choices[0].delta\n            content = delta.content\n\n            if content:\n                full_response.append(content)\n                yield content\n\n        # Track usage after streaming completes\n        response_text = \"\".join(full_response)\n        self._track_usage(messages, response_text)\n\n    except APITimeoutError as e:\n        raise StreamError(f\"Stream timed out: {e}\") from e\n    except APIConnectionError as e:\n        raise StreamError(f\"Connection error: {e}\") from e\n    except OpenAIError as e:\n        # Parse OpenAI-specific errors by exception type\n        if isinstance(e, OpenAIAuthError):\n            raise AuthenticationError(f\"Invalid API key: {e}\") from e\n        elif isinstance(e, OpenAIRateLimitError):\n            raise RateLimitError(f\"Rate limit exceeded: {e}\") from e\n        elif isinstance(e, OpenAINotFoundError):\n            raise ModelNotFoundError(f\"Model '{self._model}' not found: {e}\") from e\n        else:\n            raise StreamError(f\"Streaming failed: {e}\") from e\n    except Exception as e:\n        raise StreamError(f\"Unexpected streaming error: {e}\") from e\n</code></pre>"},{"location":"api/providers/#bruno_llm.providers.openai.OpenAIProvider.list_models","title":"<code>list_models()</code>  <code>async</code>","text":"<p>List available OpenAI models.</p> <p>Returns:</p> Type Description <code>list[str]</code> <p>List of model IDs</p> <p>Raises:</p> Type Description <code>LLMError</code> <p>If request fails</p> Source code in <code>bruno_llm/providers/openai/provider.py</code> <pre><code>async def list_models(self) -&gt; list[str]:\n    \"\"\"\n    List available OpenAI models.\n\n    Returns:\n        List of model IDs\n\n    Raises:\n        LLMError: If request fails\n    \"\"\"\n    try:\n        models = await self._client.models.list()\n        return [model.id for model in models.data]\n    except OpenAIError as e:\n        raise LLMError(f\"Failed to list models: {e}\") from e\n</code></pre>"},{"location":"api/providers/#bruno_llm.providers.openai.OpenAIProvider.check_connection","title":"<code>check_connection()</code>  <code>async</code>","text":"<p>Check if OpenAI API is accessible.</p> <p>Returns:</p> Type Description <code>bool</code> <p>True if API is accessible with valid credentials</p> Source code in <code>bruno_llm/providers/openai/provider.py</code> <pre><code>async def check_connection(self) -&gt; bool:\n    \"\"\"\n    Check if OpenAI API is accessible.\n\n    Returns:\n        True if API is accessible with valid credentials\n    \"\"\"\n    try:\n        await self._client.models.list()\n        return True\n    except Exception:\n        return False\n</code></pre>"},{"location":"api/providers/#bruno_llm.providers.openai.OpenAIProvider.get_token_count","title":"<code>get_token_count(text)</code>","text":"<p>Get accurate token count for text using tiktoken.</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>str</code> <p>Text to count tokens for</p> required <p>Returns:</p> Type Description <code>int</code> <p>Exact token count</p> Source code in <code>bruno_llm/providers/openai/provider.py</code> <pre><code>def get_token_count(self, text: str) -&gt; int:\n    \"\"\"\n    Get accurate token count for text using tiktoken.\n\n    Args:\n        text: Text to count tokens for\n\n    Returns:\n        Exact token count\n    \"\"\"\n    return self._token_counter.count_tokens(text)\n</code></pre>"},{"location":"api/providers/#bruno_llm.providers.openai.OpenAIProvider.get_model_info","title":"<code>get_model_info()</code>","text":"<p>Get current model information.</p> <p>Returns:</p> Type Description <code>dict[str, Any]</code> <p>Dictionary with model details</p> Source code in <code>bruno_llm/providers/openai/provider.py</code> <pre><code>def get_model_info(self) -&gt; dict[str, Any]:\n    \"\"\"\n    Get current model information.\n\n    Returns:\n        Dictionary with model details\n    \"\"\"\n    info = {\n        \"provider\": \"openai\",\n        \"model\": self._model,\n        \"base_url\": self._config.base_url,\n        \"temperature\": self._config.temperature,\n        \"max_tokens\": self._config.max_tokens,\n    }\n\n    # Add cost tracking info if enabled\n    if self._track_cost:\n        info[\"cost_tracking\"] = {\n            \"enabled\": True,\n            \"total_cost\": self.cost_tracker.get_total_cost(),\n            \"total_requests\": self.cost_tracker.get_request_count(),\n        }\n\n    return info\n</code></pre>"},{"location":"api/providers/#bruno_llm.providers.openai.OpenAIProvider.close","title":"<code>close()</code>  <code>async</code>","text":"<p>Close OpenAI client and cleanup resources.</p> Source code in <code>bruno_llm/providers/openai/provider.py</code> <pre><code>async def close(self) -&gt; None:\n    \"\"\"Close OpenAI client and cleanup resources.\"\"\"\n    await self._client.close()\n</code></pre>"},{"location":"api/providers/#bruno_llm.providers.openai.OpenAIProvider.__aenter__","title":"<code>__aenter__()</code>  <code>async</code>","text":"<p>Context manager entry.</p> Source code in <code>bruno_llm/providers/openai/provider.py</code> <pre><code>async def __aenter__(self):\n    \"\"\"Context manager entry.\"\"\"\n    return self\n</code></pre>"},{"location":"api/providers/#bruno_llm.providers.openai.OpenAIProvider.__aexit__","title":"<code>__aexit__(exc_type, exc_val, exc_tb)</code>  <code>async</code>","text":"<p>Context manager exit.</p> Source code in <code>bruno_llm/providers/openai/provider.py</code> <pre><code>async def __aexit__(self, exc_type, exc_val, exc_tb):\n    \"\"\"Context manager exit.\"\"\"\n    await self.close()\n</code></pre>"},{"location":"development/contributing/","title":"Contributing","text":"<p>See CONTRIBUTING.md in the repository for detailed contribution guidelines.</p>"},{"location":"development/contributing/#quick-start","title":"Quick Start","text":"<ol> <li>Fork and clone the repository</li> <li>Install dependencies: <code>pip install -e \".[all]\"</code></li> <li>Install pre-commit hooks: <code>pre-commit install</code></li> <li>Make changes and run tests: <code>pytest tests/</code></li> <li>Submit a pull request</li> </ol>"},{"location":"development/contributing/#pre-commit-hooks","title":"Pre-commit Hooks","text":"<p>Critical: Always install pre-commit hooks to catch issues before CI:</p> <pre><code>pip install pre-commit\npre-commit install\n</code></pre> <p>See Pre-commit Setup for complete guide.</p>"},{"location":"development/pre-commit/","title":"Pre-commit Hooks","text":"<p>See PRE_COMMIT_SETUP.md for complete documentation.</p>"},{"location":"development/pre-commit/#quick-setup","title":"Quick Setup","text":"<pre><code># Install pre-commit\npip install pre-commit\n\n# Install hooks (run once per repository)\npre-commit install\n</code></pre>"},{"location":"development/pre-commit/#what-it-does","title":"What It Does","text":"<p>On every commit, automatically:</p> <ul> <li>\u2705 Formats code with ruff</li> <li>\u2705 Runs linting checks</li> <li>\u2705 Performs type checking</li> <li>\u2705 Validates YAML/JSON/TOML</li> <li>\u2705 Removes trailing whitespace</li> <li>\u2705 Checks for large files and secrets</li> </ul>"},{"location":"development/pre-commit/#benefits","title":"Benefits","text":"<ul> <li>Catches issues before CI</li> <li>Auto-fixes formatting and linting</li> <li>Faster feedback (seconds vs minutes)</li> <li>Prevents CI failures</li> <li>Saves time and CI minutes</li> </ul>"},{"location":"development/testing/","title":"Testing","text":"<p>See TESTING.md for complete testing guide.</p>"},{"location":"development/testing/#running-tests","title":"Running Tests","text":"<pre><code># Run all tests\npytest tests/\n\n# Run with coverage\npytest tests/ --cov=bruno_llm --cov-report=html\n\n# Run specific test file\npytest tests/test_factory.py\n\n# Run integration tests\npytest -m integration\n</code></pre>"},{"location":"development/testing/#test-organization","title":"Test Organization","text":"<ul> <li><code>tests/providers/</code> - Provider-specific tests</li> <li><code>tests/test_*.py</code> - Feature tests</li> <li>203 total tests, 91% coverage</li> </ul>"},{"location":"getting-started/installation/","title":"Installation","text":""},{"location":"getting-started/installation/#requirements","title":"Requirements","text":"<ul> <li>Python 3.9 or higher</li> <li>pip</li> </ul>"},{"location":"getting-started/installation/#install-from-pypi","title":"Install from PyPI","text":"<pre><code># Core installation\npip install bruno-llm\n\n# With OpenAI support\npip install bruno-llm[openai]\n\n# With all optional dependencies\npip install bruno-llm[all]\n</code></pre>"},{"location":"getting-started/installation/#install-from-source","title":"Install from Source","text":"<pre><code># Clone repository\ngit clone https://github.com/meggy-ai/bruno-llm.git\ncd bruno-llm\n\n# Install bruno-core dependency\npip install git+https://github.com/meggy-ai/bruno-core.git@main\n\n# Install in development mode\npip install -e \".[all]\"\n</code></pre>"},{"location":"getting-started/installation/#verify-installation","title":"Verify Installation","text":"<pre><code>import bruno_llm\nprint(bruno_llm.__version__)\n</code></pre>"},{"location":"getting-started/installation/#next-steps","title":"Next Steps","text":"<ul> <li>Quick Start Guide</li> <li>User Guide</li> </ul>"},{"location":"getting-started/quickstart/","title":"Quick Start","text":""},{"location":"getting-started/quickstart/#basic-usage","title":"Basic Usage","text":""},{"location":"getting-started/quickstart/#1-create-a-provider","title":"1. Create a Provider","text":"<pre><code>from bruno_llm import LLMFactory\n\n# Ollama (local)\nllm = LLMFactory.create(\"ollama\", {\n    \"model\": \"llama2\",\n    \"base_url\": \"http://localhost:11434\"\n})\n\n# OpenAI (cloud)\nllm = LLMFactory.create(\"openai\", {\n    \"api_key\": \"sk-...\",\n    \"model\": \"gpt-4\"\n})\n</code></pre>"},{"location":"getting-started/quickstart/#2-generate-responses","title":"2. Generate Responses","text":"<pre><code>from bruno_core.models import Message, MessageRole\n\nmessages = [\n    Message(role=MessageRole.SYSTEM, content=\"You are a helpful assistant.\"),\n    Message(role=MessageRole.USER, content=\"What is Python?\")\n]\n\n# Generate complete response\nresponse = await llm.generate(messages)\nprint(response)\n</code></pre>"},{"location":"getting-started/quickstart/#3-stream-responses","title":"3. Stream Responses","text":"<pre><code># Stream response tokens\nasync for chunk in llm.stream(messages):\n    print(chunk, end=\"\", flush=True)\n</code></pre>"},{"location":"getting-started/quickstart/#4-generate-embeddings","title":"4. Generate Embeddings","text":"<pre><code>from bruno_llm.embedding_factory import EmbeddingFactory\n\n# Create embedding provider\nembedder = EmbeddingFactory.create(\"openai\", {\n    \"api_key\": \"sk-...\",\n    \"model\": \"text-embedding-3-small\"\n})\n\n# Generate embedding\ntext = \"Machine learning transforms data into insights\"\nembedding = await embedder.embed_text(text)\nprint(f\"Embedding dimension: {len(embedding)}\")\n\n# Batch embeddings\ntexts = [\"AI is powerful\", \"Python is versatile\", \"Data drives decisions\"]\nembeddings = await embedder.embed_texts(texts)\nprint(f\"Generated {len(embeddings)} embeddings\")\n</code></pre>"},{"location":"getting-started/quickstart/#5-similarity-search","title":"5. Similarity Search","text":"<pre><code># Calculate similarity between texts\nquery = \"artificial intelligence\"\ndoc1 = \"Machine learning algorithms\"\ndoc2 = \"Cooking recipes\"\n\nquery_emb = await embedder.embed_text(query)\ndoc1_emb = await embedder.embed_text(doc1)\ndoc2_emb = await embedder.embed_text(doc2)\n\nsimilarity1 = embedder.calculate_similarity(query_emb, doc1_emb)\nsimilarity2 = embedder.calculate_similarity(query_emb, doc2_emb)\n\nprint(f\"Query-Doc1 similarity: {similarity1:.3f}\")  # Higher\nprint(f\"Query-Doc2 similarity: {similarity2:.3f}\")  # Lower\n</code></pre>"},{"location":"getting-started/quickstart/#complete-examples","title":"Complete Examples","text":""},{"location":"getting-started/quickstart/#llm-example","title":"LLM Example","text":"<pre><code>import asyncio\nfrom bruno_core.models import Message, MessageRole\nfrom bruno_llm import LLMFactory\n\nasync def main():\n    # Create provider\n    llm = LLMFactory.create(\"ollama\", {\"model\": \"llama2\"})\n\n    # Prepare messages\n    messages = [\n        Message(role=MessageRole.USER, content=\"Tell me a joke\")\n    ]\n\n    # Generate response\n    response = await llm.generate(messages)\n    print(f\"Response: {response}\")\n\n    # Get token count\n    tokens = llm.get_token_count(response)\n    print(f\"Tokens: {tokens}\")\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n</code></pre>"},{"location":"getting-started/quickstart/#embedding-example","title":"Embedding Example","text":"<pre><code>import asyncio\nfrom bruno_llm.embedding_factory import EmbeddingFactory\n\nasync def embedding_demo():\n    # Create embedding provider (local with Ollama)\n    embedder = EmbeddingFactory.create(\"ollama\", {\n        \"model\": \"nomic-embed-text\",\n        \"base_url\": \"http://localhost:11434\"\n    })\n\n    # Sample documents\n    documents = [\n        \"Python is a programming language\",\n        \"Machine learning uses algorithms\",\n        \"Databases store information\",\n        \"APIs connect systems\"\n    ]\n\n    # Generate embeddings\n    embeddings = await embedder.embed_texts(documents)\n\n    # Search for similar content\n    query = \"programming languages\"\n    query_embedding = await embedder.embed_text(query)\n\n    # Find most similar document\n    similarities = []\n    for i, doc_embedding in enumerate(embeddings):\n        similarity = embedder.calculate_similarity(query_embedding, doc_embedding)\n        similarities.append((documents[i], similarity))\n\n    # Sort by similarity\n    similarities.sort(key=lambda x: x[1], reverse=True)\n\n    print(f\"Query: {query}\")\n    print(\"Most similar documents:\")\n    for doc, score in similarities[:3]:\n        print(f\"  {score:.3f}: {doc}\")\n\nif __name__ == \"__main__\":\n    asyncio.run(embedding_demo())\n</code></pre>"},{"location":"getting-started/quickstart/#combined-rag-example","title":"Combined RAG Example","text":"<pre><code>import asyncio\nfrom bruno_llm.factory import LLMFactory\nfrom bruno_llm.embedding_factory import EmbeddingFactory\nfrom bruno_core.models import Message, MessageRole\n\nasync def simple_rag_demo():\n    # Create providers\n    llm = LLMFactory.create_from_env(\"openai\")\n    embedder = EmbeddingFactory.create_from_env(\"openai\")\n\n    # Knowledge base\n    knowledge = [\n        \"Bruno-LLM is a Python library for LLM integration\",\n        \"It supports multiple providers like OpenAI and Ollama\",\n        \"The factory pattern makes switching providers easy\",\n        \"Embeddings enable semantic search capabilities\"\n    ]\n\n    # Generate embeddings for knowledge\n    knowledge_embeddings = await embedder.embed_texts(knowledge)\n\n    # User question\n    question = \"What is Bruno-LLM?\"\n    question_embedding = await embedder.embed_text(question)\n\n    # Find relevant knowledge\n    similarities = []\n    for i, kb_embedding in enumerate(knowledge_embeddings):\n        similarity = embedder.calculate_similarity(question_embedding, kb_embedding)\n        similarities.append((knowledge[i], similarity))\n\n    # Get top 2 most relevant\n    similarities.sort(key=lambda x: x[1], reverse=True)\n    context = \"\\n\".join([doc for doc, _ in similarities[:2]])\n\n    # Generate answer with context\n    messages = [\n        Message(role=MessageRole.SYSTEM, content=\n            \"Answer the question based on the provided context.\"),\n        Message(role=MessageRole.USER, content=f\"Context:\\n{context}\\n\\nQuestion: {question}\")\n    ]\n\n    answer = await llm.generate(messages)\n    print(f\"Question: {question}\")\n    print(f\"Answer: {answer}\")\n\nif __name__ == \"__main__\":\n    asyncio.run(simple_rag_demo())\n</code></pre>"},{"location":"getting-started/quickstart/#next-steps","title":"Next Steps","text":"<ul> <li>User Guide - Complete documentation</li> <li>Ollama Provider - Local LLM setup</li> <li>OpenAI Provider - Cloud LLM setup</li> <li>Advanced Features - Caching, streaming, etc.</li> </ul>"},{"location":"how-to/create-release-branch-steps/","title":"Release Branch Creation Steps","text":""},{"location":"how-to/create-release-branch-steps/#release-branch-creation-steps_1","title":"\ud83d\udd00 Release Branch Creation Steps","text":""},{"location":"how-to/create-release-branch-steps/#1-create-and-switch-to-release-branch","title":"1. Create and Switch to Release Branch","text":"<pre><code># Create and checkout release branch from main\ngit checkout -b release/v0.2.0\n\n# Or create branch without switching\ngit branch release/v0.2.0\n</code></pre>"},{"location":"how-to/create-release-branch-steps/#2-verify-branch-creation","title":"2. Verify Branch Creation","text":"<pre><code># Check current branch\ngit branch\n\n# Should show:\n# * release/v0.2.0\n#   main\n</code></pre>"},{"location":"how-to/create-release-branch-steps/#3-final-release-preparation-optional","title":"3. Final Release Preparation (Optional)","text":"<pre><code># Run final tests to ensure everything works\npython -m pytest tests/ -v\n\n# Check code quality one more time\nruff check .\nruff format .\nmypy bruno_llm --ignore-missing-imports\n</code></pre>"},{"location":"how-to/create-release-branch-steps/#4-push-release-branch-to-remote","title":"4. Push Release Branch to Remote","text":"<pre><code># Push the new branch to GitHub\ngit push -u origin release/v0.2.0\n</code></pre>"},{"location":"how-to/create-release-branch-steps/#5-create-release-tag-after-testing","title":"5. Create Release Tag (After Testing)","text":"<pre><code># Create annotated tag for the release\ngit tag -a v0.2.0 -m \"Release v0.2.0: Comprehensive embedding support and bruno-core integration\n\n- Complete Ollama and OpenAI embedding providers\n- EmbeddingInterface implementation from bruno-core\n- 288+ tests with 89% coverage\n- Comprehensive API documentation\n- Full backward compatibility with v0.1.0\"\n\n# Push tag to remote\ngit push origin v0.2.0\n</code></pre>"},{"location":"how-to/create-release-branch-steps/#6-create-github-release","title":"6. Create GitHub Release","text":"<ol> <li>Go to GitHub repository: <code>https://github.com/meggy-ai/bruno-llm</code></li> <li>Click \"Releases\" \u2192 \"Create a new release\"</li> <li>Choose tag: <code>v0.2.0</code></li> <li>Release title: <code>bruno-llm v0.2.0 - Embedding Support &amp; Enhanced Bruno-Core Integration</code></li> <li>Copy description from CHANGELOG.md v0.2.0 section</li> <li>Attach distribution files (if built):</li> <li><code>dist/bruno_llm-0.2.0-py3-none-any.whl</code></li> <li><code>dist/bruno_llm-0.2.0.tar.gz</code></li> <li>Mark as \"Latest release\"</li> <li>Click \"Publish release\"</li> </ol>"},{"location":"how-to/create-release-branch-steps/#7-merge-back-to-main-after-release","title":"7. Merge Back to Main (After Release)","text":"<pre><code># Switch back to main\ngit checkout main\n\n# Merge release branch (if any final changes were made)\ngit merge release/v0.2.0\n\n# Push updated main\ngit push origin main\n\n# Clean up release branch (optional)\ngit branch -d release/v0.2.0\ngit push origin --delete release/v0.2.0\n</code></pre>"},{"location":"how-to/create-release-branch-steps/#quick-commands-for-your-current-state","title":"\ud83d\ude80 Quick Commands for Your Current State","text":"<p>Since you're already on main with the v0.2.0 commit ready:</p> <pre><code># 1. Create release branch\ngit checkout -b release/v0.2.0\n\n# 2. Push to remote\ngit push -u origin release/v0.2.0\n\n# 3. Create and push tag\ngit tag -a v0.2.0 -m \"Release v0.2.0: Embedding support and bruno-core integration\"\ngit push origin v0.2.0\n</code></pre>"},{"location":"how-to/create-release-branch-steps/#alternative-direct-release-from-main","title":"\ud83d\udccb Alternative: Direct Release from Main","text":"<p>Since your main branch is already clean and ready, you could also:</p> <pre><code># Create tag directly from main\ngit tag -a v0.2.0 -m \"Release v0.2.0\"\ngit push origin v0.2.0\n\n# Then create GitHub release pointing to this tag\n</code></pre>"},{"location":"how-to/create-release-branch-steps/#when-to-use-release-branches","title":"\ud83c\udfaf When to Use Release Branches","text":"<p>The release branch approach is recommended for: - Additional testing in isolation - Final documentation updates - Hotfixes if issues are found - Maintaining a clean release history - Multiple environments (staging, production)</p>"},{"location":"how-to/create-release-branch-steps/#building-distribution-packages-optional","title":"\ud83d\udce6 Building Distribution Packages (Optional)","text":"<p>If you want to build and test the package locally:</p> <pre><code># Clean previous builds\nrm -rf dist/ build/ *.egg-info/\n\n# Build package\npython -m build\n\n# Test installation in clean environment\npip install dist/bruno_llm-0.2.0-py3-none-any.whl\n\n# Verify installation\npython -c \"from bruno_llm import LLMFactory; from bruno_llm.embedding_factory import EmbeddingFactory; print('\u2705 Package installed successfully')\"\n</code></pre>"},{"location":"how-to/create-release-branch-steps/#pre-release-checklist","title":"\ud83d\udea8 Pre-Release Checklist","text":"<p>Before creating the release, ensure:</p> <ul> <li>[ ] All tests pass (<code>python -m pytest tests/ -v</code>)</li> <li>[ ] Code quality checks pass (<code>ruff check . &amp;&amp; ruff format . &amp;&amp; mypy bruno_llm</code>)</li> <li>[ ] Version numbers are updated consistently</li> <li>[ ] CHANGELOG.md is updated with release notes</li> <li>[ ] Documentation is current and accurate</li> <li>[ ] No sensitive information in code or configs</li> <li>[ ] All commits are properly signed and clean</li> </ul>"},{"location":"how-to/create-release-branch-steps/#post-release-tasks","title":"\ud83d\udccb Post-Release Tasks","text":"<p>After successful release:</p> <ol> <li>Update Documentation</li> <li>Verify documentation links work</li> <li>Update any version references</li> <li> <p>Check that examples still work</p> </li> <li> <p>Announce Release</p> </li> <li>GitHub Discussions post</li> <li>Community notifications</li> <li> <p>Update related projects</p> </li> <li> <p>Monitor Issues</p> </li> <li>Watch for bug reports</li> <li>Address any immediate issues</li> <li> <p>Plan next release based on feedback</p> </li> <li> <p>Prepare for Next Development</p> </li> <li>Create milestone for next version</li> <li>Update project roadmap</li> <li>Begin next feature development</li> </ol>"},{"location":"releases/GITHUB_RELEASE/","title":"GitHub Release - bruno-llm v0.1.0","text":""},{"location":"releases/GITHUB_RELEASE/#release-information","title":"Release Information","text":"<p>Version: 0.1.0 Release Date: December 9, 2025 Tag: v0.1.0 Type: Initial Release</p>"},{"location":"releases/GITHUB_RELEASE/#overview","title":"Overview","text":"<p>First stable release of bruno-llm, providing production-ready LLM provider implementations for the bruno-core framework. This release includes comprehensive support for local and cloud-based language models with advanced features for caching, context management, streaming, and cost tracking.</p>"},{"location":"releases/GITHUB_RELEASE/#key-features","title":"Key Features","text":""},{"location":"releases/GITHUB_RELEASE/#core-providers","title":"Core Providers","text":"<ul> <li>Ollama Provider - Local LLM inference with zero-config setup</li> <li>OpenAI Provider - Cloud-based GPT models with full API support</li> <li>Factory Pattern - Three flexible instantiation methods</li> </ul>"},{"location":"releases/GITHUB_RELEASE/#advanced-features","title":"Advanced Features","text":"<ul> <li>Response Caching (100% test coverage) - Redis and in-memory caching with TTL</li> <li>Context Window Management (96% coverage) - Automatic token counting and truncation</li> <li>Stream Aggregation (93% coverage) - Timeout handling and chunk collection</li> <li>Cost Tracking (98% coverage) - Per-model pricing with detailed reports</li> <li>Middleware System (93% coverage) - Logging, validation, and custom processing</li> </ul>"},{"location":"releases/GITHUB_RELEASE/#base-utilities","title":"Base Utilities","text":"<ul> <li>Token Counter (83% coverage) - Multi-provider token estimation</li> <li>Rate Limiter (84% coverage) - Sliding window and token bucket algorithms</li> <li>Retry Logic (92% coverage) - Exponential backoff with jitter</li> <li>Exception System (100% coverage) - Comprehensive error hierarchy</li> </ul>"},{"location":"releases/GITHUB_RELEASE/#installation","title":"Installation","text":"<pre><code># Core installation\npip install bruno-llm\n\n# With OpenAI support\npip install bruno-llm[openai]\n\n# With all optional dependencies\npip install bruno-llm[all]\n</code></pre>"},{"location":"releases/GITHUB_RELEASE/#quick-start","title":"Quick Start","text":"<pre><code>from bruno_core.models import Message, MessageRole\nfrom bruno_llm import LLMFactory\n\n# Create Ollama provider (local)\nllm = LLMFactory.create(\"ollama\", {\"model\": \"llama2\"})\n\n# Or OpenAI provider (cloud)\nllm = LLMFactory.create(\"openai\", {\n    \"api_key\": \"sk-...\",\n    \"model\": \"gpt-4\"\n})\n\n# Generate response\nmessages = [Message(role=MessageRole.USER, content=\"Hello!\")]\nresponse = await llm.generate(messages)\nprint(response)\n\n# Stream response\nasync for chunk in llm.stream(messages):\n    print(chunk, end=\"\", flush=True)\n</code></pre>"},{"location":"releases/GITHUB_RELEASE/#technical-details","title":"Technical Details","text":""},{"location":"releases/GITHUB_RELEASE/#requirements","title":"Requirements","text":"<ul> <li>Python: 3.9, 3.10, 3.11, 3.12</li> <li>Core Dependencies:</li> <li>bruno-core &gt;= 0.1.0</li> <li>httpx &gt;= 0.24.0</li> <li>aiohttp &gt;= 3.8.0</li> <li>pydantic &gt;= 2.0.0</li> <li>structlog &gt;= 23.1.0</li> </ul>"},{"location":"releases/GITHUB_RELEASE/#test-coverage","title":"Test Coverage","text":"<ul> <li>Total Tests: 203 (193 passing, 3 skipped, 7 WIP)</li> <li>Code Coverage: 91% overall</li> <li>Test Categories:</li> <li>Provider tests: 42</li> <li>Base utilities: 80</li> <li>Factory pattern: 22</li> <li>Integration tests: 15</li> <li>Other: 44</li> </ul>"},{"location":"releases/GITHUB_RELEASE/#documentation","title":"Documentation","text":"<ul> <li>README.md - 543 lines with comprehensive examples</li> <li>USER_GUIDE.md - 870+ lines with installation, setup, usage, best practices</li> <li>TESTING.md - Complete testing guide</li> <li>CONTRIBUTING.md - Development workflow and guidelines</li> <li>Example Scripts - basic_usage.py, advanced_features.py</li> </ul>"},{"location":"releases/GITHUB_RELEASE/#whats-included","title":"What's Included","text":""},{"location":"releases/GITHUB_RELEASE/#distribution-files","title":"Distribution Files","text":"<ul> <li><code>bruno_llm-0.1.0-py3-none-any.whl</code> - Wheel package</li> <li><code>bruno_llm-0.1.0.tar.gz</code> - Source distribution</li> </ul>"},{"location":"releases/GITHUB_RELEASE/#package-contents","title":"Package Contents","text":"<ul> <li>Full provider implementations (Ollama, OpenAI)</li> <li>All base utilities (cache, context, streaming, cost tracking, middleware, rate limiter, retry)</li> <li>Factory pattern for easy instantiation</li> <li>Type hints (py.typed included)</li> <li>Complete documentation</li> </ul>"},{"location":"releases/GITHUB_RELEASE/#known-limitations","title":"Known Limitations","text":"<ol> <li>bruno-core Dependency - Requires bruno-core to be installed (not yet on PyPI)</li> <li>Provider Support - Currently Ollama and OpenAI only (Claude, Gemini, Azure coming soon)</li> <li>Function Calling - Not yet implemented (planned for v0.2.0)</li> </ol>"},{"location":"releases/GITHUB_RELEASE/#upgrade-notes","title":"Upgrade Notes","text":"<p>This is the initial release, so no upgrade paths exist yet. Future releases will include migration guides.</p>"},{"location":"releases/GITHUB_RELEASE/#coming-in-v020","title":"Coming in v0.2.0","text":"<ul> <li>Claude provider (Anthropic API)</li> <li>Google Gemini provider</li> <li>Azure OpenAI support</li> <li>Function calling / tool use</li> <li>Prompt template system</li> <li>Batch processing support</li> </ul>"},{"location":"releases/GITHUB_RELEASE/#breaking-changes","title":"Breaking Changes","text":"<p>None - this is the initial release.</p>"},{"location":"releases/GITHUB_RELEASE/#contributors","title":"Contributors","text":"<ul> <li>Meggy AI Team</li> </ul>"},{"location":"releases/GITHUB_RELEASE/#support-resources","title":"Support &amp; Resources","text":"<ul> <li>Documentation: USER_GUIDE.md</li> <li>Issues: https://github.com/meggy-ai/bruno-llm/issues</li> <li>Bruno Core: https://github.com/meggy-ai/bruno-core</li> <li>License: MIT</li> </ul>"},{"location":"releases/GITHUB_RELEASE/#installation-verification","title":"Installation Verification","text":"<p>After installation, verify with:</p> <pre><code>import bruno_llm\nprint(bruno_llm.__version__)  # Should print: 0.1.0\n\nfrom bruno_llm import LLMFactory\nprint(\"\u2713 Installation successful!\")\n</code></pre>"},{"location":"releases/GITHUB_RELEASE/#acknowledgments","title":"Acknowledgments","text":"<p>Built with support from the bruno-core framework team. Special thanks to all early testers and contributors.</p> <p>Full Changelog: CHANGELOG.md</p>"},{"location":"releases/RELEASE_CHECKLIST_v0.2.0/","title":"Release Checklist v0.2.0","text":""},{"location":"releases/RELEASE_CHECKLIST_v0.2.0/#pre-release-verification","title":"Pre-Release Verification \u2705","text":""},{"location":"releases/RELEASE_CHECKLIST_v0.2.0/#code-quality","title":"Code Quality","text":"<ul> <li>[x] All tests passing (288+/288)</li> <li>[x] Code coverage at 89%</li> <li>[x] No linting errors (ruff check passes)</li> <li>[x] No formatting issues (ruff format passes)</li> <li>[x] Type checking passes</li> <li>[x] Pre-commit hooks configured and working</li> <li>[x] All CI checks would pass</li> </ul>"},{"location":"releases/RELEASE_CHECKLIST_v0.2.0/#new-features-v020","title":"New Features (v0.2.0)","text":"<ul> <li>[x] Embedding Support Complete</li> <li>[x] Ollama embedding provider (29 tests, 91% coverage)</li> <li>[x] OpenAI embedding provider</li> <li>[x] EmbeddingInterface implementation from bruno-core</li> <li>[x] Embedding factory with full pattern support</li> <li>[x] Similarity calculation utilities</li> <li>[x] Batch processing support</li> <li>[x] Bruno-Core Compatibility</li> <li>[x] Interface compatibility tests (24 tests)</li> <li>[x] Method signature verification</li> <li>[x] Async generator validation</li> <li>[x] Full compatibility validation script</li> <li>[x] Enhanced LLM Providers</li> <li>[x] Updated generate() and stream() methods with explicit parameters</li> <li>[x] Improved bruno-core interface compliance</li> <li>[x] Better parameter validation</li> </ul>"},{"location":"releases/RELEASE_CHECKLIST_v0.2.0/#documentation","title":"Documentation \u2705","text":"<ul> <li>[x] API Documentation Complete</li> <li>[x] Main API reference (comprehensive)</li> <li>[x] OpenAI provider documentation (detailed)</li> <li>[x] Ollama provider documentation (comprehensive)</li> <li>[x] Embedding guide with examples and patterns</li> <li>[x] User Guides Updated</li> <li>[x] USER_GUIDE.md updated with embedding functionality</li> <li>[x] Quick start guide enhanced with embedding examples</li> <li>[x] Provider-specific guides updated</li> <li>[x] Troubleshooting section enhanced</li> <li>[x] Documentation Consistency</li> <li>[x] All examples include embedding usage</li> <li>[x] RAG (Retrieval-Augmented Generation) patterns documented</li> <li>[x] Integration examples provided</li> <li>[x] Best practices documented</li> </ul>"},{"location":"releases/RELEASE_CHECKLIST_v0.2.0/#version-management","title":"Version Management \u2705","text":"<ul> <li>[x] Version bumped to 0.2.0 in <code>__version__.py</code></li> <li>[x] Version updated in <code>pyproject.toml</code></li> <li>[x] Description updated to include embeddings</li> <li>[x] Keywords updated with embedding-related terms</li> <li>[x] CHANGELOG.md updated with all v0.2.0 features</li> <li>[x] README.md updated with embedding functionality</li> </ul>"},{"location":"releases/RELEASE_CHECKLIST_v0.2.0/#package-integrity","title":"Package Integrity \u2705","text":"<ul> <li>[x] All imports work correctly</li> <li>[x] Factory patterns function for embeddings</li> <li>[x] Bruno-core interface compatibility verified</li> <li>[x] No breaking changes to existing functionality</li> </ul>"},{"location":"releases/RELEASE_CHECKLIST_v0.2.0/#release-tasks-to-be-completed","title":"Release Tasks (To Be Completed)","text":""},{"location":"releases/RELEASE_CHECKLIST_v0.2.0/#final-testing","title":"Final Testing","text":"<ul> <li>[ ] Test installation from wheel in clean environment</li> <li>[ ] Verify all imports work (LLM + embedding)</li> <li>[ ] Run embedding usage examples</li> <li>[ ] Test RAG examples</li> <li>[ ] Verify bruno-core compatibility in practice</li> <li>[ ] Test with both Ollama and OpenAI providers (LLM + embeddings)</li> </ul>"},{"location":"releases/RELEASE_CHECKLIST_v0.2.0/#package-build","title":"Package Build","text":"<ul> <li>[ ] Clean previous build artifacts</li> <li>[ ] Build new package with version 0.2.0</li> <li>[ ] Verify package contents include all new modules</li> <li>[ ] Test wheel installation in isolated environment</li> </ul>"},{"location":"releases/RELEASE_CHECKLIST_v0.2.0/#github-release","title":"GitHub Release","text":"<ul> <li>[ ] Create git tag: <code>v0.2.0</code></li> <li>[ ] Push tag to GitHub</li> <li>[ ] Create GitHub release with:</li> <li>Release title: \"bruno-llm v0.2.0 - Embedding Support &amp; Enhanced Bruno-Core Integration\"</li> <li>Description highlighting embedding functionality from CHANGELOG.md</li> <li>Attach distribution files (.whl and .tar.gz)</li> <li>Mark as \"latest release\"</li> </ul>"},{"location":"releases/RELEASE_CHECKLIST_v0.2.0/#pypi-release-optional","title":"PyPI Release (Optional)","text":"<ul> <li>[ ] Upload to PyPI test environment first</li> <li>[ ] Test installation from PyPI test</li> <li>[ ] Upload to production PyPI</li> <li>[ ] Verify package page looks correct</li> </ul>"},{"location":"releases/RELEASE_CHECKLIST_v0.2.0/#post-release-tasks","title":"Post-Release Tasks","text":""},{"location":"releases/RELEASE_CHECKLIST_v0.2.0/#documentation-updates","title":"Documentation Updates","text":"<ul> <li>[ ] Update main README badges if needed</li> <li>[ ] Update documentation links</li> <li>[ ] Create migration guide from v0.1.0 to v0.2.0 (if needed)</li> </ul>"},{"location":"releases/RELEASE_CHECKLIST_v0.2.0/#community","title":"Community","text":"<ul> <li>[ ] Announce on GitHub Discussions</li> <li>[ ] Update examples repository if exists</li> <li>[ ] Notify bruno-core project of new embedding support</li> </ul>"},{"location":"releases/RELEASE_CHECKLIST_v0.2.0/#quality-gates","title":"Quality Gates","text":""},{"location":"releases/RELEASE_CHECKLIST_v0.2.0/#must-pass-before-release","title":"Must Pass Before Release","text":"<ol> <li>\u2705 All existing functionality still works (no breaking changes)</li> <li>\u2705 New embedding functionality works as documented</li> <li>\u2705 Bruno-core compatibility tests pass</li> <li>\u2705 Documentation is complete and accurate</li> <li>\u2705 Version numbers are consistent across all files</li> <li>[ ] Clean installation test passes</li> <li>[ ] Basic usage examples work in clean environment</li> </ol>"},{"location":"releases/RELEASE_CHECKLIST_v0.2.0/#success-criteria","title":"Success Criteria","text":"<ul> <li>Users can upgrade from v0.1.0 to v0.2.0 without code changes</li> <li>New embedding functionality is immediately usable</li> <li>Bruno-core integration works seamlessly</li> <li>All documentation examples are functional</li> </ul>"},{"location":"releases/RELEASE_CHECKLIST_v0.2.0/#notes-for-v020","title":"Notes for v0.2.0","text":"<p>Major New Features: - Complete embedding provider support (Ollama + OpenAI) - Bruno-core interface compatibility - Enhanced documentation with embedding patterns - RAG implementation examples - Semantic search capabilities</p> <p>Breaking Changes: None (fully backward compatible)</p> <p>Migration: No migration needed - v0.2.0 is a pure feature addition</p> <p>Future Roadmap: Enhanced for next release planning - Function calling support - Additional embedding providers - Advanced RAG utilities - Vector database integrations</p>"},{"location":"user-guide/overview/","title":"User Guide Overview","text":"<p>Complete guide to using bruno-llm with bruno-core framework.</p>"},{"location":"user-guide/overview/#what-is-bruno-llm","title":"What is bruno-llm?","text":"<p>bruno-llm provides production-ready LLM and embedding provider implementations that integrate with the bruno-core framework. It enables you to:</p> <ul> <li>Use multiple LLM providers with a unified interface</li> <li>Generate embeddings for semantic search and RAG applications</li> <li>Switch between local and cloud models</li> <li>Leverage advanced features like caching, streaming, and cost tracking</li> </ul>"},{"location":"user-guide/overview/#providers","title":"Providers","text":"<ul> <li>Ollama - Local LLM and embedding inference</li> <li>OpenAI - Cloud GPT models and embeddings</li> </ul>"},{"location":"user-guide/overview/#advanced-features","title":"Advanced Features","text":"<ul> <li>Embeddings - Semantic search and vector operations</li> <li>Response Caching - Cache LLM responses to reduce costs</li> <li>Context Management - Handle context window limits</li> <li>Streaming - Stream tokens as they're generated</li> <li>Cost Tracking - Track API costs per request</li> <li>Middleware - Add logging, validation, and custom processing</li> </ul>"},{"location":"user-guide/overview/#see-also","title":"See Also","text":"<ul> <li>API Reference</li> <li>Examples</li> </ul>"},{"location":"user-guide/advanced/caching/","title":"Response Caching","text":"<p>Documentation coming soon.</p>"},{"location":"user-guide/advanced/context/","title":"Context Management","text":"<p>Documentation coming soon.</p>"},{"location":"user-guide/advanced/cost-tracking/","title":"Cost Tracking","text":"<p>Documentation coming soon.</p>"},{"location":"user-guide/advanced/middleware/","title":"Middleware","text":"<p>Documentation coming soon.</p>"},{"location":"user-guide/advanced/streaming/","title":"Streaming","text":"<p>Documentation coming soon.</p>"},{"location":"user-guide/providers/ollama/","title":"Ollama Provider Guide","text":"<p>The Ollama provider enables local LLM and embedding model execution, providing privacy-focused AI capabilities without external dependencies or usage costs.</p>"},{"location":"user-guide/providers/ollama/#overview","title":"Overview","text":"<ul> <li>Privacy First: All processing happens locally</li> <li>No Usage Costs: Free to run after initial setup</li> <li>No Rate Limits: Process as much as your hardware allows</li> <li>Offline Capable: Works without internet connection</li> <li>Multiple Models: Support for various LLM and embedding models</li> </ul>"},{"location":"user-guide/providers/ollama/#quick-setup","title":"Quick Setup","text":""},{"location":"user-guide/providers/ollama/#1-install-ollama","title":"1. Install Ollama","text":"<p>macOS/Linux: <pre><code>curl -fsSL https://ollama.ai/install.sh | sh\n</code></pre></p> <p>Windows: Download from ollama.ai</p>"},{"location":"user-guide/providers/ollama/#2-start-ollama-service","title":"2. Start Ollama Service","text":"<pre><code>ollama serve\n</code></pre>"},{"location":"user-guide/providers/ollama/#3-pull-models","title":"3. Pull Models","text":"<pre><code># LLM models\nollama pull llama2\nollama pull mistral\n\n# Embedding models\nollama pull nomic-embed-text\nollama pull mxbai-embed-large\n</code></pre>"},{"location":"user-guide/providers/ollama/#llm-usage","title":"LLM Usage","text":""},{"location":"user-guide/providers/ollama/#basic-configuration","title":"Basic Configuration","text":"<pre><code>from bruno_llm.providers.ollama import OllamaProvider\n\n# Create provider\nllm = OllamaProvider(\n    base_url=\"http://localhost:11434\",\n    model=\"llama2\"\n)\n</code></pre>"},{"location":"user-guide/providers/ollama/#factory-pattern","title":"Factory Pattern","text":"<pre><code>from bruno_llm.factory import LLMFactory\n\n# Direct creation\nllm = LLMFactory.create(\"ollama\", {\n    \"base_url\": \"http://localhost:11434\",\n    \"model\": \"mistral\"\n})\n\n# From environment\nllm = LLMFactory.create_from_env(\"ollama\")\n</code></pre>"},{"location":"user-guide/providers/ollama/#environment-variables","title":"Environment Variables","text":"<pre><code>export OLLAMA_BASE_URL=http://localhost:11434\nexport OLLAMA_MODEL=llama2\nexport OLLAMA_TIMEOUT=60.0\n</code></pre>"},{"location":"user-guide/providers/ollama/#embedding-usage","title":"Embedding Usage","text":""},{"location":"user-guide/providers/ollama/#basic-configuration_1","title":"Basic Configuration","text":"<pre><code>from bruno_llm.providers.ollama import OllamaEmbeddingProvider\n\n# Create embedding provider\nembedder = OllamaEmbeddingProvider(\n    base_url=\"http://localhost:11434\",\n    model=\"nomic-embed-text\"\n)\n</code></pre>"},{"location":"user-guide/providers/ollama/#factory-pattern_1","title":"Factory Pattern","text":"<pre><code>from bruno_llm.embedding_factory import EmbeddingFactory\n\n# Direct creation\nembedder = EmbeddingFactory.create(\"ollama\", {\n    \"base_url\": \"http://localhost:11434\",\n    \"model\": \"mxbai-embed-large\"\n})\n\n# From environment\nembedder = EmbeddingFactory.create_from_env(\"ollama\")\n</code></pre>"},{"location":"user-guide/providers/ollama/#environment-variables_1","title":"Environment Variables","text":"<pre><code>export OLLAMA_BASE_URL=http://localhost:11434\nexport OLLAMA_EMBEDDING_MODEL=nomic-embed-text\nexport EMBEDDING_BATCH_SIZE=32\n</code></pre>"},{"location":"user-guide/providers/ollama/#advanced-configuration","title":"Advanced Configuration","text":""},{"location":"user-guide/providers/ollama/#custom-parameters","title":"Custom Parameters","text":"<pre><code>from bruno_llm.providers.ollama import OllamaConfig\n\nconfig = OllamaConfig(\n    base_url=\"http://localhost:11434\",\n    model=\"llama2:13b\",\n    timeout=120.0,\n    keep_alive=\"10m\",\n    num_ctx=4096,\n    temperature=0.8,\n    top_k=40,\n    top_p=0.9\n)\n\nllm = OllamaProvider(config=config)\n</code></pre>"},{"location":"user-guide/providers/ollama/#performance-tuning","title":"Performance Tuning","text":"<pre><code># For speed (lower quality)\nfast_config = OllamaConfig(\n    model=\"llama2\",\n    num_ctx=2048,\n    num_predict=256,\n    temperature=0.3\n)\n\n# For quality (slower)\nquality_config = OllamaConfig(\n    model=\"llama2:13b\",\n    num_ctx=4096,\n    temperature=0.7,\n    top_p=0.95\n)\n</code></pre>"},{"location":"user-guide/providers/ollama/#popular-models","title":"Popular Models","text":""},{"location":"user-guide/providers/ollama/#llm-models","title":"LLM Models","text":"Model Size Best For <code>llama2</code> 7B General purpose <code>llama2:13b</code> 13B Better reasoning <code>mistral</code> 7B Fast, efficient <code>codellama</code> 7B Code generation <code>neural-chat</code> 7B Conversations"},{"location":"user-guide/providers/ollama/#embedding-models","title":"Embedding Models","text":"Model Dimensions Best For <code>nomic-embed-text</code> 768 General purpose <code>mxbai-embed-large</code> 1024 High accuracy <code>snowflake-arctic-embed</code> 1024 Retrieval tasks <code>all-minilm</code> 384 Compact, fast"},{"location":"user-guide/providers/ollama/#complete-examples","title":"Complete Examples","text":"<p>See Ollama Provider API Documentation for comprehensive examples including:</p> <ul> <li>Installation and setup</li> <li>Model management</li> <li>Performance optimization</li> <li>Error handling</li> <li>Integration patterns</li> <li>Troubleshooting</li> </ul>"},{"location":"user-guide/providers/ollama/#troubleshooting","title":"Troubleshooting","text":""},{"location":"user-guide/providers/ollama/#common-issues","title":"Common Issues","text":"<p>Ollama not running: <pre><code># Check if running\ncurl http://localhost:11434/api/tags\n\n# Start service\nollama serve\n</code></pre></p> <p>Model not found: <pre><code># List models\nollama list\n\n# Pull model\nollama pull llama2\nollama pull nomic-embed-text\n</code></pre></p> <p>Slow performance: - Use smaller models - Reduce context window - Enable GPU if available</p> <p>For detailed troubleshooting, see the main troubleshooting guide.</p>"},{"location":"user-guide/providers/openai/","title":"OpenAI Provider Guide","text":"<p>The OpenAI provider enables access to GPT models and embedding services through OpenAI's API, providing state-of-the-art language models and high-quality embeddings.</p>"},{"location":"user-guide/providers/openai/#overview","title":"Overview","text":"<ul> <li>Latest Models: Access to GPT-4, GPT-3.5, and latest embedding models</li> <li>High Quality: Industry-leading model performance</li> <li>Scalable: Handle high-volume production workloads</li> <li>Rich Features: Function calling, fine-tuning, advanced parameters</li> </ul>"},{"location":"user-guide/providers/openai/#quick-setup","title":"Quick Setup","text":""},{"location":"user-guide/providers/openai/#1-get-api-key","title":"1. Get API Key","text":"<ol> <li>Sign up at platform.openai.com</li> <li>Navigate to API Keys section</li> <li>Create a new API key</li> <li>Note your Organization ID (optional)</li> </ol>"},{"location":"user-guide/providers/openai/#2-install-dependencies","title":"2. Install Dependencies","text":"<pre><code>pip install bruno-llm[openai]\n</code></pre>"},{"location":"user-guide/providers/openai/#3-configure-environment","title":"3. Configure Environment","text":"<pre><code>export OPENAI_API_KEY=sk-your-key-here\nexport OPENAI_MODEL=gpt-4\nexport OPENAI_ORG_ID=org-your-org-id  # Optional\n</code></pre>"},{"location":"user-guide/providers/openai/#llm-usage","title":"LLM Usage","text":""},{"location":"user-guide/providers/openai/#basic-configuration","title":"Basic Configuration","text":"<pre><code>from bruno_llm.providers.openai import OpenAIProvider\n\n# Create provider\nllm = OpenAIProvider(\n    api_key=\"sk-your-key-here\",\n    model=\"gpt-4\"\n)\n</code></pre>"},{"location":"user-guide/providers/openai/#factory-pattern","title":"Factory Pattern","text":"<pre><code>from bruno_llm.factory import LLMFactory\n\n# Direct creation\nllm = LLMFactory.create(\"openai\", {\n    \"api_key\": \"sk-your-key-here\",\n    \"model\": \"gpt-3.5-turbo\",\n    \"organization\": \"org-your-org-id\"\n})\n\n# From environment\nllm = LLMFactory.create_from_env(\"openai\")\n</code></pre>"},{"location":"user-guide/providers/openai/#available-llm-models","title":"Available LLM Models","text":"Model Context Best For Cost <code>gpt-4</code> 8K Complex reasoning $$$ <code>gpt-4-turbo-preview</code> 128K Large context $$ <code>gpt-3.5-turbo</code> 4K Fast, general use $ <code>gpt-3.5-turbo-16k</code> 16K Longer context $$"},{"location":"user-guide/providers/openai/#embedding-usage","title":"Embedding Usage","text":""},{"location":"user-guide/providers/openai/#basic-configuration_1","title":"Basic Configuration","text":"<pre><code>from bruno_llm.providers.openai import OpenAIEmbeddingProvider\n\n# Create embedding provider\nembedder = OpenAIEmbeddingProvider(\n    api_key=\"sk-your-key-here\",\n    model=\"text-embedding-3-small\"\n)\n</code></pre>"},{"location":"user-guide/providers/openai/#factory-pattern_1","title":"Factory Pattern","text":"<pre><code>from bruno_llm.embedding_factory import EmbeddingFactory\n\n# Direct creation\nembedder = EmbeddingFactory.create(\"openai\", {\n    \"api_key\": \"sk-your-key-here\",\n    \"model\": \"text-embedding-3-large\"\n})\n\n# From environment\nembedder = EmbeddingFactory.create_from_env(\"openai\")\n</code></pre>"},{"location":"user-guide/providers/openai/#available-embedding-models","title":"Available Embedding Models","text":"Model Dimensions Max Input Cost/1M tokens Best For <code>text-embedding-3-small</code> 1536 8191 $0.02 Cost-effective <code>text-embedding-3-large</code> 3072 8191 $0.13 High performance <code>text-embedding-ada-002</code> 1536 8191 $0.10 Legacy, stable"},{"location":"user-guide/providers/openai/#advanced-configuration","title":"Advanced Configuration","text":""},{"location":"user-guide/providers/openai/#custom-parameters","title":"Custom Parameters","text":"<pre><code>from bruno_llm.providers.openai import OpenAIConfig\n\nconfig = OpenAIConfig(\n    api_key=\"sk-your-key-here\",\n    model=\"gpt-4\",\n    temperature=0.8,\n    max_tokens=1000,\n    top_p=0.9,\n    frequency_penalty=0.1,\n    presence_penalty=0.1,\n    timeout=30.0\n)\n\nllm = OpenAIProvider(config=config)\n</code></pre>"},{"location":"user-guide/providers/openai/#cost-optimization","title":"Cost Optimization","text":"<pre><code># Use cheaper models for simple tasks\ncheap_llm = OpenAIProvider(\n    api_key=\"sk-your-key-here\",\n    model=\"gpt-3.5-turbo\"  # Much cheaper than GPT-4\n)\n\n# Use efficient embedding model\ncheap_embedder = OpenAIEmbeddingProvider(\n    api_key=\"sk-your-key-here\",\n    model=\"text-embedding-3-small\"  # 5x cheaper than ada-002\n)\n</code></pre>"},{"location":"user-guide/providers/openai/#function-calling","title":"Function Calling","text":"<pre><code>functions = [{\n    \"name\": \"get_weather\",\n    \"description\": \"Get weather for a location\",\n    \"parameters\": {\n        \"type\": \"object\",\n        \"properties\": {\n            \"location\": {\"type\": \"string\"}\n        }\n    }\n}]\n\nresponse = await llm.generate(\n    messages=messages,\n    functions=functions,\n    function_call=\"auto\"\n)\n</code></pre>"},{"location":"user-guide/providers/openai/#environment-configuration","title":"Environment Configuration","text":""},{"location":"user-guide/providers/openai/#required-variables","title":"Required Variables","text":"<pre><code>export OPENAI_API_KEY=sk-your-key-here\n</code></pre>"},{"location":"user-guide/providers/openai/#optional-variables","title":"Optional Variables","text":"<pre><code># Organization (for team accounts)\nexport OPENAI_ORG_ID=org-your-org-id\n\n# Model selection\nexport OPENAI_MODEL=gpt-4\nexport OPENAI_EMBEDDING_MODEL=text-embedding-3-small\n\n# Performance tuning\nexport OPENAI_TIMEOUT=30.0\nexport OPENAI_MAX_RETRIES=3\n\n# Cost control\nexport OPENAI_MAX_TOKENS=1000\nexport OPENAI_TEMPERATURE=0.7\n</code></pre>"},{"location":"user-guide/providers/openai/#production-considerations","title":"Production Considerations","text":""},{"location":"user-guide/providers/openai/#rate-limiting","title":"Rate Limiting","text":"<pre><code>from bruno_llm.base import RateLimiter\n\n# Respect OpenAI rate limits\nlimiter = RateLimiter(requests_per_minute=3500)  # Adjust based on your tier\n\nasync def safe_generate(messages):\n    async with limiter:\n        return await llm.generate(messages)\n</code></pre>"},{"location":"user-guide/providers/openai/#cost-monitoring","title":"Cost Monitoring","text":"<pre><code>from bruno_llm.base import CostTracker\n\n# Track costs per request\ncost_tracker = CostTracker(\n    provider_name=\"openai\",\n    pricing={\n        \"gpt-4\": {\"input\": 0.03, \"output\": 0.06},\n        \"gpt-3.5-turbo\": {\"input\": 0.001, \"output\": 0.002}\n    }\n)\n\n# Costs are automatically tracked\nresponse = await llm.generate(messages)\ndaily_cost = cost_tracker.get_daily_cost()\n</code></pre>"},{"location":"user-guide/providers/openai/#error-handling","title":"Error Handling","text":"<pre><code>from bruno_llm.exceptions import (\n    AuthenticationError,\n    RateLimitError,\n    ContextLengthExceededError\n)\n\ntry:\n    response = await llm.generate(messages)\nexcept AuthenticationError:\n    print(\"Invalid API key\")\nexcept RateLimitError:\n    print(\"Rate limit hit, backing off...\")\n    await asyncio.sleep(60)\nexcept ContextLengthExceededError:\n    print(\"Message too long, truncating...\")\n</code></pre>"},{"location":"user-guide/providers/openai/#complete-examples","title":"Complete Examples","text":"<p>See OpenAI Provider API Documentation for comprehensive examples including:</p> <ul> <li>Model configurations and features</li> <li>Advanced parameters and fine-tuning</li> <li>Cost optimization strategies</li> <li>Performance monitoring</li> <li>Integration patterns</li> <li>Best practices</li> </ul>"},{"location":"user-guide/providers/openai/#troubleshooting","title":"Troubleshooting","text":""},{"location":"user-guide/providers/openai/#common-issues","title":"Common Issues","text":"<p>Authentication errors: <pre><code># Verify API key\nimport openai\nopenai.api_key = \"sk-your-key-here\"\ntry:\n    models = openai.Model.list()\n    print(\"API key is valid\")\nexcept openai.error.AuthenticationError:\n    print(\"Invalid API key\")\n</code></pre></p> <p>Rate limit exceeded: <pre><code># Add retry logic with backoff\nimport asyncio\nfrom bruno_llm.exceptions import RateLimitError\n\nasync def retry_on_rate_limit(func, max_retries=3):\n    for attempt in range(max_retries):\n        try:\n            return await func()\n        except RateLimitError:\n            if attempt &lt; max_retries - 1:\n                wait_time = 2 ** attempt\n                await asyncio.sleep(wait_time)\n            else:\n                raise\n</code></pre></p> <p>High costs: - Use <code>gpt-3.5-turbo</code> instead of <code>gpt-4</code> for simple tasks - Set reasonable <code>max_tokens</code> limits - Implement response caching - Use <code>text-embedding-3-small</code> for embeddings</p> <p>For detailed troubleshooting, see the main troubleshooting guide.</p>"}]}