{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"bruno-llm","text":"<p>Production-ready LLM provider implementations for the bruno-core framework.</p>"},{"location":"#features","title":"Features","text":"<p>\u2728 Multiple Providers - Ollama (local models) - OpenAI (GPT models) - More coming soon (Claude, Gemini, Azure)</p> <p>\ud83d\ude80 Advanced Features - Response caching (100% test coverage) - Context window management (96% coverage) - Stream aggregation (93% coverage) - Cost tracking (98% coverage) - Middleware system (93% coverage)</p> <p>\ud83d\udee0\ufe0f Production Ready - 203 comprehensive tests - 91% code coverage - Type hints throughout - Async-first design</p>"},{"location":"#quick-start","title":"Quick Start","text":""},{"location":"#installation","title":"Installation","text":"<pre><code>pip install bruno-llm\n</code></pre>"},{"location":"#basic-usage","title":"Basic Usage","text":"<pre><code>from bruno_core.models import Message, MessageRole\nfrom bruno_llm import LLMFactory\n\n# Create provider\nllm = LLMFactory.create(\"ollama\", {\"model\": \"llama2\"})\n\n# Generate response\nmessages = [Message(role=MessageRole.USER, content=\"Hello!\")]\nresponse = await llm.generate(messages)\nprint(response)\n</code></pre>"},{"location":"#documentation-sections","title":"Documentation Sections","text":"<ul> <li>Getting Started - Installation and quick start guide</li> <li>User Guide - Comprehensive usage documentation</li> <li>API Reference - Complete API documentation</li> <li>Development - Contributing guidelines</li> </ul>"},{"location":"#links","title":"Links","text":"<ul> <li>GitHub: meggy-ai/bruno-llm</li> <li>Issues: Report a bug</li> <li>Changelog: View releases</li> </ul>"},{"location":"#license","title":"License","text":"<p>MIT License - see LICENSE for details.</p>"},{"location":"PRE_COMMIT_SETUP/","title":"Pre-commit Hooks Setup","text":"<p>Pre-commit hooks automatically check your code before each commit to catch issues that would fail CI.</p>"},{"location":"PRE_COMMIT_SETUP/#why-pre-commit-hooks","title":"Why Pre-commit Hooks?","text":"<p>Without pre-commit hooks: - \u274c Linting errors discovered in CI (after push) - \u274c Formatting issues found during code review - \u274c Type errors caught late in the process - \u274c Wasted CI minutes on preventable failures</p> <p>With pre-commit hooks: - \u2705 Errors caught before commit - \u2705 Code automatically formatted - \u2705 Fast feedback loop (seconds, not minutes) - \u2705 CI always passes</p>"},{"location":"PRE_COMMIT_SETUP/#installation","title":"Installation","text":""},{"location":"PRE_COMMIT_SETUP/#one-time-setup","title":"One-time Setup","text":"<pre><code># 1. Install pre-commit (if not already installed)\npip install pre-commit\n\n# 2. Install the git hooks\npre-commit install\n\n# 3. (Optional) Run on all files to check current state\npre-commit run --all-files\n</code></pre>"},{"location":"PRE_COMMIT_SETUP/#what-gets-checked","title":"What Gets Checked","text":"<p>On every <code>git commit</code>, these hooks run automatically:</p> <ol> <li>Basic Checks</li> <li>Remove trailing whitespace</li> <li>Fix end-of-file issues</li> <li>Validate YAML/JSON/TOML syntax</li> <li>Check for large files</li> <li>Detect merge conflicts</li> <li> <p>Detect private keys</p> </li> <li> <p>Code Quality</p> </li> <li>Ruff format - Auto-format code</li> <li>Ruff lint - Check code quality (auto-fix when possible)</li> <li>MyPy - Type checking (relaxed settings)</li> </ol>"},{"location":"PRE_COMMIT_SETUP/#usage","title":"Usage","text":""},{"location":"PRE_COMMIT_SETUP/#normal-workflow","title":"Normal Workflow","text":"<pre><code># Make changes\nvim bruno_llm/some_file.py\n\n# Stage changes\ngit add bruno_llm/some_file.py\n\n# Commit (hooks run automatically)\ngit commit -m \"Your message\"\n</code></pre> <p>If hooks fail: - Code is automatically fixed (formatting, some lint issues) - You'll see what failed - Stage the auto-fixes and commit again:</p> <pre><code>git add -u\ngit commit -m \"Your message\"\n</code></pre>"},{"location":"PRE_COMMIT_SETUP/#bypass-hooks-emergency-only","title":"Bypass Hooks (Emergency Only)","text":"<pre><code># Skip hooks (not recommended!)\ngit commit --no-verify -m \"Emergency fix\"\n</code></pre>"},{"location":"PRE_COMMIT_SETUP/#run-manually","title":"Run Manually","text":"<pre><code># Run all hooks on staged files\npre-commit run\n\n# Run all hooks on all files\npre-commit run --all-files\n\n# Run specific hook\npre-commit run ruff --all-files\npre-commit run mypy --all-files\n</code></pre>"},{"location":"PRE_COMMIT_SETUP/#update-hooks","title":"Update Hooks","text":"<pre><code># Update to latest versions\npre-commit autoupdate\n\n# Re-install after changes\npre-commit install\n</code></pre>"},{"location":"PRE_COMMIT_SETUP/#configuration","title":"Configuration","text":"<p>Hooks are configured in <code>.pre-commit-config.yaml</code>:</p> <pre><code>repos:\n  - repo: https://github.com/astral-sh/ruff-pre-commit\n    rev: v0.1.0\n    hooks:\n      - id: ruff\n        args: [--fix]           # Auto-fix issues\n      - id: ruff-format          # Format code\n\n  - repo: https://github.com/pre-commit/mirrors-mypy\n    rev: v1.5.1\n    hooks:\n      - id: mypy\n        args: [--ignore-missing-imports]  # Match CI settings\n        exclude: ^tests/                   # Skip test files\n</code></pre>"},{"location":"PRE_COMMIT_SETUP/#troubleshooting","title":"Troubleshooting","text":""},{"location":"PRE_COMMIT_SETUP/#hooks-not-running","title":"Hooks Not Running","text":"<pre><code># Verify installation\npre-commit --version\n\n# Reinstall hooks\npre-commit uninstall\npre-commit install\n</code></pre>"},{"location":"PRE_COMMIT_SETUP/#hooks-too-slow","title":"Hooks Too Slow","text":"<pre><code># Skip specific hooks temporarily\nSKIP=mypy git commit -m \"Message\"\n\n# Or disable specific hooks in .pre-commit-config.yaml\n</code></pre>"},{"location":"PRE_COMMIT_SETUP/#false-positives","title":"False Positives","text":"<p>If a hook incorrectly flags an issue:</p> <ol> <li>Fix the issue (preferred)</li> <li>Add exception in config file</li> <li>Disable hook in <code>.pre-commit-config.yaml</code> (last resort)</li> </ol>"},{"location":"PRE_COMMIT_SETUP/#ci-alignment","title":"CI Alignment","text":"<p>Pre-commit hooks are configured to match CI workflows:</p> Check Pre-commit CI (.github/workflows) Formatting \u2705 ruff format \u2705 ruff format --check Linting \u2705 ruff check \u2705 ruff check Type checking \u2705 mypy (relaxed) \u2705 mypy (relaxed) Tests \u274c (too slow) \u2705 pytest"},{"location":"PRE_COMMIT_SETUP/#best-practices","title":"Best Practices","text":"<ol> <li>Always install hooks when cloning the repo</li> <li>Don't bypass hooks unless emergency</li> <li>Update regularly with <code>pre-commit autoupdate</code></li> <li>Fix issues rather than suppressing them</li> <li>Run manually on all files after config changes</li> </ol>"},{"location":"PRE_COMMIT_SETUP/#why-some-checks-are-skipped","title":"Why Some Checks Are Skipped","text":"<ul> <li>Tests - Too slow for pre-commit (run locally with <code>pytest</code>)</li> <li>Coverage - Needs full test run</li> <li>Integration tests - Require external services</li> </ul> <p>These run in CI instead, which is fine since pre-commit catches 90% of issues.</p>"},{"location":"PRE_COMMIT_SETUP/#summary","title":"Summary","text":"<pre><code># Setup once\npip install pre-commit\npre-commit install\n\n# Then forget about it - it just works!\ngit commit -m \"Feature: Add new provider\"\n# Hooks run automatically \u2728\n</code></pre> <p>Pre-commit hooks are your first line of defense against CI failures. Install them and save yourself time! \ud83d\ude80</p>"},{"location":"USER_GUIDE/","title":"bruno-llm User Guide","text":"<p>Complete guide to using bruno-llm for LLM provider integration.</p>"},{"location":"USER_GUIDE/#table-of-contents","title":"Table of Contents","text":"<ol> <li>Installation</li> <li>Quick Start</li> <li>Provider Setup</li> <li>Basic Usage</li> <li>Advanced Features</li> <li>Best Practices</li> <li>Troubleshooting</li> </ol>"},{"location":"USER_GUIDE/#installation","title":"Installation","text":""},{"location":"USER_GUIDE/#basic-installation","title":"Basic Installation","text":"<pre><code>pip install bruno-llm\n</code></pre> <p>This includes: - bruno-core framework - Ollama provider support - All base utilities</p>"},{"location":"USER_GUIDE/#with-openai-support","title":"With OpenAI Support","text":"<pre><code>pip install bruno-llm[openai]\n</code></pre> <p>Additional packages: - <code>openai</code> - Official OpenAI Python client - <code>tiktoken</code> - Accurate token counting for GPT models</p>"},{"location":"USER_GUIDE/#development-installation","title":"Development Installation","text":"<pre><code>git clone https://github.com/meggy-ai/bruno-llm.git\ncd bruno-llm\npip install -e \".[dev]\"\n</code></pre> <p>Includes testing and development tools.</p>"},{"location":"USER_GUIDE/#quick-start","title":"Quick Start","text":""},{"location":"USER_GUIDE/#hello-world-example","title":"Hello World Example","text":"<pre><code>import asyncio\nfrom bruno_llm import LLMFactory\nfrom bruno_core.models import Message, MessageRole\n\nasync def main():\n    # Create provider\n    llm = LLMFactory.create(\"ollama\", {\"model\": \"llama2\"})\n\n    # Create message\n    messages = [\n        Message(role=MessageRole.USER, content=\"Hello! Who are you?\")\n    ]\n\n    # Generate response\n    response = await llm.generate(messages)\n    print(response)\n\n    # Clean up\n    await llm.close()\n\nasyncio.run(main())\n</code></pre>"},{"location":"USER_GUIDE/#streaming-example","title":"Streaming Example","text":"<pre><code>async def streaming_demo():\n    llm = LLMFactory.create(\"ollama\", {\"model\": \"llama2\"})\n\n    messages = [\n        Message(role=MessageRole.USER, content=\"Count from 1 to 10\")\n    ]\n\n    print(\"Response: \", end=\"\", flush=True)\n    async for chunk in llm.stream(messages):\n        print(chunk, end=\"\", flush=True)\n    print()\n\n    await llm.close()\n\nasyncio.run(streaming_demo())\n</code></pre>"},{"location":"USER_GUIDE/#provider-setup","title":"Provider Setup","text":""},{"location":"USER_GUIDE/#ollama","title":"Ollama","text":"<p>Installation:</p> <p>Visit ollama.ai and follow installation instructions.</p> <p>Starting Ollama:</p> <pre><code>ollama serve\n</code></pre> <p>Pulling Models:</p> <pre><code># General purpose\nollama pull llama2\nollama pull mistral\n\n# Code generation\nollama pull codellama\n\n# Larger models\nollama pull llama2:70b\n</code></pre> <p>Usage:</p> <pre><code>from bruno_llm import LLMFactory\n\nllm = LLMFactory.create(\"ollama\", {\n    \"base_url\": \"http://localhost:11434\",\n    \"model\": \"llama2\",\n    \"timeout\": 60.0\n})\n</code></pre> <p>Environment Variables:</p> <pre><code>export OLLAMA_BASE_URL=http://localhost:11434\nexport OLLAMA_MODEL=llama2\n</code></pre> <p>Then:</p> <pre><code>llm = LLMFactory.create_from_env(\"ollama\")\n</code></pre>"},{"location":"USER_GUIDE/#openai","title":"OpenAI","text":"<p>Get API Key:</p> <ol> <li>Sign up at platform.openai.com</li> <li>Navigate to API Keys</li> <li>Create new key</li> </ol> <p>Usage:</p> <pre><code>from bruno_llm import LLMFactory\n\nllm = LLMFactory.create(\"openai\", {\n    \"api_key\": \"sk-...\",\n    \"model\": \"gpt-4\",\n    \"organization\": \"org-...\"  # Optional\n})\n</code></pre> <p>Environment Variables:</p> <pre><code>export OPENAI_API_KEY=sk-...\nexport OPENAI_MODEL=gpt-4\nexport OPENAI_ORG_ID=org-...  # Optional\n</code></pre> <p>Then:</p> <pre><code>llm = LLMFactory.create_from_env(\"openai\")\n</code></pre> <p>Available Models:</p> <ul> <li><code>gpt-4</code> - Most capable</li> <li><code>gpt-4-turbo-preview</code> - Latest GPT-4 with larger context</li> <li><code>gpt-3.5-turbo</code> - Fast and cost-effective</li> </ul>"},{"location":"USER_GUIDE/#basic-usage","title":"Basic Usage","text":""},{"location":"USER_GUIDE/#creating-providers","title":"Creating Providers","text":"<p>Method 1: Factory Pattern (Recommended)</p> <pre><code>from bruno_llm import LLMFactory\n\nllm = LLMFactory.create(\n    provider=\"ollama\",\n    config={\"model\": \"llama2\"}\n)\n</code></pre> <p>Method 2: Direct Instantiation</p> <pre><code>from bruno_llm.providers.ollama import OllamaProvider\n\nllm = OllamaProvider(model=\"llama2\")\n</code></pre> <p>Method 3: From Environment</p> <pre><code>llm = LLMFactory.create_from_env(\"openai\")\n</code></pre>"},{"location":"USER_GUIDE/#message-format","title":"Message Format","text":"<p>bruno-llm uses <code>Message</code> objects from bruno-core:</p> <pre><code>from bruno_core.models import Message, MessageRole\n\n# User message\nuser_msg = Message(\n    role=MessageRole.USER,\n    content=\"What is Python?\"\n)\n\n# System message (sets context/behavior)\nsystem_msg = Message(\n    role=MessageRole.SYSTEM,\n    content=\"You are a helpful programming tutor.\"\n)\n\n# Assistant message (previous AI responses)\nassistant_msg = Message(\n    role=MessageRole.ASSISTANT,\n    content=\"Python is a programming language...\"\n)\n\n# Conversation\nmessages = [system_msg, user_msg]\n</code></pre>"},{"location":"USER_GUIDE/#generating-responses","title":"Generating Responses","text":"<p>Basic Generation:</p> <pre><code>response = await llm.generate(messages)\nprint(response)  # String response\n</code></pre> <p>With Parameters:</p> <pre><code>response = await llm.generate(\n    messages=messages,\n    max_tokens=500,           # Limit response length\n    temperature=0.7,          # Creativity (0.0 = deterministic, 2.0 = very creative)\n    top_p=0.9,               # Nucleus sampling\n    stop=[\"###\", \"END\"]      # Stop sequences\n)\n</code></pre> <p>Streaming Responses:</p> <pre><code>async for chunk in llm.stream(messages, max_tokens=200):\n    print(chunk, end=\"\", flush=True)\n</code></pre>"},{"location":"USER_GUIDE/#provider-methods","title":"Provider Methods","text":"<p>Check Connection:</p> <pre><code>if await llm.check_connection():\n    print(\"Provider is accessible\")\nelse:\n    print(\"Cannot connect to provider\")\n</code></pre> <p>List Available Models:</p> <pre><code>models = await llm.list_models()\nfor model in models:\n    print(f\"- {model}\")\n</code></pre> <p>Get Model Information:</p> <pre><code>info = llm.get_model_info()\nprint(f\"Provider: {info['provider']}\")\nprint(f\"Model: {info['model']}\")\nprint(f\"Context Window: {info.get('max_context_tokens', 'Unknown')}\")\n</code></pre> <p>Token Counting:</p> <pre><code>text = \"Hello, world!\"\ntokens = llm.get_token_count(text)\nprint(f\"Token count: {tokens}\")\n</code></pre> <p>System Prompts:</p> <pre><code># Set system prompt\nllm.set_system_prompt(\"You are a helpful assistant.\")\n\n# Get current system prompt\nprompt = llm.get_system_prompt()\n\n# System prompt is automatically added to messages\nmessages = [Message(role=MessageRole.USER, content=\"Hello\")]\nresponse = await llm.generate(messages)  # System prompt included\n</code></pre>"},{"location":"USER_GUIDE/#advanced-features","title":"Advanced Features","text":""},{"location":"USER_GUIDE/#response-caching","title":"Response Caching","text":"<p>Reduce API costs and latency by caching responses:</p> <pre><code>from bruno_llm import LLMFactory\nfrom bruno_llm.base import ResponseCache\n\nllm = LLMFactory.create(\"ollama\", {\"model\": \"llama2\"})\ncache = ResponseCache(\n    max_size=100,  # Maximum number of cached responses\n    ttl=300        # Time-to-live in seconds (5 minutes)\n)\n\nmessages = [Message(role=MessageRole.USER, content=\"What is 2+2?\")]\n\n# First request - cache miss\nresponse = await llm.generate(messages, temperature=0.0)\ncache.set(messages, response, temperature=0.0)\n\n# Second request - cache hit\ncached_response = cache.get(messages, temperature=0.0)\nif cached_response:\n    print(\"From cache!\")\n    response = cached_response\nelse:\n    response = await llm.generate(messages, temperature=0.0)\n    cache.set(messages, response, temperature=0.0)\n\n# Cache statistics\nstats = cache.get_stats()\nprint(f\"Hit rate: {stats['hit_rate']:.1%}\")\nprint(f\"Cache size: {stats['size']}/{stats['max_size']}\")\n</code></pre> <p>Cache Management:</p> <pre><code># Clear entire cache\ncache.clear()\n\n# Invalidate specific entry\ncache.invalidate(messages, temperature=0.0)\n\n# Clean up expired entries\nexpired_count = cache.cleanup_expired()\n\n# Get top entries by access count\ntop_entries = cache.get_top_entries(n=10)\n</code></pre>"},{"location":"USER_GUIDE/#context-window-management","title":"Context Window Management","text":"<p>Intelligently truncate conversations that exceed token limits:</p> <pre><code>from bruno_llm.base import (\n    ContextWindowManager,\n    ContextLimits,\n    TruncationStrategy\n)\n\n# Create context manager\ncontext_mgr = ContextWindowManager(\n    model=\"gpt-4\",  # Uses predefined limits for known models\n    limits=ContextLimits(\n        max_tokens=8000,\n        max_output_tokens=500  # Reserve tokens for response\n    ),\n    strategy=TruncationStrategy.SMART  # Preserve important messages\n)\n\n# Check if messages fit\nif context_mgr.check_limit(messages):\n    print(\"Within limit\")\nelse:\n    print(\"Exceeds limit, truncating...\")\n    messages = context_mgr.truncate(messages)\n\n# Get statistics\nstats = context_mgr.get_stats(messages)\nprint(f\"Input tokens: {stats['input_tokens']}\")\nprint(f\"Available output tokens: {stats['available_output_tokens']}\")\nprint(f\"Usage: {stats['usage_percent']:.1f}%\")\n</code></pre> <p>Truncation Strategies:</p> <ul> <li><code>OLDEST_FIRST</code>: Remove oldest messages first (keeps recent context)</li> <li><code>MIDDLE_OUT</code>: Remove middle messages (keeps beginning and end)</li> <li><code>SLIDING_WINDOW</code>: Keep most recent N messages</li> <li><code>SMART</code>: Preserve system messages and recent important messages</li> </ul> <p>Custom Limits:</p> <pre><code># For models without predefined limits\ncontext_mgr = ContextWindowManager(\n    model=\"custom-model\",\n    limits=ContextLimits(max_tokens=4096, max_output_tokens=256)\n)\n</code></pre>"},{"location":"USER_GUIDE/#stream-aggregation","title":"Stream Aggregation","text":"<p>Control how streaming chunks are batched:</p> <pre><code>from bruno_llm.base import StreamAggregator\n\n# Word-by-word aggregation\naggregator = StreamAggregator(strategy=\"word\")\nasync for word in aggregator.aggregate(llm.stream(messages)):\n    print(f\"[{word.strip()}]\", end=\" \")\n\n# Sentence-by-sentence\naggregator = StreamAggregator(strategy=\"sentence\")\nasync for sentence in aggregator.aggregate(llm.stream(messages)):\n    print(f\"\\nSentence: {sentence.strip()}\")\n\n# Fixed size chunks\naggregator = StreamAggregator(strategy=\"fixed\", chunk_size=10)\nasync for chunk in aggregator.aggregate(llm.stream(messages)):\n    print(chunk, end=\"\")\n\n# Time-based batching (wait up to N seconds)\naggregator = StreamAggregator(strategy=\"time\", interval=0.5)\nasync for batch in aggregator.aggregate(llm.stream(messages)):\n    print(f\"Batch: {batch}\")\n\n# Passthrough (no aggregation)\naggregator = StreamAggregator(strategy=\"passthrough\")\nasync for chunk in aggregator.aggregate(llm.stream(messages)):\n    print(chunk, end=\"\")\n</code></pre>"},{"location":"USER_GUIDE/#cost-tracking","title":"Cost Tracking","text":"<p>Monitor API usage and costs:</p> <pre><code>from bruno_llm import LLMFactory\n\nllm = LLMFactory.create_from_env(\"openai\")\n\n# Make some requests\nawait llm.generate(messages)\nawait llm.generate(messages)\n\n# Get usage report\nreport = llm.cost_tracker.get_usage_report()\nprint(f\"Total requests: {report['total_requests']}\")\nprint(f\"Total tokens: {report['total_tokens']}\")\nprint(f\"Total cost: ${report['total_cost']:.4f}\")\n\n# Model breakdown\nfor model, stats in report['model_breakdown'].items():\n    print(f\"{model}:\")\n    print(f\"  Requests: {stats['requests']}\")\n    print(f\"  Tokens: {stats['input_tokens']} in + {stats['output_tokens']} out\")\n    print(f\"  Cost: ${stats['cost']:.4f}\")\n\n# Export to CSV\nllm.cost_tracker.export_to_csv(\"usage_report.csv\")\n\n# Export to JSON\nllm.cost_tracker.export_to_json(\"usage_report.json\")\n\n# Time range report\nfrom datetime import datetime, timedelta\n\nstart = datetime.now() - timedelta(days=7)\nend = datetime.now()\nweekly_report = llm.cost_tracker.get_time_range_report(start, end)\n\n# Budget checking\nstatus = llm.cost_tracker.check_budget(\n    budget_limit=10.0,\n    warning_threshold=0.8  # Warn at 80%\n)\n\nif status['warning']:\n    print(f\"\u26a0\ufe0f Warning: {status['percent_used']:.1f}% of budget used\")\n\nif not status['within_budget']:\n    print(f\"\u274c Budget exceeded! ${status['total_spent']:.2f} / ${status['budget_limit']:.2f}\")\n</code></pre>"},{"location":"USER_GUIDE/#provider-fallback","title":"Provider Fallback","text":"<p>Try multiple providers in order:</p> <pre><code>from bruno_llm import LLMFactory\n\n# Try OpenAI, fallback to Ollama\nllm = await LLMFactory.create_with_fallback(\n    providers=[\"openai\", \"ollama\"],\n    configs=[\n        {\"api_key\": \"sk-...\", \"model\": \"gpt-4\"},\n        {\"model\": \"llama2\"}\n    ]\n)\n\n# The first successfully connected provider is used\ninfo = llm.get_model_info()\nprint(f\"Using provider: {info['provider']}\")\n\nresponse = await llm.generate(messages)\n</code></pre>"},{"location":"USER_GUIDE/#concurrent-requests","title":"Concurrent Requests","text":"<p>Handle multiple requests in parallel:</p> <pre><code>import asyncio\n\nasync def process_multiple():\n    llm = LLMFactory.create(\"ollama\", {\"model\": \"llama2\"})\n\n    # Create multiple message sets\n    message_sets = [\n        [Message(role=MessageRole.USER, content=f\"Tell me about topic {i}\")]\n        for i in range(5)\n    ]\n\n    # Process concurrently\n    tasks = [llm.generate(msgs) for msgs in message_sets]\n    responses = await asyncio.gather(*tasks, return_exceptions=True)\n\n    # Handle results\n    for i, response in enumerate(responses):\n        if isinstance(response, Exception):\n            print(f\"Request {i} failed: {response}\")\n        else:\n            print(f\"Request {i}: {response[:50]}...\")\n\n    await llm.close()\n\nasyncio.run(process_multiple())\n</code></pre>"},{"location":"USER_GUIDE/#best-practices","title":"Best Practices","text":""},{"location":"USER_GUIDE/#1-resource-management","title":"1. Resource Management","text":"<p>Always close providers:</p> <pre><code># Using context manager (recommended)\nasync def with_context_manager():\n    llm = LLMFactory.create(\"ollama\", {\"model\": \"llama2\"})\n    async with llm:\n        response = await llm.generate(messages)\n    # Automatically closed\n\n# Manual cleanup\ntry:\n    llm = LLMFactory.create(\"ollama\", {\"model\": \"llama2\"})\n    response = await llm.generate(messages)\nfinally:\n    await llm.close()\n</code></pre>"},{"location":"USER_GUIDE/#2-error-handling","title":"2. Error Handling","text":"<p>Handle provider-specific errors:</p> <pre><code>from bruno_llm.exceptions import (\n    ModelNotFoundError,\n    RateLimitError,\n    AuthenticationError,\n    ContextLengthExceededError\n)\n\ntry:\n    response = await llm.generate(messages)\nexcept ModelNotFoundError as e:\n    print(f\"Model not available: {e}\")\n    # Try alternative model\nexcept RateLimitError as e:\n    print(f\"Rate limited: {e}\")\n    # Wait and retry\n    await asyncio.sleep(60)\nexcept AuthenticationError as e:\n    print(f\"Authentication failed: {e}\")\n    # Check API key\nexcept ContextLengthExceededError as e:\n    print(f\"Context too long: {e}\")\n    # Truncate messages\n    messages = context_mgr.truncate(messages)\n    response = await llm.generate(messages)\nexcept Exception as e:\n    print(f\"Unexpected error: {e}\")\n</code></pre>"},{"location":"USER_GUIDE/#3-configuration-management","title":"3. Configuration Management","text":"<p>Use environment variables for sensitive data:</p> <pre><code>import os\nfrom dotenv import load_dotenv\n\n# Load from .env file\nload_dotenv()\n\n# Use environment variables\nllm = LLMFactory.create(\"openai\", {\n    \"api_key\": os.getenv(\"OPENAI_API_KEY\"),\n    \"model\": os.getenv(\"OPENAI_MODEL\", \"gpt-4\")\n})\n</code></pre> <p>.env file: <pre><code>OPENAI_API_KEY=sk-...\nOPENAI_MODEL=gpt-4\nOLLAMA_BASE_URL=http://localhost:11434\n</code></pre></p>"},{"location":"USER_GUIDE/#4-temperature-settings","title":"4. Temperature Settings","text":"<p>Choose appropriate temperature based on use case:</p> <pre><code># Deterministic (temperature=0.0) - for factual, consistent responses\nresponse = await llm.generate(messages, temperature=0.0)\n\n# Balanced (temperature=0.7) - good default\nresponse = await llm.generate(messages, temperature=0.7)\n\n# Creative (temperature=1.5) - for brainstorming, creative writing\nresponse = await llm.generate(messages, temperature=1.5)\n</code></pre>"},{"location":"USER_GUIDE/#5-token-management","title":"5. Token Management","text":"<p>Monitor token usage:</p> <pre><code># Check message token count before sending\ntotal_tokens = sum(llm.get_token_count(msg.content) for msg in messages)\nprint(f\"Request will use approximately {total_tokens} tokens\")\n\n# Limit response length\nresponse = await llm.generate(messages, max_tokens=500)\n</code></pre>"},{"location":"USER_GUIDE/#6-caching-strategy","title":"6. Caching Strategy","text":"<p>Use caching for repeated queries:</p> <pre><code># Cache deterministic responses (temperature=0)\ncache = ResponseCache(max_size=1000, ttl=3600)  # 1 hour TTL\n\n# Check cache first\ncached = cache.get(messages, temperature=0.0)\nif cached:\n    response = cached\nelse:\n    response = await llm.generate(messages, temperature=0.0)\n    cache.set(messages, response, temperature=0.0)\n</code></pre> <p>Don't cache: - Creative/random responses (temperature &gt; 0) - Time-sensitive information - User-specific data</p>"},{"location":"USER_GUIDE/#troubleshooting","title":"Troubleshooting","text":""},{"location":"USER_GUIDE/#ollama-issues","title":"Ollama Issues","text":"<p>\"Cannot connect to Ollama\"</p> <pre><code># Check if Ollama is running\nimport httpx\n\ntry:\n    response = httpx.get(\"http://localhost:11434/api/tags\", timeout=5.0)\n    print(f\"Ollama is running. Models: {response.json()}\")\nexcept Exception as e:\n    print(f\"Ollama not accessible: {e}\")\n    print(\"Start Ollama with: ollama serve\")\n</code></pre> <p>\"Model not found\"</p> <pre><code># List installed models\nollama list\n\n# Pull missing model\nollama pull llama2\n</code></pre> <p>Slow responses</p> <ul> <li>Use smaller models (llama2:7b instead of llama2:70b)</li> <li>Reduce max_tokens</li> <li>Use GPU if available</li> </ul>"},{"location":"USER_GUIDE/#openai-issues","title":"OpenAI Issues","text":"<p>\"Authentication failed\"</p> <pre><code># Verify API key\nimport openai\n\ntry:\n    openai.api_key = \"sk-...\"\n    models = openai.Model.list()\n    print(\"API key is valid\")\nexcept openai.error.AuthenticationError:\n    print(\"Invalid API key\")\n</code></pre> <p>\"Rate limit exceeded\"</p> <pre><code>from bruno_llm.base import RateLimiter\n\n# Add rate limiting\nlimiter = RateLimiter(requests_per_minute=50)\n\nasync def rate_limited_request():\n    async with limiter:\n        return await llm.generate(messages)\n</code></pre> <p>High costs</p> <ul> <li>Use gpt-3.5-turbo instead of gpt-4</li> <li>Reduce max_tokens</li> <li>Implement caching</li> <li>Monitor with cost_tracker</li> </ul>"},{"location":"USER_GUIDE/#general-issues","title":"General Issues","text":"<p>Import errors</p> <pre><code># Reinstall package\npip uninstall bruno-llm\npip install bruno-llm\n\n# Or install from source\ngit clone https://github.com/meggy-ai/bruno-llm.git\ncd bruno-llm\npip install -e .\n</code></pre> <p>Timeout errors</p> <pre><code># Increase timeout\nllm = LLMFactory.create(\"ollama\", {\n    \"model\": \"llama2\",\n    \"timeout\": 120.0  # 2 minutes\n})\n</code></pre> <p>Memory issues</p> <ul> <li>Process in batches</li> <li>Clear cache periodically</li> <li>Use streaming for large responses</li> </ul>"},{"location":"USER_GUIDE/#next-steps","title":"Next Steps","text":"<ul> <li>Read examples/ for more complete examples</li> <li>Check API Reference for detailed method documentation</li> <li>See TESTING.md for testing guide</li> <li>Join our community for support</li> </ul> <p>Need help? Open an issue or email contact@meggy.ai</p>"},{"location":"about/changelog/","title":"Changelog","text":"<p>See CHANGELOG.md for complete version history.</p>"},{"location":"about/changelog/#latest-release","title":"Latest Release","text":""},{"location":"about/changelog/#v010-2025-12-09","title":"v0.1.0 (2025-12-09)","text":"<p>Initial release with Ollama and OpenAI providers, comprehensive test coverage, and production-ready features.</p>"},{"location":"about/license/","title":"License","text":"<p>bruno-llm is released under the MIT License.</p> <p>See LICENSE for full text.</p>"},{"location":"about/license/#mit-license-summary","title":"MIT License Summary","text":"<p>Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software.</p> <p>THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND.</p>"},{"location":"api/base/","title":"Base Utilities","text":""},{"location":"api/base/#bruno_llm.base","title":"<code>bruno_llm.base</code>","text":"<p>Base utilities and common functionality for LLM providers.</p> <p>This module provides shared utilities that all provider implementations can use, including token counting, rate limiting, retry logic, cost tracking, caching, streaming, context management, and middleware.</p>"},{"location":"api/base/#bruno_llm.base.BaseProvider","title":"<code>BaseProvider</code>","text":"<p>               Bases: <code>LLMInterface</code>, <code>ABC</code></p> <p>Base class for LLM provider implementations.</p> <p>Provides common functionality that all providers can use: - Retry logic with exponential backoff - Rate limiting - Cost tracking - Error handling patterns</p> <p>Subclasses must implement the LLMInterface methods: - generate() - stream() - get_token_count() - check_connection() - list_models() - get_model_info() - set_system_prompt() - get_system_prompt()</p> Example <p>class MyProvider(BaseProvider): ...     async def generate(self, messages, kwargs): ...         return await self._with_retry( ...             self._generate_impl(messages, kwargs) ...         )</p> Source code in <code>bruno_llm/base/base_provider.py</code> <pre><code>class BaseProvider(LLMInterface, ABC):\n    \"\"\"\n    Base class for LLM provider implementations.\n\n    Provides common functionality that all providers can use:\n    - Retry logic with exponential backoff\n    - Rate limiting\n    - Cost tracking\n    - Error handling patterns\n\n    Subclasses must implement the LLMInterface methods:\n    - generate()\n    - stream()\n    - get_token_count()\n    - check_connection()\n    - list_models()\n    - get_model_info()\n    - set_system_prompt()\n    - get_system_prompt()\n\n    Example:\n        &gt;&gt;&gt; class MyProvider(BaseProvider):\n        ...     async def generate(self, messages, **kwargs):\n        ...         return await self._with_retry(\n        ...             self._generate_impl(messages, **kwargs)\n        ...         )\n    \"\"\"\n\n    def __init__(\n        self,\n        provider_name: str,\n        max_retries: int = 3,\n        timeout: float = 30.0,\n        **kwargs: Any,\n    ):\n        \"\"\"\n        Initialize base provider.\n\n        Args:\n            provider_name: Name of the provider (e.g., \"ollama\", \"openai\")\n            max_retries: Maximum number of retry attempts\n            timeout: Request timeout in seconds\n            **kwargs: Additional provider-specific configuration\n        \"\"\"\n        self.provider_name = provider_name\n        self.max_retries = max_retries\n        self.timeout = timeout\n        self._system_prompt: Optional[str] = None\n        self._config = kwargs\n\n    async def _with_retry(\n        self,\n        coro: Callable[[], T],\n        max_retries: Optional[int] = None,\n    ) -&gt; T:\n        \"\"\"\n        Execute coroutine with retry logic.\n\n        Implements exponential backoff with jitter for retries.\n\n        Args:\n            coro: Async function to execute\n            max_retries: Override default max_retries\n\n        Returns:\n            Result from coroutine\n\n        Raises:\n            LLMError: If all retries are exhausted\n        \"\"\"\n        retries = max_retries if max_retries is not None else self.max_retries\n        last_error = None\n\n        for attempt in range(retries + 1):\n            try:\n                return await coro()\n            except Exception as e:\n                last_error = e\n\n                if attempt &lt; retries:\n                    # Exponential backoff with jitter\n                    delay = (2**attempt) + (asyncio.get_event_loop().time() % 1)\n                    await asyncio.sleep(delay)\n                    continue\n\n                # All retries exhausted\n                if isinstance(e, LLMError):\n                    raise\n                raise LLMError(\n                    f\"Request failed after {retries + 1} attempts\",\n                    provider=self.provider_name,\n                    original_error=e,\n                ) from e\n\n        # Should never reach here, but for type safety\n        raise LLMError(\n            \"Unexpected retry loop exit\",\n            provider=self.provider_name,\n            original_error=last_error,\n        )\n\n    def set_system_prompt(self, prompt: str) -&gt; None:\n        \"\"\"\n        Set system prompt for the provider.\n\n        Args:\n            prompt: System prompt text\n        \"\"\"\n        self._system_prompt = prompt\n\n    def get_system_prompt(self) -&gt; Optional[str]:\n        \"\"\"\n        Get current system prompt.\n\n        Returns:\n            Current system prompt or None\n        \"\"\"\n        return self._system_prompt\n\n    def _add_system_prompt(self, messages: list[Message]) -&gt; list[Message]:\n        \"\"\"\n        Add system prompt to messages if set.\n\n        Args:\n            messages: Original messages\n\n        Returns:\n            Messages with system prompt prepended if set\n        \"\"\"\n        if not self._system_prompt:\n            return messages\n\n        from bruno_core.models import MessageRole\n\n        # Check if first message is already a system message\n        if messages and messages[0].role == MessageRole.SYSTEM:\n            return messages\n\n        # Prepend system prompt\n        system_message = Message(\n            role=MessageRole.SYSTEM,\n            content=self._system_prompt,\n        )\n        return [system_message] + messages\n\n    def get_model_info(self) -&gt; dict[str, Any]:\n        \"\"\"\n        Get provider and configuration information.\n\n        Returns:\n            Dict with provider information\n        \"\"\"\n        return {\n            \"provider\": self.provider_name,\n            \"max_retries\": self.max_retries,\n            \"timeout\": self.timeout,\n            \"system_prompt\": self._system_prompt,\n            **self._config,\n        }\n</code></pre>"},{"location":"api/base/#bruno_llm.base.BaseProvider.__init__","title":"<code>__init__(provider_name, max_retries=3, timeout=30.0, **kwargs)</code>","text":"<p>Initialize base provider.</p> <p>Parameters:</p> Name Type Description Default <code>provider_name</code> <code>str</code> <p>Name of the provider (e.g., \"ollama\", \"openai\")</p> required <code>max_retries</code> <code>int</code> <p>Maximum number of retry attempts</p> <code>3</code> <code>timeout</code> <code>float</code> <p>Request timeout in seconds</p> <code>30.0</code> <code>**kwargs</code> <code>Any</code> <p>Additional provider-specific configuration</p> <code>{}</code> Source code in <code>bruno_llm/base/base_provider.py</code> <pre><code>def __init__(\n    self,\n    provider_name: str,\n    max_retries: int = 3,\n    timeout: float = 30.0,\n    **kwargs: Any,\n):\n    \"\"\"\n    Initialize base provider.\n\n    Args:\n        provider_name: Name of the provider (e.g., \"ollama\", \"openai\")\n        max_retries: Maximum number of retry attempts\n        timeout: Request timeout in seconds\n        **kwargs: Additional provider-specific configuration\n    \"\"\"\n    self.provider_name = provider_name\n    self.max_retries = max_retries\n    self.timeout = timeout\n    self._system_prompt: Optional[str] = None\n    self._config = kwargs\n</code></pre>"},{"location":"api/base/#bruno_llm.base.BaseProvider.set_system_prompt","title":"<code>set_system_prompt(prompt)</code>","text":"<p>Set system prompt for the provider.</p> <p>Parameters:</p> Name Type Description Default <code>prompt</code> <code>str</code> <p>System prompt text</p> required Source code in <code>bruno_llm/base/base_provider.py</code> <pre><code>def set_system_prompt(self, prompt: str) -&gt; None:\n    \"\"\"\n    Set system prompt for the provider.\n\n    Args:\n        prompt: System prompt text\n    \"\"\"\n    self._system_prompt = prompt\n</code></pre>"},{"location":"api/base/#bruno_llm.base.BaseProvider.get_system_prompt","title":"<code>get_system_prompt()</code>","text":"<p>Get current system prompt.</p> <p>Returns:</p> Type Description <code>Optional[str]</code> <p>Current system prompt or None</p> Source code in <code>bruno_llm/base/base_provider.py</code> <pre><code>def get_system_prompt(self) -&gt; Optional[str]:\n    \"\"\"\n    Get current system prompt.\n\n    Returns:\n        Current system prompt or None\n    \"\"\"\n    return self._system_prompt\n</code></pre>"},{"location":"api/base/#bruno_llm.base.BaseProvider.get_model_info","title":"<code>get_model_info()</code>","text":"<p>Get provider and configuration information.</p> <p>Returns:</p> Type Description <code>dict[str, Any]</code> <p>Dict with provider information</p> Source code in <code>bruno_llm/base/base_provider.py</code> <pre><code>def get_model_info(self) -&gt; dict[str, Any]:\n    \"\"\"\n    Get provider and configuration information.\n\n    Returns:\n        Dict with provider information\n    \"\"\"\n    return {\n        \"provider\": self.provider_name,\n        \"max_retries\": self.max_retries,\n        \"timeout\": self.timeout,\n        \"system_prompt\": self._system_prompt,\n        **self._config,\n    }\n</code></pre>"},{"location":"api/base/#bruno_llm.base.CacheEntry","title":"<code>CacheEntry</code>  <code>dataclass</code>","text":"<p>Cache entry for a response.</p> <p>Attributes:</p> Name Type Description <code>response</code> <code>str</code> <p>The cached response</p> <code>timestamp</code> <code>float</code> <p>When the entry was cached</p> <code>hit_count</code> <code>int</code> <p>Number of times this entry was accessed</p> <code>tokens</code> <code>Optional[int]</code> <p>Token count for the response (if available)</p> Source code in <code>bruno_llm/base/cache.py</code> <pre><code>@dataclass\nclass CacheEntry:\n    \"\"\"\n    Cache entry for a response.\n\n    Attributes:\n        response: The cached response\n        timestamp: When the entry was cached\n        hit_count: Number of times this entry was accessed\n        tokens: Token count for the response (if available)\n    \"\"\"\n\n    response: str\n    timestamp: float\n    hit_count: int = 0\n    tokens: Optional[int] = None\n</code></pre>"},{"location":"api/base/#bruno_llm.base.ResponseCache","title":"<code>ResponseCache</code>","text":"<p>LRU cache for LLM responses with TTL support.</p> <p>Caches responses to avoid redundant API calls. Uses message content and parameters as cache keys. Includes TTL (time-to-live) to ensure responses don't become stale.</p> <p>Features: - LRU eviction when max_size is reached - TTL-based expiration - Hit/miss statistics - Thread-safe operations (async-safe)</p> <p>Parameters:</p> Name Type Description Default <code>max_size</code> <code>int</code> <p>Maximum number of entries to cache (default: 1000)</p> <code>1000</code> <code>ttl</code> <code>float</code> <p>Time-to-live in seconds (default: 3600 = 1 hour)</p> <code>3600</code> Example <p>cache = ResponseCache(max_size=100, ttl=300)</p> Source code in <code>bruno_llm/base/cache.py</code> <pre><code>class ResponseCache:\n    \"\"\"\n    LRU cache for LLM responses with TTL support.\n\n    Caches responses to avoid redundant API calls. Uses message content\n    and parameters as cache keys. Includes TTL (time-to-live) to ensure\n    responses don't become stale.\n\n    Features:\n    - LRU eviction when max_size is reached\n    - TTL-based expiration\n    - Hit/miss statistics\n    - Thread-safe operations (async-safe)\n\n    Args:\n        max_size: Maximum number of entries to cache (default: 1000)\n        ttl: Time-to-live in seconds (default: 3600 = 1 hour)\n\n    Example:\n        &gt;&gt;&gt; cache = ResponseCache(max_size=100, ttl=300)\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Check if response is cached\n        &gt;&gt;&gt; response = cache.get(messages, temperature=0.7)\n        &gt;&gt;&gt; if response is None:\n        ...     response = await provider.generate(messages)\n        ...     cache.set(messages, response, temperature=0.7)\n    \"\"\"\n\n    def __init__(self, max_size: int = 1000, ttl: float = 3600):\n        \"\"\"\n        Initialize response cache.\n\n        Args:\n            max_size: Maximum number of cached entries\n            ttl: Time-to-live in seconds for cached entries\n        \"\"\"\n        self._cache: OrderedDict[str, CacheEntry] = OrderedDict()\n        self._max_size = max_size\n        self._ttl = ttl\n        self._hits = 0\n        self._misses = 0\n\n    def _generate_key(self, messages: list[Message], **kwargs: Any) -&gt; str:\n        \"\"\"\n        Generate cache key from messages and parameters.\n\n        Args:\n            messages: List of conversation messages\n            **kwargs: Additional generation parameters\n\n        Returns:\n            Cache key as hex string\n        \"\"\"\n        # Convert messages to dict representation\n        messages_dict = [{\"role\": msg.role.value, \"content\": msg.content} for msg in messages]\n\n        # Create deterministic JSON representation\n        key_data = {\"messages\": messages_dict, \"params\": dict(sorted(kwargs.items()))}\n\n        key_json = json.dumps(key_data, sort_keys=True)\n        return hashlib.sha256(key_json.encode()).hexdigest()\n\n    def get(self, messages: list[Message], **kwargs: Any) -&gt; Optional[str]:\n        \"\"\"\n        Get cached response if available and not expired.\n\n        Args:\n            messages: List of conversation messages\n            **kwargs: Additional generation parameters\n\n        Returns:\n            Cached response or None if not found/expired\n        \"\"\"\n        key = self._generate_key(messages, **kwargs)\n\n        if key not in self._cache:\n            self._misses += 1\n            return None\n\n        entry = self._cache[key]\n        current_time = time.time()\n\n        # Check if entry has expired\n        if current_time - entry.timestamp &gt; self._ttl:\n            # Remove expired entry\n            del self._cache[key]\n            self._misses += 1\n            return None\n\n        # Move to end (most recently used)\n        self._cache.move_to_end(key)\n        entry.hit_count += 1\n        self._hits += 1\n\n        return entry.response\n\n    def set(\n        self, messages: list[Message], response: str, tokens: Optional[int] = None, **kwargs: Any\n    ) -&gt; None:\n        \"\"\"\n        Cache a response.\n\n        Args:\n            messages: List of conversation messages\n            response: The response to cache\n            tokens: Token count for the response (optional)\n            **kwargs: Additional generation parameters\n        \"\"\"\n        key = self._generate_key(messages, **kwargs)\n\n        # Create cache entry\n        entry = CacheEntry(\n            response=response,\n            timestamp=time.time(),\n            hit_count=0,\n            tokens=tokens,\n        )\n\n        # Add to cache\n        self._cache[key] = entry\n        self._cache.move_to_end(key)\n\n        # Evict oldest entry if max size exceeded\n        if len(self._cache) &gt; self._max_size:\n            self._cache.popitem(last=False)\n\n    def clear(self) -&gt; None:\n        \"\"\"Clear all cached entries.\"\"\"\n        self._cache.clear()\n        self._hits = 0\n        self._misses = 0\n\n    def invalidate(self, messages: list[Message], **kwargs: Any) -&gt; bool:\n        \"\"\"\n        Invalidate a specific cache entry.\n\n        Args:\n            messages: List of conversation messages\n            **kwargs: Additional generation parameters\n\n        Returns:\n            True if entry was found and removed, False otherwise\n        \"\"\"\n        key = self._generate_key(messages, **kwargs)\n\n        if key in self._cache:\n            del self._cache[key]\n            return True\n        return False\n\n    def get_stats(self) -&gt; dict[str, Any]:\n        \"\"\"\n        Get cache statistics.\n\n        Returns:\n            Dictionary with cache statistics\n        \"\"\"\n        total_requests = self._hits + self._misses\n        hit_rate = self._hits / total_requests if total_requests &gt; 0 else 0.0\n\n        return {\n            \"size\": len(self._cache),\n            \"max_size\": self._max_size,\n            \"hits\": self._hits,\n            \"misses\": self._misses,\n            \"hit_rate\": hit_rate,\n            \"ttl\": self._ttl,\n        }\n\n    def get_size_bytes(self) -&gt; int:\n        \"\"\"\n        Estimate cache size in bytes.\n\n        Returns:\n            Approximate cache size in bytes\n        \"\"\"\n        total_size = 0\n        for key, entry in self._cache.items():\n            # Key size\n            total_size += len(key.encode())\n            # Response size\n            total_size += len(entry.response.encode())\n            # Overhead for entry metadata (~100 bytes)\n            total_size += 100\n\n        return total_size\n\n    def cleanup_expired(self) -&gt; int:\n        \"\"\"\n        Remove all expired entries.\n\n        Returns:\n            Number of entries removed\n        \"\"\"\n        current_time = time.time()\n        expired_keys = [\n            key for key, entry in self._cache.items() if current_time - entry.timestamp &gt; self._ttl\n        ]\n\n        for key in expired_keys:\n            del self._cache[key]\n\n        return len(expired_keys)\n\n    def get_top_entries(self, n: int = 10) -&gt; list[tuple[str, CacheEntry]]:\n        \"\"\"\n        Get top N most frequently accessed entries.\n\n        Args:\n            n: Number of entries to return\n\n        Returns:\n            List of (key, entry) tuples sorted by hit count\n        \"\"\"\n        sorted_entries = sorted(self._cache.items(), key=lambda x: x[1].hit_count, reverse=True)\n        return sorted_entries[:n]\n</code></pre>"},{"location":"api/base/#bruno_llm.base.ResponseCache--check-if-response-is-cached","title":"Check if response is cached","text":"<p>response = cache.get(messages, temperature=0.7) if response is None: ...     response = await provider.generate(messages) ...     cache.set(messages, response, temperature=0.7)</p>"},{"location":"api/base/#bruno_llm.base.ResponseCache.__init__","title":"<code>__init__(max_size=1000, ttl=3600)</code>","text":"<p>Initialize response cache.</p> <p>Parameters:</p> Name Type Description Default <code>max_size</code> <code>int</code> <p>Maximum number of cached entries</p> <code>1000</code> <code>ttl</code> <code>float</code> <p>Time-to-live in seconds for cached entries</p> <code>3600</code> Source code in <code>bruno_llm/base/cache.py</code> <pre><code>def __init__(self, max_size: int = 1000, ttl: float = 3600):\n    \"\"\"\n    Initialize response cache.\n\n    Args:\n        max_size: Maximum number of cached entries\n        ttl: Time-to-live in seconds for cached entries\n    \"\"\"\n    self._cache: OrderedDict[str, CacheEntry] = OrderedDict()\n    self._max_size = max_size\n    self._ttl = ttl\n    self._hits = 0\n    self._misses = 0\n</code></pre>"},{"location":"api/base/#bruno_llm.base.ResponseCache.get","title":"<code>get(messages, **kwargs)</code>","text":"<p>Get cached response if available and not expired.</p> <p>Parameters:</p> Name Type Description Default <code>messages</code> <code>list[Message]</code> <p>List of conversation messages</p> required <code>**kwargs</code> <code>Any</code> <p>Additional generation parameters</p> <code>{}</code> <p>Returns:</p> Type Description <code>Optional[str]</code> <p>Cached response or None if not found/expired</p> Source code in <code>bruno_llm/base/cache.py</code> <pre><code>def get(self, messages: list[Message], **kwargs: Any) -&gt; Optional[str]:\n    \"\"\"\n    Get cached response if available and not expired.\n\n    Args:\n        messages: List of conversation messages\n        **kwargs: Additional generation parameters\n\n    Returns:\n        Cached response or None if not found/expired\n    \"\"\"\n    key = self._generate_key(messages, **kwargs)\n\n    if key not in self._cache:\n        self._misses += 1\n        return None\n\n    entry = self._cache[key]\n    current_time = time.time()\n\n    # Check if entry has expired\n    if current_time - entry.timestamp &gt; self._ttl:\n        # Remove expired entry\n        del self._cache[key]\n        self._misses += 1\n        return None\n\n    # Move to end (most recently used)\n    self._cache.move_to_end(key)\n    entry.hit_count += 1\n    self._hits += 1\n\n    return entry.response\n</code></pre>"},{"location":"api/base/#bruno_llm.base.ResponseCache.set","title":"<code>set(messages, response, tokens=None, **kwargs)</code>","text":"<p>Cache a response.</p> <p>Parameters:</p> Name Type Description Default <code>messages</code> <code>list[Message]</code> <p>List of conversation messages</p> required <code>response</code> <code>str</code> <p>The response to cache</p> required <code>tokens</code> <code>Optional[int]</code> <p>Token count for the response (optional)</p> <code>None</code> <code>**kwargs</code> <code>Any</code> <p>Additional generation parameters</p> <code>{}</code> Source code in <code>bruno_llm/base/cache.py</code> <pre><code>def set(\n    self, messages: list[Message], response: str, tokens: Optional[int] = None, **kwargs: Any\n) -&gt; None:\n    \"\"\"\n    Cache a response.\n\n    Args:\n        messages: List of conversation messages\n        response: The response to cache\n        tokens: Token count for the response (optional)\n        **kwargs: Additional generation parameters\n    \"\"\"\n    key = self._generate_key(messages, **kwargs)\n\n    # Create cache entry\n    entry = CacheEntry(\n        response=response,\n        timestamp=time.time(),\n        hit_count=0,\n        tokens=tokens,\n    )\n\n    # Add to cache\n    self._cache[key] = entry\n    self._cache.move_to_end(key)\n\n    # Evict oldest entry if max size exceeded\n    if len(self._cache) &gt; self._max_size:\n        self._cache.popitem(last=False)\n</code></pre>"},{"location":"api/base/#bruno_llm.base.ResponseCache.clear","title":"<code>clear()</code>","text":"<p>Clear all cached entries.</p> Source code in <code>bruno_llm/base/cache.py</code> <pre><code>def clear(self) -&gt; None:\n    \"\"\"Clear all cached entries.\"\"\"\n    self._cache.clear()\n    self._hits = 0\n    self._misses = 0\n</code></pre>"},{"location":"api/base/#bruno_llm.base.ResponseCache.invalidate","title":"<code>invalidate(messages, **kwargs)</code>","text":"<p>Invalidate a specific cache entry.</p> <p>Parameters:</p> Name Type Description Default <code>messages</code> <code>list[Message]</code> <p>List of conversation messages</p> required <code>**kwargs</code> <code>Any</code> <p>Additional generation parameters</p> <code>{}</code> <p>Returns:</p> Type Description <code>bool</code> <p>True if entry was found and removed, False otherwise</p> Source code in <code>bruno_llm/base/cache.py</code> <pre><code>def invalidate(self, messages: list[Message], **kwargs: Any) -&gt; bool:\n    \"\"\"\n    Invalidate a specific cache entry.\n\n    Args:\n        messages: List of conversation messages\n        **kwargs: Additional generation parameters\n\n    Returns:\n        True if entry was found and removed, False otherwise\n    \"\"\"\n    key = self._generate_key(messages, **kwargs)\n\n    if key in self._cache:\n        del self._cache[key]\n        return True\n    return False\n</code></pre>"},{"location":"api/base/#bruno_llm.base.ResponseCache.get_stats","title":"<code>get_stats()</code>","text":"<p>Get cache statistics.</p> <p>Returns:</p> Type Description <code>dict[str, Any]</code> <p>Dictionary with cache statistics</p> Source code in <code>bruno_llm/base/cache.py</code> <pre><code>def get_stats(self) -&gt; dict[str, Any]:\n    \"\"\"\n    Get cache statistics.\n\n    Returns:\n        Dictionary with cache statistics\n    \"\"\"\n    total_requests = self._hits + self._misses\n    hit_rate = self._hits / total_requests if total_requests &gt; 0 else 0.0\n\n    return {\n        \"size\": len(self._cache),\n        \"max_size\": self._max_size,\n        \"hits\": self._hits,\n        \"misses\": self._misses,\n        \"hit_rate\": hit_rate,\n        \"ttl\": self._ttl,\n    }\n</code></pre>"},{"location":"api/base/#bruno_llm.base.ResponseCache.get_size_bytes","title":"<code>get_size_bytes()</code>","text":"<p>Estimate cache size in bytes.</p> <p>Returns:</p> Type Description <code>int</code> <p>Approximate cache size in bytes</p> Source code in <code>bruno_llm/base/cache.py</code> <pre><code>def get_size_bytes(self) -&gt; int:\n    \"\"\"\n    Estimate cache size in bytes.\n\n    Returns:\n        Approximate cache size in bytes\n    \"\"\"\n    total_size = 0\n    for key, entry in self._cache.items():\n        # Key size\n        total_size += len(key.encode())\n        # Response size\n        total_size += len(entry.response.encode())\n        # Overhead for entry metadata (~100 bytes)\n        total_size += 100\n\n    return total_size\n</code></pre>"},{"location":"api/base/#bruno_llm.base.ResponseCache.cleanup_expired","title":"<code>cleanup_expired()</code>","text":"<p>Remove all expired entries.</p> <p>Returns:</p> Type Description <code>int</code> <p>Number of entries removed</p> Source code in <code>bruno_llm/base/cache.py</code> <pre><code>def cleanup_expired(self) -&gt; int:\n    \"\"\"\n    Remove all expired entries.\n\n    Returns:\n        Number of entries removed\n    \"\"\"\n    current_time = time.time()\n    expired_keys = [\n        key for key, entry in self._cache.items() if current_time - entry.timestamp &gt; self._ttl\n    ]\n\n    for key in expired_keys:\n        del self._cache[key]\n\n    return len(expired_keys)\n</code></pre>"},{"location":"api/base/#bruno_llm.base.ResponseCache.get_top_entries","title":"<code>get_top_entries(n=10)</code>","text":"<p>Get top N most frequently accessed entries.</p> <p>Parameters:</p> Name Type Description Default <code>n</code> <code>int</code> <p>Number of entries to return</p> <code>10</code> <p>Returns:</p> Type Description <code>list[tuple[str, CacheEntry]]</code> <p>List of (key, entry) tuples sorted by hit count</p> Source code in <code>bruno_llm/base/cache.py</code> <pre><code>def get_top_entries(self, n: int = 10) -&gt; list[tuple[str, CacheEntry]]:\n    \"\"\"\n    Get top N most frequently accessed entries.\n\n    Args:\n        n: Number of entries to return\n\n    Returns:\n        List of (key, entry) tuples sorted by hit count\n    \"\"\"\n    sorted_entries = sorted(self._cache.items(), key=lambda x: x[1].hit_count, reverse=True)\n    return sorted_entries[:n]\n</code></pre>"},{"location":"api/base/#bruno_llm.base.ContextLimits","title":"<code>ContextLimits</code>  <code>dataclass</code>","text":"<p>Context window limits for a model.</p> <p>Attributes:</p> Name Type Description <code>max_tokens</code> <code>int</code> <p>Maximum total tokens (input + output)</p> <code>max_input_tokens</code> <code>Optional[int]</code> <p>Maximum input tokens</p> <code>max_output_tokens</code> <code>Optional[int]</code> <p>Maximum output tokens</p> <code>warning_threshold</code> <code>float</code> <p>Warn when this % of limit is reached (0.0-1.0)</p> Source code in <code>bruno_llm/base/context.py</code> <pre><code>@dataclass\nclass ContextLimits:\n    \"\"\"\n    Context window limits for a model.\n\n    Attributes:\n        max_tokens: Maximum total tokens (input + output)\n        max_input_tokens: Maximum input tokens\n        max_output_tokens: Maximum output tokens\n        warning_threshold: Warn when this % of limit is reached (0.0-1.0)\n    \"\"\"\n\n    max_tokens: int\n    max_input_tokens: Optional[int] = None\n    max_output_tokens: Optional[int] = None\n    warning_threshold: float = 0.9\n\n    def __post_init__(self):\n        \"\"\"Validate limits after initialization.\"\"\"\n        if self.max_input_tokens is None:\n            self.max_input_tokens = self.max_tokens\n        if self.max_output_tokens is None:\n            self.max_output_tokens = self.max_tokens // 4  # Default 25% for output\n</code></pre>"},{"location":"api/base/#bruno_llm.base.ContextLimits.__post_init__","title":"<code>__post_init__()</code>","text":"<p>Validate limits after initialization.</p> Source code in <code>bruno_llm/base/context.py</code> <pre><code>def __post_init__(self):\n    \"\"\"Validate limits after initialization.\"\"\"\n    if self.max_input_tokens is None:\n        self.max_input_tokens = self.max_tokens\n    if self.max_output_tokens is None:\n        self.max_output_tokens = self.max_tokens // 4  # Default 25% for output\n</code></pre>"},{"location":"api/base/#bruno_llm.base.ContextWindowManager","title":"<code>ContextWindowManager</code>","text":"<p>Manage context windows and message truncation.</p> <p>Handles: - Token counting for messages - Context limit checking - Automatic message truncation - Warning when approaching limits</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>str</code> <p>Model name for context limits</p> required <code>token_counter</code> <code>Optional[TokenCounter]</code> <p>Token counter instance</p> <code>None</code> <code>limits</code> <code>Optional[ContextLimits]</code> <p>Custom context limits (overrides model defaults)</p> <code>None</code> <code>strategy</code> <code>TruncationStrategy</code> <p>Truncation strategy to use</p> <code>SLIDING_WINDOW</code> Example <p>manager = ContextWindowManager(model=\"gpt-4\")</p> Source code in <code>bruno_llm/base/context.py</code> <pre><code>class ContextWindowManager:\n    \"\"\"\n    Manage context windows and message truncation.\n\n    Handles:\n    - Token counting for messages\n    - Context limit checking\n    - Automatic message truncation\n    - Warning when approaching limits\n\n    Args:\n        model: Model name for context limits\n        token_counter: Token counter instance\n        limits: Custom context limits (overrides model defaults)\n        strategy: Truncation strategy to use\n\n    Example:\n        &gt;&gt;&gt; manager = ContextWindowManager(model=\"gpt-4\")\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Check if messages fit\n        &gt;&gt;&gt; if manager.check_limit(messages):\n        ...     response = await provider.generate(messages)\n        ... else:\n        ...     # Truncate messages\n        ...     truncated = manager.truncate(messages)\n        ...     response = await provider.generate(truncated)\n    \"\"\"\n\n    def __init__(\n        self,\n        model: str,\n        token_counter: Optional[TokenCounter] = None,\n        limits: Optional[ContextLimits] = None,\n        strategy: TruncationStrategy = TruncationStrategy.SLIDING_WINDOW,\n    ):\n        \"\"\"\n        Initialize context window manager.\n\n        Args:\n            model: Model name\n            token_counter: Token counter instance\n            limits: Custom context limits\n            strategy: Truncation strategy\n        \"\"\"\n        self.model = model\n        self.token_counter = token_counter or create_token_counter(model)\n        self.limits = limits or self._get_model_limits(model)\n        self.strategy = strategy\n        self._warning_callback: Optional[Callable[[int, int], None]] = None\n\n    def _get_model_limits(self, model: str) -&gt; ContextLimits:\n        \"\"\"\n        Get context limits for a model.\n\n        Args:\n            model: Model name\n\n        Returns:\n            Context limits for the model\n        \"\"\"\n        # Try exact match first\n        if model in MODEL_LIMITS:\n            return MODEL_LIMITS[model]\n\n        # Try partial match\n        for model_name, limits in MODEL_LIMITS.items():\n            if model.startswith(model_name):\n                return limits\n\n        # Default conservative limit\n        return ContextLimits(max_tokens=4096)\n\n    def count_tokens(self, messages: list[Message]) -&gt; int:\n        \"\"\"\n        Count tokens in messages.\n\n        Args:\n            messages: List of messages\n\n        Returns:\n            Total token count\n        \"\"\"\n        return self.token_counter.count_messages_tokens(messages)\n\n    def check_limit(\n        self,\n        messages: list[Message],\n        max_output_tokens: Optional[int] = None,\n    ) -&gt; bool:\n        \"\"\"\n        Check if messages fit within context limit.\n\n        Args:\n            messages: List of messages\n            max_output_tokens: Expected output tokens\n\n        Returns:\n            True if messages fit, False otherwise\n        \"\"\"\n        input_tokens = self.count_tokens(messages)\n        output_tokens = max_output_tokens or self.limits.max_output_tokens\n        total_tokens = input_tokens + output_tokens\n\n        # Check warning threshold\n        if input_tokens / self.limits.max_input_tokens &gt;= self.limits.warning_threshold:\n            if self._warning_callback:\n                self._warning_callback(input_tokens, self.limits.max_input_tokens)\n\n        return total_tokens &lt;= self.limits.max_tokens\n\n    def get_available_tokens(self, messages: list[Message]) -&gt; int:\n        \"\"\"\n        Get number of tokens available for output.\n\n        Args:\n            messages: List of messages\n\n        Returns:\n            Available tokens for output\n        \"\"\"\n        input_tokens = self.count_tokens(messages)\n        return max(0, self.limits.max_tokens - input_tokens)\n\n    def truncate(\n        self,\n        messages: list[Message],\n        max_output_tokens: Optional[int] = None,\n    ) -&gt; list[Message]:\n        \"\"\"\n        Truncate messages to fit within context limit.\n\n        Args:\n            messages: List of messages\n            max_output_tokens: Expected output tokens\n\n        Returns:\n            Truncated message list\n\n        Raises:\n            ContextLengthExceededError: If messages can't be truncated enough\n        \"\"\"\n        output_tokens = max_output_tokens or self.limits.max_output_tokens\n        target_input_tokens = self.limits.max_tokens - output_tokens\n\n        if target_input_tokens &lt;= 0:\n            raise ContextLengthExceededError(\n                f\"Output tokens ({output_tokens}) exceed total limit ({self.limits.max_tokens})\"\n            )\n\n        if self.strategy == TruncationStrategy.OLDEST_FIRST:\n            return self._truncate_oldest_first(messages, target_input_tokens)\n        elif self.strategy == TruncationStrategy.MIDDLE_OUT:\n            return self._truncate_middle_out(messages, target_input_tokens)\n        elif self.strategy == TruncationStrategy.SLIDING_WINDOW:\n            return self._truncate_sliding_window(messages, target_input_tokens)\n        elif self.strategy == TruncationStrategy.SMART:\n            return self._truncate_smart(messages, target_input_tokens)\n        else:\n            return self._truncate_oldest_first(messages, target_input_tokens)\n\n    def _truncate_oldest_first(\n        self,\n        messages: list[Message],\n        target_tokens: int,\n    ) -&gt; list[Message]:\n        \"\"\"Remove oldest messages first (keep system message).\"\"\"\n        # Always keep system messages\n        system_messages = [m for m in messages if m.role == MessageRole.SYSTEM]\n        other_messages = [m for m in messages if m.role != MessageRole.SYSTEM]\n\n        # Start with system messages\n        result = system_messages[:]\n        current_tokens = self.count_tokens(result)\n\n        # Add messages from newest to oldest\n        for message in reversed(other_messages):\n            message_tokens = self.token_counter.count_message_tokens(message)\n            if current_tokens + message_tokens &lt;= target_tokens:\n                result.insert(len(system_messages), message)\n                current_tokens += message_tokens\n            else:\n                break\n\n        # Re-order to maintain chronological order (except system at start)\n        return system_messages + list(reversed(result[len(system_messages) :]))\n\n    def _truncate_middle_out(\n        self,\n        messages: list[Message],\n        target_tokens: int,\n    ) -&gt; list[Message]:\n        \"\"\"Keep first and last messages, remove middle.\"\"\"\n        if len(messages) &lt;= 2:\n            return messages\n\n        # Keep system messages and last message\n        system_messages = [m for m in messages if m.role == MessageRole.SYSTEM]\n        other_messages = [m for m in messages if m.role != MessageRole.SYSTEM]\n\n        if not other_messages:\n            return messages\n\n        result = system_messages + [other_messages[-1]]\n        current_tokens = self.count_tokens(result)\n\n        # Add messages from the start\n        for message in other_messages[:-1]:\n            message_tokens = self.token_counter.count_message_tokens(message)\n            if current_tokens + message_tokens &lt;= target_tokens:\n                result.insert(len(system_messages), message)\n                current_tokens += message_tokens\n            else:\n                break\n\n        return result\n\n    def _truncate_sliding_window(\n        self,\n        messages: list[Message],\n        target_tokens: int,\n    ) -&gt; list[Message]:\n        \"\"\"Keep most recent N messages.\"\"\"\n        # Always keep system messages\n        system_messages = [m for m in messages if m.role == MessageRole.SYSTEM]\n        other_messages = [m for m in messages if m.role != MessageRole.SYSTEM]\n\n        result = system_messages[:]\n        current_tokens = self.count_tokens(result)\n\n        # Add messages from newest to oldest\n        for message in reversed(other_messages):\n            message_tokens = self.token_counter.count_message_tokens(message)\n            if current_tokens + message_tokens &lt;= target_tokens:\n                result.append(message)\n                current_tokens += message_tokens\n            else:\n                break\n\n        # Keep system messages at start, reverse others\n        return system_messages + list(reversed(result[len(system_messages) :]))\n\n    def _truncate_smart(\n        self,\n        messages: list[Message],\n        target_tokens: int,\n    ) -&gt; list[Message]:\n        \"\"\"\n        Smart truncation: keep system + important messages + recent.\n\n        Priority:\n        1. System messages (always keep)\n        2. Last 2 messages (recent context)\n        3. Messages with high token count (likely important)\n        4. Fill remaining space with recent messages\n        \"\"\"\n        system_messages = [m for m in messages if m.role == MessageRole.SYSTEM]\n        other_messages = [m for m in messages if m.role != MessageRole.SYSTEM]\n\n        if not other_messages:\n            return messages\n\n        # Start with system messages\n        result = system_messages[:]\n        current_tokens = self.count_tokens(result)\n\n        # Always include last 2 messages (most recent context)\n        priority_messages = other_messages[-2:]\n        for message in priority_messages:\n            message_tokens = self.token_counter.count_message_tokens(message)\n            if current_tokens + message_tokens &lt;= target_tokens:\n                result.append(message)\n                current_tokens += message_tokens\n\n        # Fill remaining space with other messages (newest first)\n        remaining = list(other_messages[:-2])\n        for message in reversed(remaining):\n            message_tokens = self.token_counter.count_message_tokens(message)\n            if current_tokens + message_tokens &lt;= target_tokens:\n                result.insert(len(system_messages), message)\n                current_tokens += message_tokens\n            else:\n                break\n\n        return result\n\n    def set_warning_callback(self, callback: Callable[[int, int], None]) -&gt; None:\n        \"\"\"\n        Set callback for context limit warnings.\n\n        Args:\n            callback: Function (current_tokens, max_tokens) -&gt; None\n        \"\"\"\n        self._warning_callback = callback\n\n    def get_stats(self, messages: list[Message]) -&gt; dict:\n        \"\"\"\n        Get statistics about context usage.\n\n        Args:\n            messages: List of messages\n\n        Returns:\n            Dictionary with context statistics\n        \"\"\"\n        input_tokens = self.count_tokens(messages)\n        available_tokens = self.get_available_tokens(messages)\n        usage_percent = (input_tokens / self.limits.max_input_tokens) * 100\n\n        return {\n            \"model\": self.model,\n            \"input_tokens\": input_tokens,\n            \"max_input_tokens\": self.limits.max_input_tokens,\n            \"available_output_tokens\": available_tokens,\n            \"max_output_tokens\": self.limits.max_output_tokens,\n            \"total_limit\": self.limits.max_tokens,\n            \"usage_percent\": usage_percent,\n            \"within_limit\": self.check_limit(messages),\n            \"message_count\": len(messages),\n        }\n</code></pre>"},{"location":"api/base/#bruno_llm.base.ContextWindowManager--check-if-messages-fit","title":"Check if messages fit","text":"<p>if manager.check_limit(messages): ...     response = await provider.generate(messages) ... else: ...     # Truncate messages ...     truncated = manager.truncate(messages) ...     response = await provider.generate(truncated)</p>"},{"location":"api/base/#bruno_llm.base.ContextWindowManager.__init__","title":"<code>__init__(model, token_counter=None, limits=None, strategy=TruncationStrategy.SLIDING_WINDOW)</code>","text":"<p>Initialize context window manager.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>str</code> <p>Model name</p> required <code>token_counter</code> <code>Optional[TokenCounter]</code> <p>Token counter instance</p> <code>None</code> <code>limits</code> <code>Optional[ContextLimits]</code> <p>Custom context limits</p> <code>None</code> <code>strategy</code> <code>TruncationStrategy</code> <p>Truncation strategy</p> <code>SLIDING_WINDOW</code> Source code in <code>bruno_llm/base/context.py</code> <pre><code>def __init__(\n    self,\n    model: str,\n    token_counter: Optional[TokenCounter] = None,\n    limits: Optional[ContextLimits] = None,\n    strategy: TruncationStrategy = TruncationStrategy.SLIDING_WINDOW,\n):\n    \"\"\"\n    Initialize context window manager.\n\n    Args:\n        model: Model name\n        token_counter: Token counter instance\n        limits: Custom context limits\n        strategy: Truncation strategy\n    \"\"\"\n    self.model = model\n    self.token_counter = token_counter or create_token_counter(model)\n    self.limits = limits or self._get_model_limits(model)\n    self.strategy = strategy\n    self._warning_callback: Optional[Callable[[int, int], None]] = None\n</code></pre>"},{"location":"api/base/#bruno_llm.base.ContextWindowManager.count_tokens","title":"<code>count_tokens(messages)</code>","text":"<p>Count tokens in messages.</p> <p>Parameters:</p> Name Type Description Default <code>messages</code> <code>list[Message]</code> <p>List of messages</p> required <p>Returns:</p> Type Description <code>int</code> <p>Total token count</p> Source code in <code>bruno_llm/base/context.py</code> <pre><code>def count_tokens(self, messages: list[Message]) -&gt; int:\n    \"\"\"\n    Count tokens in messages.\n\n    Args:\n        messages: List of messages\n\n    Returns:\n        Total token count\n    \"\"\"\n    return self.token_counter.count_messages_tokens(messages)\n</code></pre>"},{"location":"api/base/#bruno_llm.base.ContextWindowManager.check_limit","title":"<code>check_limit(messages, max_output_tokens=None)</code>","text":"<p>Check if messages fit within context limit.</p> <p>Parameters:</p> Name Type Description Default <code>messages</code> <code>list[Message]</code> <p>List of messages</p> required <code>max_output_tokens</code> <code>Optional[int]</code> <p>Expected output tokens</p> <code>None</code> <p>Returns:</p> Type Description <code>bool</code> <p>True if messages fit, False otherwise</p> Source code in <code>bruno_llm/base/context.py</code> <pre><code>def check_limit(\n    self,\n    messages: list[Message],\n    max_output_tokens: Optional[int] = None,\n) -&gt; bool:\n    \"\"\"\n    Check if messages fit within context limit.\n\n    Args:\n        messages: List of messages\n        max_output_tokens: Expected output tokens\n\n    Returns:\n        True if messages fit, False otherwise\n    \"\"\"\n    input_tokens = self.count_tokens(messages)\n    output_tokens = max_output_tokens or self.limits.max_output_tokens\n    total_tokens = input_tokens + output_tokens\n\n    # Check warning threshold\n    if input_tokens / self.limits.max_input_tokens &gt;= self.limits.warning_threshold:\n        if self._warning_callback:\n            self._warning_callback(input_tokens, self.limits.max_input_tokens)\n\n    return total_tokens &lt;= self.limits.max_tokens\n</code></pre>"},{"location":"api/base/#bruno_llm.base.ContextWindowManager.get_available_tokens","title":"<code>get_available_tokens(messages)</code>","text":"<p>Get number of tokens available for output.</p> <p>Parameters:</p> Name Type Description Default <code>messages</code> <code>list[Message]</code> <p>List of messages</p> required <p>Returns:</p> Type Description <code>int</code> <p>Available tokens for output</p> Source code in <code>bruno_llm/base/context.py</code> <pre><code>def get_available_tokens(self, messages: list[Message]) -&gt; int:\n    \"\"\"\n    Get number of tokens available for output.\n\n    Args:\n        messages: List of messages\n\n    Returns:\n        Available tokens for output\n    \"\"\"\n    input_tokens = self.count_tokens(messages)\n    return max(0, self.limits.max_tokens - input_tokens)\n</code></pre>"},{"location":"api/base/#bruno_llm.base.ContextWindowManager.truncate","title":"<code>truncate(messages, max_output_tokens=None)</code>","text":"<p>Truncate messages to fit within context limit.</p> <p>Parameters:</p> Name Type Description Default <code>messages</code> <code>list[Message]</code> <p>List of messages</p> required <code>max_output_tokens</code> <code>Optional[int]</code> <p>Expected output tokens</p> <code>None</code> <p>Returns:</p> Type Description <code>list[Message]</code> <p>Truncated message list</p> <p>Raises:</p> Type Description <code>ContextLengthExceededError</code> <p>If messages can't be truncated enough</p> Source code in <code>bruno_llm/base/context.py</code> <pre><code>def truncate(\n    self,\n    messages: list[Message],\n    max_output_tokens: Optional[int] = None,\n) -&gt; list[Message]:\n    \"\"\"\n    Truncate messages to fit within context limit.\n\n    Args:\n        messages: List of messages\n        max_output_tokens: Expected output tokens\n\n    Returns:\n        Truncated message list\n\n    Raises:\n        ContextLengthExceededError: If messages can't be truncated enough\n    \"\"\"\n    output_tokens = max_output_tokens or self.limits.max_output_tokens\n    target_input_tokens = self.limits.max_tokens - output_tokens\n\n    if target_input_tokens &lt;= 0:\n        raise ContextLengthExceededError(\n            f\"Output tokens ({output_tokens}) exceed total limit ({self.limits.max_tokens})\"\n        )\n\n    if self.strategy == TruncationStrategy.OLDEST_FIRST:\n        return self._truncate_oldest_first(messages, target_input_tokens)\n    elif self.strategy == TruncationStrategy.MIDDLE_OUT:\n        return self._truncate_middle_out(messages, target_input_tokens)\n    elif self.strategy == TruncationStrategy.SLIDING_WINDOW:\n        return self._truncate_sliding_window(messages, target_input_tokens)\n    elif self.strategy == TruncationStrategy.SMART:\n        return self._truncate_smart(messages, target_input_tokens)\n    else:\n        return self._truncate_oldest_first(messages, target_input_tokens)\n</code></pre>"},{"location":"api/base/#bruno_llm.base.ContextWindowManager.set_warning_callback","title":"<code>set_warning_callback(callback)</code>","text":"<p>Set callback for context limit warnings.</p> <p>Parameters:</p> Name Type Description Default <code>callback</code> <code>Callable[[int, int], None]</code> <p>Function (current_tokens, max_tokens) -&gt; None</p> required Source code in <code>bruno_llm/base/context.py</code> <pre><code>def set_warning_callback(self, callback: Callable[[int, int], None]) -&gt; None:\n    \"\"\"\n    Set callback for context limit warnings.\n\n    Args:\n        callback: Function (current_tokens, max_tokens) -&gt; None\n    \"\"\"\n    self._warning_callback = callback\n</code></pre>"},{"location":"api/base/#bruno_llm.base.ContextWindowManager.get_stats","title":"<code>get_stats(messages)</code>","text":"<p>Get statistics about context usage.</p> <p>Parameters:</p> Name Type Description Default <code>messages</code> <code>list[Message]</code> <p>List of messages</p> required <p>Returns:</p> Type Description <code>dict</code> <p>Dictionary with context statistics</p> Source code in <code>bruno_llm/base/context.py</code> <pre><code>def get_stats(self, messages: list[Message]) -&gt; dict:\n    \"\"\"\n    Get statistics about context usage.\n\n    Args:\n        messages: List of messages\n\n    Returns:\n        Dictionary with context statistics\n    \"\"\"\n    input_tokens = self.count_tokens(messages)\n    available_tokens = self.get_available_tokens(messages)\n    usage_percent = (input_tokens / self.limits.max_input_tokens) * 100\n\n    return {\n        \"model\": self.model,\n        \"input_tokens\": input_tokens,\n        \"max_input_tokens\": self.limits.max_input_tokens,\n        \"available_output_tokens\": available_tokens,\n        \"max_output_tokens\": self.limits.max_output_tokens,\n        \"total_limit\": self.limits.max_tokens,\n        \"usage_percent\": usage_percent,\n        \"within_limit\": self.check_limit(messages),\n        \"message_count\": len(messages),\n    }\n</code></pre>"},{"location":"api/base/#bruno_llm.base.TruncationStrategy","title":"<code>TruncationStrategy</code>","text":"<p>               Bases: <code>Enum</code></p> <p>Strategy for truncating messages when context limit is exceeded.</p> Source code in <code>bruno_llm/base/context.py</code> <pre><code>class TruncationStrategy(Enum):\n    \"\"\"Strategy for truncating messages when context limit is exceeded.\"\"\"\n\n    OLDEST_FIRST = \"oldest_first\"  # Remove oldest messages first\n    MIDDLE_OUT = \"middle_out\"  # Keep first and last, remove middle\n    SLIDING_WINDOW = \"sliding_window\"  # Keep most recent N messages\n    SMART = \"smart\"  # Keep system + important messages + recent\n</code></pre>"},{"location":"api/base/#bruno_llm.base.CostTracker","title":"<code>CostTracker</code>","text":"<p>Track API usage costs across requests.</p> <p>Maintains history of API calls with token usage and costs. Supports multiple models with different pricing.</p> Example <p>tracker = CostTracker( ...     provider_name=\"openai\", ...     pricing={ ...         \"gpt-4\": {\"input\": 0.03, \"output\": 0.06}, ...         \"gpt-3.5-turbo\": {\"input\": 0.001, \"output\": 0.002}, ...     } ... ) tracker.track_request( ...     model=\"gpt-4\", ...     input_tokens=100, ...     output_tokens=50 ... ) print(tracker.get_total_cost()) 6.0  # $0.06 (in cents)</p> Source code in <code>bruno_llm/base/cost_tracker.py</code> <pre><code>class CostTracker:\n    \"\"\"\n    Track API usage costs across requests.\n\n    Maintains history of API calls with token usage and costs.\n    Supports multiple models with different pricing.\n\n    Example:\n        &gt;&gt;&gt; tracker = CostTracker(\n        ...     provider_name=\"openai\",\n        ...     pricing={\n        ...         \"gpt-4\": {\"input\": 0.03, \"output\": 0.06},\n        ...         \"gpt-3.5-turbo\": {\"input\": 0.001, \"output\": 0.002},\n        ...     }\n        ... )\n        &gt;&gt;&gt; tracker.track_request(\n        ...     model=\"gpt-4\",\n        ...     input_tokens=100,\n        ...     output_tokens=50\n        ... )\n        &gt;&gt;&gt; print(tracker.get_total_cost())\n        6.0  # $0.06 (in cents)\n    \"\"\"\n\n    def __init__(\n        self,\n        provider_name: str,\n        pricing: dict[str, dict[str, float]],\n        currency: str = \"USD\",\n    ):\n        \"\"\"\n        Initialize cost tracker.\n\n        Args:\n            provider_name: Name of the provider\n            pricing: Pricing per model (per 1K tokens)\n                Format: {\"model_name\": {\"input\": price, \"output\": price}}\n            currency: Currency code (default: \"USD\")\n        \"\"\"\n        self.provider_name = provider_name\n        self.pricing = pricing\n        self.currency = currency\n        self.usage_history: list[UsageRecord] = []\n\n    def track_request(\n        self,\n        model: str,\n        input_tokens: int,\n        output_tokens: int,\n        metadata: Optional[dict[str, str]] = None,\n    ) -&gt; UsageRecord:\n        \"\"\"\n        Track a single API request.\n\n        Args:\n            model: Model name\n            input_tokens: Number of input tokens\n            output_tokens: Number of output tokens\n            metadata: Optional metadata about the request\n\n        Returns:\n            UsageRecord with calculated costs\n        \"\"\"\n        # Get pricing for model (with fallback to default if available)\n        model_pricing = self.pricing.get(model, self.pricing.get(\"default\", {}))\n\n        # Calculate costs (pricing is per 1K tokens)\n        input_cost = (input_tokens / 1000.0) * model_pricing.get(\"input\", 0.0)\n        output_cost = (output_tokens / 1000.0) * model_pricing.get(\"output\", 0.0)\n        total_cost = input_cost + output_cost\n\n        # Create record\n        record = UsageRecord(\n            timestamp=time.time(),\n            model=model,\n            input_tokens=input_tokens,\n            output_tokens=output_tokens,\n            input_cost=input_cost,\n            output_cost=output_cost,\n            total_cost=total_cost,\n            metadata=metadata or {},\n        )\n\n        self.usage_history.append(record)\n        return record\n\n    def get_total_cost(self, model: Optional[str] = None) -&gt; float:\n        \"\"\"\n        Get total cost across all requests.\n\n        Args:\n            model: Optional model filter (None = all models)\n\n        Returns:\n            Total cost in the configured currency\n        \"\"\"\n        total = 0.0\n        for record in self.usage_history:\n            if model is None or record.model == model:\n                total += record.total_cost\n        return total\n\n    def get_total_tokens(self, model: Optional[str] = None) -&gt; dict[str, int]:\n        \"\"\"\n        Get total tokens used.\n\n        Args:\n            model: Optional model filter (None = all models)\n\n        Returns:\n            Dict with input, output, and total token counts\n        \"\"\"\n        input_tokens = 0\n        output_tokens = 0\n\n        for record in self.usage_history:\n            if model is None or record.model == model:\n                input_tokens += record.input_tokens\n                output_tokens += record.output_tokens\n\n        return {\n            \"input\": input_tokens,\n            \"output\": output_tokens,\n            \"total\": input_tokens + output_tokens,\n        }\n\n    def get_request_count(self, model: Optional[str] = None) -&gt; int:\n        \"\"\"\n        Get number of requests made.\n\n        Args:\n            model: Optional model filter (None = all models)\n\n        Returns:\n            Number of requests\n        \"\"\"\n        if model is None:\n            return len(self.usage_history)\n        return sum(1 for r in self.usage_history if r.model == model)\n\n    def get_model_breakdown(self) -&gt; dict[str, dict[str, float]]:\n        \"\"\"\n        Get cost breakdown by model.\n\n        Returns:\n            Dict mapping model names to their usage statistics\n        \"\"\"\n        breakdown: dict[str, dict[str, float]] = {}\n\n        for record in self.usage_history:\n            if record.model not in breakdown:\n                breakdown[record.model] = {\n                    \"cost\": 0.0,\n                    \"input_tokens\": 0,\n                    \"output_tokens\": 0,\n                    \"requests\": 0,\n                }\n\n            breakdown[record.model][\"cost\"] += record.total_cost\n            breakdown[record.model][\"input_tokens\"] += record.input_tokens\n            breakdown[record.model][\"output_tokens\"] += record.output_tokens\n            breakdown[record.model][\"requests\"] += 1\n\n        return breakdown\n\n    def get_usage_report(self) -&gt; dict:\n        \"\"\"\n        Get comprehensive usage report.\n\n        Returns:\n            Dict with complete usage statistics\n        \"\"\"\n        return {\n            \"provider\": self.provider_name,\n            \"currency\": self.currency,\n            \"total_cost\": self.get_total_cost(),\n            \"total_requests\": self.get_request_count(),\n            \"total_tokens\": self.get_total_tokens(),\n            \"model_breakdown\": self.get_model_breakdown(),\n            \"first_request\": (\n                self.usage_history[0].datetime.isoformat() if self.usage_history else None\n            ),\n            \"last_request\": (\n                self.usage_history[-1].datetime.isoformat() if self.usage_history else None\n            ),\n        }\n\n    def clear_history(self) -&gt; None:\n        \"\"\"Clear all usage history.\"\"\"\n        self.usage_history.clear()\n\n    def export_history(self) -&gt; list[dict]:\n        \"\"\"\n        Export usage history as list of dicts.\n\n        Returns:\n            List of usage records as dictionaries\n        \"\"\"\n        return [\n            {\n                \"timestamp\": record.datetime.isoformat(),\n                \"model\": record.model,\n                \"input_tokens\": record.input_tokens,\n                \"output_tokens\": record.output_tokens,\n                \"input_cost\": record.input_cost,\n                \"output_cost\": record.output_cost,\n                \"total_cost\": record.total_cost,\n                \"metadata\": record.metadata,\n            }\n            for record in self.usage_history\n        ]\n\n    def export_to_csv(self, filepath: str) -&gt; None:\n        \"\"\"\n        Export usage history to CSV file.\n\n        Args:\n            filepath: Path to output CSV file\n        \"\"\"\n        import csv\n\n        if not self.usage_history:\n            return\n\n        with open(filepath, \"w\", newline=\"\", encoding=\"utf-8\") as f:\n            writer = csv.writer(f)\n            writer.writerow(\n                [\n                    \"Timestamp\",\n                    \"Model\",\n                    \"Input Tokens\",\n                    \"Output Tokens\",\n                    \"Total Tokens\",\n                    \"Input Cost\",\n                    \"Output Cost\",\n                    \"Total Cost\",\n                ]\n            )\n\n            for record in self.usage_history:\n                writer.writerow(\n                    [\n                        record.datetime.isoformat(),\n                        record.model,\n                        record.input_tokens,\n                        record.output_tokens,\n                        record.total_tokens,\n                        f\"{record.input_cost:.6f}\",\n                        f\"{record.output_cost:.6f}\",\n                        f\"{record.total_cost:.6f}\",\n                    ]\n                )\n\n    def export_to_json(self, filepath: str) -&gt; None:\n        \"\"\"\n        Export usage history to JSON file.\n\n        Args:\n            filepath: Path to output JSON file\n        \"\"\"\n        import json\n\n        data = {\n            \"provider\": self.provider_name,\n            \"currency\": self.currency,\n            \"export_date\": datetime.now().isoformat(),\n            \"summary\": self.get_usage_report(),\n            \"history\": self.export_history(),\n        }\n\n        with open(filepath, \"w\", encoding=\"utf-8\") as f:\n            json.dump(data, f, indent=2)\n\n    def get_time_range_report(\n        self,\n        start_time: Optional[float] = None,\n        end_time: Optional[float] = None,\n    ) -&gt; dict:\n        \"\"\"\n        Get usage report for a specific time range.\n\n        Args:\n            start_time: Start timestamp (inclusive)\n            end_time: End timestamp (inclusive)\n\n        Returns:\n            Usage report for the time range\n        \"\"\"\n        filtered_records = self.usage_history\n\n        if start_time:\n            filtered_records = [r for r in filtered_records if r.timestamp &gt;= start_time]\n\n        if end_time:\n            filtered_records = [r for r in filtered_records if r.timestamp &lt;= end_time]\n\n        if not filtered_records:\n            return {\n                \"total_cost\": 0.0,\n                \"total_tokens\": 0,\n                \"total_requests\": 0,\n                \"model_breakdown\": {},\n            }\n\n        total_cost = sum(r.total_cost for r in filtered_records)\n        total_tokens = sum(r.total_tokens for r in filtered_records)\n\n        # Model breakdown\n        breakdown = {}\n        for record in filtered_records:\n            if record.model not in breakdown:\n                breakdown[record.model] = {\n                    \"cost\": 0.0,\n                    \"input_tokens\": 0,\n                    \"output_tokens\": 0,\n                    \"requests\": 0,\n                }\n\n            breakdown[record.model][\"cost\"] += record.total_cost\n            breakdown[record.model][\"input_tokens\"] += record.input_tokens\n            breakdown[record.model][\"output_tokens\"] += record.output_tokens\n            breakdown[record.model][\"requests\"] += 1\n\n        return {\n            \"total_cost\": total_cost,\n            \"total_tokens\": total_tokens,\n            \"total_requests\": len(filtered_records),\n            \"model_breakdown\": breakdown,\n            \"start_time\": (filtered_records[0].datetime.isoformat() if filtered_records else None),\n            \"end_time\": (filtered_records[-1].datetime.isoformat() if filtered_records else None),\n        }\n\n    def check_budget(self, budget_limit: float) -&gt; dict:\n        \"\"\"\n        Check if spending is within budget.\n\n        Args:\n            budget_limit: Budget limit in currency units\n\n        Returns:\n            Budget status information\n        \"\"\"\n        total_cost = self.get_total_cost()\n        remaining = budget_limit - total_cost\n        percent_used = (total_cost / budget_limit * 100) if budget_limit &gt; 0 else 0\n\n        return {\n            \"budget_limit\": budget_limit,\n            \"total_spent\": total_cost,\n            \"remaining\": remaining,\n            \"percent_used\": percent_used,\n            \"within_budget\": total_cost &lt;= budget_limit,\n            \"near_limit\": percent_used &gt;= 90,  # Warning at 90%\n        }\n</code></pre>"},{"location":"api/base/#bruno_llm.base.CostTracker.__init__","title":"<code>__init__(provider_name, pricing, currency='USD')</code>","text":"<p>Initialize cost tracker.</p> <p>Parameters:</p> Name Type Description Default <code>provider_name</code> <code>str</code> <p>Name of the provider</p> required <code>pricing</code> <code>dict[str, dict[str, float]]</code> <p>Pricing per model (per 1K tokens) Format: {\"model_name\": {\"input\": price, \"output\": price}}</p> required <code>currency</code> <code>str</code> <p>Currency code (default: \"USD\")</p> <code>'USD'</code> Source code in <code>bruno_llm/base/cost_tracker.py</code> <pre><code>def __init__(\n    self,\n    provider_name: str,\n    pricing: dict[str, dict[str, float]],\n    currency: str = \"USD\",\n):\n    \"\"\"\n    Initialize cost tracker.\n\n    Args:\n        provider_name: Name of the provider\n        pricing: Pricing per model (per 1K tokens)\n            Format: {\"model_name\": {\"input\": price, \"output\": price}}\n        currency: Currency code (default: \"USD\")\n    \"\"\"\n    self.provider_name = provider_name\n    self.pricing = pricing\n    self.currency = currency\n    self.usage_history: list[UsageRecord] = []\n</code></pre>"},{"location":"api/base/#bruno_llm.base.CostTracker.track_request","title":"<code>track_request(model, input_tokens, output_tokens, metadata=None)</code>","text":"<p>Track a single API request.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>str</code> <p>Model name</p> required <code>input_tokens</code> <code>int</code> <p>Number of input tokens</p> required <code>output_tokens</code> <code>int</code> <p>Number of output tokens</p> required <code>metadata</code> <code>Optional[dict[str, str]]</code> <p>Optional metadata about the request</p> <code>None</code> <p>Returns:</p> Type Description <code>UsageRecord</code> <p>UsageRecord with calculated costs</p> Source code in <code>bruno_llm/base/cost_tracker.py</code> <pre><code>def track_request(\n    self,\n    model: str,\n    input_tokens: int,\n    output_tokens: int,\n    metadata: Optional[dict[str, str]] = None,\n) -&gt; UsageRecord:\n    \"\"\"\n    Track a single API request.\n\n    Args:\n        model: Model name\n        input_tokens: Number of input tokens\n        output_tokens: Number of output tokens\n        metadata: Optional metadata about the request\n\n    Returns:\n        UsageRecord with calculated costs\n    \"\"\"\n    # Get pricing for model (with fallback to default if available)\n    model_pricing = self.pricing.get(model, self.pricing.get(\"default\", {}))\n\n    # Calculate costs (pricing is per 1K tokens)\n    input_cost = (input_tokens / 1000.0) * model_pricing.get(\"input\", 0.0)\n    output_cost = (output_tokens / 1000.0) * model_pricing.get(\"output\", 0.0)\n    total_cost = input_cost + output_cost\n\n    # Create record\n    record = UsageRecord(\n        timestamp=time.time(),\n        model=model,\n        input_tokens=input_tokens,\n        output_tokens=output_tokens,\n        input_cost=input_cost,\n        output_cost=output_cost,\n        total_cost=total_cost,\n        metadata=metadata or {},\n    )\n\n    self.usage_history.append(record)\n    return record\n</code></pre>"},{"location":"api/base/#bruno_llm.base.CostTracker.get_total_cost","title":"<code>get_total_cost(model=None)</code>","text":"<p>Get total cost across all requests.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>Optional[str]</code> <p>Optional model filter (None = all models)</p> <code>None</code> <p>Returns:</p> Type Description <code>float</code> <p>Total cost in the configured currency</p> Source code in <code>bruno_llm/base/cost_tracker.py</code> <pre><code>def get_total_cost(self, model: Optional[str] = None) -&gt; float:\n    \"\"\"\n    Get total cost across all requests.\n\n    Args:\n        model: Optional model filter (None = all models)\n\n    Returns:\n        Total cost in the configured currency\n    \"\"\"\n    total = 0.0\n    for record in self.usage_history:\n        if model is None or record.model == model:\n            total += record.total_cost\n    return total\n</code></pre>"},{"location":"api/base/#bruno_llm.base.CostTracker.get_total_tokens","title":"<code>get_total_tokens(model=None)</code>","text":"<p>Get total tokens used.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>Optional[str]</code> <p>Optional model filter (None = all models)</p> <code>None</code> <p>Returns:</p> Type Description <code>dict[str, int]</code> <p>Dict with input, output, and total token counts</p> Source code in <code>bruno_llm/base/cost_tracker.py</code> <pre><code>def get_total_tokens(self, model: Optional[str] = None) -&gt; dict[str, int]:\n    \"\"\"\n    Get total tokens used.\n\n    Args:\n        model: Optional model filter (None = all models)\n\n    Returns:\n        Dict with input, output, and total token counts\n    \"\"\"\n    input_tokens = 0\n    output_tokens = 0\n\n    for record in self.usage_history:\n        if model is None or record.model == model:\n            input_tokens += record.input_tokens\n            output_tokens += record.output_tokens\n\n    return {\n        \"input\": input_tokens,\n        \"output\": output_tokens,\n        \"total\": input_tokens + output_tokens,\n    }\n</code></pre>"},{"location":"api/base/#bruno_llm.base.CostTracker.get_request_count","title":"<code>get_request_count(model=None)</code>","text":"<p>Get number of requests made.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>Optional[str]</code> <p>Optional model filter (None = all models)</p> <code>None</code> <p>Returns:</p> Type Description <code>int</code> <p>Number of requests</p> Source code in <code>bruno_llm/base/cost_tracker.py</code> <pre><code>def get_request_count(self, model: Optional[str] = None) -&gt; int:\n    \"\"\"\n    Get number of requests made.\n\n    Args:\n        model: Optional model filter (None = all models)\n\n    Returns:\n        Number of requests\n    \"\"\"\n    if model is None:\n        return len(self.usage_history)\n    return sum(1 for r in self.usage_history if r.model == model)\n</code></pre>"},{"location":"api/base/#bruno_llm.base.CostTracker.get_model_breakdown","title":"<code>get_model_breakdown()</code>","text":"<p>Get cost breakdown by model.</p> <p>Returns:</p> Type Description <code>dict[str, dict[str, float]]</code> <p>Dict mapping model names to their usage statistics</p> Source code in <code>bruno_llm/base/cost_tracker.py</code> <pre><code>def get_model_breakdown(self) -&gt; dict[str, dict[str, float]]:\n    \"\"\"\n    Get cost breakdown by model.\n\n    Returns:\n        Dict mapping model names to their usage statistics\n    \"\"\"\n    breakdown: dict[str, dict[str, float]] = {}\n\n    for record in self.usage_history:\n        if record.model not in breakdown:\n            breakdown[record.model] = {\n                \"cost\": 0.0,\n                \"input_tokens\": 0,\n                \"output_tokens\": 0,\n                \"requests\": 0,\n            }\n\n        breakdown[record.model][\"cost\"] += record.total_cost\n        breakdown[record.model][\"input_tokens\"] += record.input_tokens\n        breakdown[record.model][\"output_tokens\"] += record.output_tokens\n        breakdown[record.model][\"requests\"] += 1\n\n    return breakdown\n</code></pre>"},{"location":"api/base/#bruno_llm.base.CostTracker.get_usage_report","title":"<code>get_usage_report()</code>","text":"<p>Get comprehensive usage report.</p> <p>Returns:</p> Type Description <code>dict</code> <p>Dict with complete usage statistics</p> Source code in <code>bruno_llm/base/cost_tracker.py</code> <pre><code>def get_usage_report(self) -&gt; dict:\n    \"\"\"\n    Get comprehensive usage report.\n\n    Returns:\n        Dict with complete usage statistics\n    \"\"\"\n    return {\n        \"provider\": self.provider_name,\n        \"currency\": self.currency,\n        \"total_cost\": self.get_total_cost(),\n        \"total_requests\": self.get_request_count(),\n        \"total_tokens\": self.get_total_tokens(),\n        \"model_breakdown\": self.get_model_breakdown(),\n        \"first_request\": (\n            self.usage_history[0].datetime.isoformat() if self.usage_history else None\n        ),\n        \"last_request\": (\n            self.usage_history[-1].datetime.isoformat() if self.usage_history else None\n        ),\n    }\n</code></pre>"},{"location":"api/base/#bruno_llm.base.CostTracker.clear_history","title":"<code>clear_history()</code>","text":"<p>Clear all usage history.</p> Source code in <code>bruno_llm/base/cost_tracker.py</code> <pre><code>def clear_history(self) -&gt; None:\n    \"\"\"Clear all usage history.\"\"\"\n    self.usage_history.clear()\n</code></pre>"},{"location":"api/base/#bruno_llm.base.CostTracker.export_history","title":"<code>export_history()</code>","text":"<p>Export usage history as list of dicts.</p> <p>Returns:</p> Type Description <code>list[dict]</code> <p>List of usage records as dictionaries</p> Source code in <code>bruno_llm/base/cost_tracker.py</code> <pre><code>def export_history(self) -&gt; list[dict]:\n    \"\"\"\n    Export usage history as list of dicts.\n\n    Returns:\n        List of usage records as dictionaries\n    \"\"\"\n    return [\n        {\n            \"timestamp\": record.datetime.isoformat(),\n            \"model\": record.model,\n            \"input_tokens\": record.input_tokens,\n            \"output_tokens\": record.output_tokens,\n            \"input_cost\": record.input_cost,\n            \"output_cost\": record.output_cost,\n            \"total_cost\": record.total_cost,\n            \"metadata\": record.metadata,\n        }\n        for record in self.usage_history\n    ]\n</code></pre>"},{"location":"api/base/#bruno_llm.base.CostTracker.export_to_csv","title":"<code>export_to_csv(filepath)</code>","text":"<p>Export usage history to CSV file.</p> <p>Parameters:</p> Name Type Description Default <code>filepath</code> <code>str</code> <p>Path to output CSV file</p> required Source code in <code>bruno_llm/base/cost_tracker.py</code> <pre><code>def export_to_csv(self, filepath: str) -&gt; None:\n    \"\"\"\n    Export usage history to CSV file.\n\n    Args:\n        filepath: Path to output CSV file\n    \"\"\"\n    import csv\n\n    if not self.usage_history:\n        return\n\n    with open(filepath, \"w\", newline=\"\", encoding=\"utf-8\") as f:\n        writer = csv.writer(f)\n        writer.writerow(\n            [\n                \"Timestamp\",\n                \"Model\",\n                \"Input Tokens\",\n                \"Output Tokens\",\n                \"Total Tokens\",\n                \"Input Cost\",\n                \"Output Cost\",\n                \"Total Cost\",\n            ]\n        )\n\n        for record in self.usage_history:\n            writer.writerow(\n                [\n                    record.datetime.isoformat(),\n                    record.model,\n                    record.input_tokens,\n                    record.output_tokens,\n                    record.total_tokens,\n                    f\"{record.input_cost:.6f}\",\n                    f\"{record.output_cost:.6f}\",\n                    f\"{record.total_cost:.6f}\",\n                ]\n            )\n</code></pre>"},{"location":"api/base/#bruno_llm.base.CostTracker.export_to_json","title":"<code>export_to_json(filepath)</code>","text":"<p>Export usage history to JSON file.</p> <p>Parameters:</p> Name Type Description Default <code>filepath</code> <code>str</code> <p>Path to output JSON file</p> required Source code in <code>bruno_llm/base/cost_tracker.py</code> <pre><code>def export_to_json(self, filepath: str) -&gt; None:\n    \"\"\"\n    Export usage history to JSON file.\n\n    Args:\n        filepath: Path to output JSON file\n    \"\"\"\n    import json\n\n    data = {\n        \"provider\": self.provider_name,\n        \"currency\": self.currency,\n        \"export_date\": datetime.now().isoformat(),\n        \"summary\": self.get_usage_report(),\n        \"history\": self.export_history(),\n    }\n\n    with open(filepath, \"w\", encoding=\"utf-8\") as f:\n        json.dump(data, f, indent=2)\n</code></pre>"},{"location":"api/base/#bruno_llm.base.CostTracker.get_time_range_report","title":"<code>get_time_range_report(start_time=None, end_time=None)</code>","text":"<p>Get usage report for a specific time range.</p> <p>Parameters:</p> Name Type Description Default <code>start_time</code> <code>Optional[float]</code> <p>Start timestamp (inclusive)</p> <code>None</code> <code>end_time</code> <code>Optional[float]</code> <p>End timestamp (inclusive)</p> <code>None</code> <p>Returns:</p> Type Description <code>dict</code> <p>Usage report for the time range</p> Source code in <code>bruno_llm/base/cost_tracker.py</code> <pre><code>def get_time_range_report(\n    self,\n    start_time: Optional[float] = None,\n    end_time: Optional[float] = None,\n) -&gt; dict:\n    \"\"\"\n    Get usage report for a specific time range.\n\n    Args:\n        start_time: Start timestamp (inclusive)\n        end_time: End timestamp (inclusive)\n\n    Returns:\n        Usage report for the time range\n    \"\"\"\n    filtered_records = self.usage_history\n\n    if start_time:\n        filtered_records = [r for r in filtered_records if r.timestamp &gt;= start_time]\n\n    if end_time:\n        filtered_records = [r for r in filtered_records if r.timestamp &lt;= end_time]\n\n    if not filtered_records:\n        return {\n            \"total_cost\": 0.0,\n            \"total_tokens\": 0,\n            \"total_requests\": 0,\n            \"model_breakdown\": {},\n        }\n\n    total_cost = sum(r.total_cost for r in filtered_records)\n    total_tokens = sum(r.total_tokens for r in filtered_records)\n\n    # Model breakdown\n    breakdown = {}\n    for record in filtered_records:\n        if record.model not in breakdown:\n            breakdown[record.model] = {\n                \"cost\": 0.0,\n                \"input_tokens\": 0,\n                \"output_tokens\": 0,\n                \"requests\": 0,\n            }\n\n        breakdown[record.model][\"cost\"] += record.total_cost\n        breakdown[record.model][\"input_tokens\"] += record.input_tokens\n        breakdown[record.model][\"output_tokens\"] += record.output_tokens\n        breakdown[record.model][\"requests\"] += 1\n\n    return {\n        \"total_cost\": total_cost,\n        \"total_tokens\": total_tokens,\n        \"total_requests\": len(filtered_records),\n        \"model_breakdown\": breakdown,\n        \"start_time\": (filtered_records[0].datetime.isoformat() if filtered_records else None),\n        \"end_time\": (filtered_records[-1].datetime.isoformat() if filtered_records else None),\n    }\n</code></pre>"},{"location":"api/base/#bruno_llm.base.CostTracker.check_budget","title":"<code>check_budget(budget_limit)</code>","text":"<p>Check if spending is within budget.</p> <p>Parameters:</p> Name Type Description Default <code>budget_limit</code> <code>float</code> <p>Budget limit in currency units</p> required <p>Returns:</p> Type Description <code>dict</code> <p>Budget status information</p> Source code in <code>bruno_llm/base/cost_tracker.py</code> <pre><code>def check_budget(self, budget_limit: float) -&gt; dict:\n    \"\"\"\n    Check if spending is within budget.\n\n    Args:\n        budget_limit: Budget limit in currency units\n\n    Returns:\n        Budget status information\n    \"\"\"\n    total_cost = self.get_total_cost()\n    remaining = budget_limit - total_cost\n    percent_used = (total_cost / budget_limit * 100) if budget_limit &gt; 0 else 0\n\n    return {\n        \"budget_limit\": budget_limit,\n        \"total_spent\": total_cost,\n        \"remaining\": remaining,\n        \"percent_used\": percent_used,\n        \"within_budget\": total_cost &lt;= budget_limit,\n        \"near_limit\": percent_used &gt;= 90,  # Warning at 90%\n    }\n</code></pre>"},{"location":"api/base/#bruno_llm.base.UsageRecord","title":"<code>UsageRecord</code>  <code>dataclass</code>","text":"<p>Record of a single API usage event.</p> <p>Attributes:</p> Name Type Description <code>timestamp</code> <code>float</code> <p>When the request was made</p> <code>model</code> <code>str</code> <p>Model name used</p> <code>input_tokens</code> <code>int</code> <p>Number of input tokens</p> <code>output_tokens</code> <code>int</code> <p>Number of output tokens</p> <code>input_cost</code> <code>float</code> <p>Cost for input tokens</p> <code>output_cost</code> <code>float</code> <p>Cost for output tokens</p> <code>total_cost</code> <code>float</code> <p>Total cost for this request</p> Source code in <code>bruno_llm/base/cost_tracker.py</code> <pre><code>@dataclass\nclass UsageRecord:\n    \"\"\"\n    Record of a single API usage event.\n\n    Attributes:\n        timestamp: When the request was made\n        model: Model name used\n        input_tokens: Number of input tokens\n        output_tokens: Number of output tokens\n        input_cost: Cost for input tokens\n        output_cost: Cost for output tokens\n        total_cost: Total cost for this request\n    \"\"\"\n\n    timestamp: float\n    model: str\n    input_tokens: int\n    output_tokens: int\n    input_cost: float\n    output_cost: float\n    total_cost: float\n    metadata: dict[str, str] = field(default_factory=dict)\n\n    @property\n    def datetime(self) -&gt; datetime:\n        \"\"\"Get datetime from timestamp.\"\"\"\n        return datetime.fromtimestamp(self.timestamp)\n\n    @property\n    def total_tokens(self) -&gt; int:\n        \"\"\"Get total tokens used.\"\"\"\n        return self.input_tokens + self.output_tokens\n</code></pre>"},{"location":"api/base/#bruno_llm.base.UsageRecord.datetime","title":"<code>datetime</code>  <code>property</code>","text":"<p>Get datetime from timestamp.</p>"},{"location":"api/base/#bruno_llm.base.UsageRecord.total_tokens","title":"<code>total_tokens</code>  <code>property</code>","text":"<p>Get total tokens used.</p>"},{"location":"api/base/#bruno_llm.base.CachingMiddleware","title":"<code>CachingMiddleware</code>","text":"<p>               Bases: <code>Middleware</code></p> <p>Cache responses using ResponseCache.</p> <p>Parameters:</p> Name Type Description Default <code>cache</code> <p>ResponseCache instance</p> required <code>cache_streaming</code> <code>bool</code> <p>Whether to cache streaming responses</p> <code>True</code> Example <p>from bruno_llm.base.cache import ResponseCache cache = ResponseCache(max_size=100, ttl=300) middleware = CachingMiddleware(cache)</p> Source code in <code>bruno_llm/base/middleware.py</code> <pre><code>class CachingMiddleware(Middleware):\n    \"\"\"\n    Cache responses using ResponseCache.\n\n    Args:\n        cache: ResponseCache instance\n        cache_streaming: Whether to cache streaming responses\n\n    Example:\n        &gt;&gt;&gt; from bruno_llm.base.cache import ResponseCache\n        &gt;&gt;&gt; cache = ResponseCache(max_size=100, ttl=300)\n        &gt;&gt;&gt; middleware = CachingMiddleware(cache)\n    \"\"\"\n\n    def __init__(self, cache, cache_streaming: bool = True):\n        \"\"\"\n        Initialize caching middleware.\n\n        Args:\n            cache: ResponseCache instance\n            cache_streaming: Whether to cache streaming responses\n        \"\"\"\n        self.cache = cache\n        self.cache_streaming = cache_streaming\n        self._current_stream_chunks: Optional[list[str]] = None\n\n    async def before_request(\n        self, messages: list[Message], **kwargs: Any\n    ) -&gt; tuple[list[Message], dict[str, Any]]:\n        \"\"\"Check cache before request.\"\"\"\n        # Cache lookup is handled externally\n        return messages, kwargs\n\n    async def after_response(self, messages: list[Message], response: str, **kwargs: Any) -&gt; str:\n        \"\"\"Cache response after receiving.\"\"\"\n        self.cache.set(messages, response, **kwargs)\n        return response\n\n    async def on_stream_chunk(self, chunk: str, **kwargs: Any) -&gt; str:\n        \"\"\"Collect stream chunks for caching.\"\"\"\n        if self.cache_streaming:\n            if self._current_stream_chunks is None:\n                self._current_stream_chunks = []\n            self._current_stream_chunks.append(chunk)\n\n        return chunk\n</code></pre>"},{"location":"api/base/#bruno_llm.base.CachingMiddleware.__init__","title":"<code>__init__(cache, cache_streaming=True)</code>","text":"<p>Initialize caching middleware.</p> <p>Parameters:</p> Name Type Description Default <code>cache</code> <p>ResponseCache instance</p> required <code>cache_streaming</code> <code>bool</code> <p>Whether to cache streaming responses</p> <code>True</code> Source code in <code>bruno_llm/base/middleware.py</code> <pre><code>def __init__(self, cache, cache_streaming: bool = True):\n    \"\"\"\n    Initialize caching middleware.\n\n    Args:\n        cache: ResponseCache instance\n        cache_streaming: Whether to cache streaming responses\n    \"\"\"\n    self.cache = cache\n    self.cache_streaming = cache_streaming\n    self._current_stream_chunks: Optional[list[str]] = None\n</code></pre>"},{"location":"api/base/#bruno_llm.base.CachingMiddleware.before_request","title":"<code>before_request(messages, **kwargs)</code>  <code>async</code>","text":"<p>Check cache before request.</p> Source code in <code>bruno_llm/base/middleware.py</code> <pre><code>async def before_request(\n    self, messages: list[Message], **kwargs: Any\n) -&gt; tuple[list[Message], dict[str, Any]]:\n    \"\"\"Check cache before request.\"\"\"\n    # Cache lookup is handled externally\n    return messages, kwargs\n</code></pre>"},{"location":"api/base/#bruno_llm.base.CachingMiddleware.after_response","title":"<code>after_response(messages, response, **kwargs)</code>  <code>async</code>","text":"<p>Cache response after receiving.</p> Source code in <code>bruno_llm/base/middleware.py</code> <pre><code>async def after_response(self, messages: list[Message], response: str, **kwargs: Any) -&gt; str:\n    \"\"\"Cache response after receiving.\"\"\"\n    self.cache.set(messages, response, **kwargs)\n    return response\n</code></pre>"},{"location":"api/base/#bruno_llm.base.CachingMiddleware.on_stream_chunk","title":"<code>on_stream_chunk(chunk, **kwargs)</code>  <code>async</code>","text":"<p>Collect stream chunks for caching.</p> Source code in <code>bruno_llm/base/middleware.py</code> <pre><code>async def on_stream_chunk(self, chunk: str, **kwargs: Any) -&gt; str:\n    \"\"\"Collect stream chunks for caching.\"\"\"\n    if self.cache_streaming:\n        if self._current_stream_chunks is None:\n            self._current_stream_chunks = []\n        self._current_stream_chunks.append(chunk)\n\n    return chunk\n</code></pre>"},{"location":"api/base/#bruno_llm.base.LoggingMiddleware","title":"<code>LoggingMiddleware</code>","text":"<p>               Bases: <code>Middleware</code></p> <p>Log all requests and responses.</p> <p>Parameters:</p> Name Type Description Default <code>logger</code> <p>Logger instance (defaults to structlog)</p> <code>None</code> <code>log_messages</code> <code>bool</code> <p>Whether to log full message content</p> <code>False</code> Example <p>middleware = LoggingMiddleware(log_messages=False) provider = MiddlewareProvider(base_provider, [middleware])</p> Source code in <code>bruno_llm/base/middleware.py</code> <pre><code>class LoggingMiddleware(Middleware):\n    \"\"\"\n    Log all requests and responses.\n\n    Args:\n        logger: Logger instance (defaults to structlog)\n        log_messages: Whether to log full message content\n\n    Example:\n        &gt;&gt;&gt; middleware = LoggingMiddleware(log_messages=False)\n        &gt;&gt;&gt; provider = MiddlewareProvider(base_provider, [middleware])\n    \"\"\"\n\n    def __init__(self, logger=None, log_messages: bool = False):\n        \"\"\"\n        Initialize logging middleware.\n\n        Args:\n            logger: Logger instance\n            log_messages: Whether to log message content\n        \"\"\"\n        self.log_messages = log_messages\n\n        if logger is None:\n            import structlog\n\n            self.logger = structlog.get_logger(__name__)\n        else:\n            self.logger = logger\n\n    async def before_request(\n        self, messages: list[Message], **kwargs: Any\n    ) -&gt; tuple[list[Message], dict[str, Any]]:\n        \"\"\"Log before request.\"\"\"\n        log_data = {\n            \"event\": \"llm_request\",\n            \"message_count\": len(messages),\n            \"params\": {k: v for k, v in kwargs.items() if k not in [\"api_key\"]},\n        }\n\n        if self.log_messages:\n            log_data[\"messages\"] = [{\"role\": m.role.value, \"content\": m.content} for m in messages]\n\n        self.logger.info(\"LLM request\", **log_data)\n        return messages, kwargs\n\n    async def after_response(self, messages: list[Message], response: str, **kwargs: Any) -&gt; str:\n        \"\"\"Log after response.\"\"\"\n        log_data = {\n            \"event\": \"llm_response\",\n            \"response_length\": len(response),\n        }\n\n        if self.log_messages:\n            log_data[\"response\"] = response\n\n        self.logger.info(\"LLM response\", **log_data)\n        return response\n\n    async def on_error(self, error: Exception, messages: list[Message], **kwargs: Any) -&gt; None:\n        \"\"\"Log errors.\"\"\"\n        self.logger.error(\n            \"LLM error\",\n            event=\"llm_error\",\n            error=str(error),\n            error_type=type(error).__name__,\n            message_count=len(messages),\n        )\n</code></pre>"},{"location":"api/base/#bruno_llm.base.LoggingMiddleware.__init__","title":"<code>__init__(logger=None, log_messages=False)</code>","text":"<p>Initialize logging middleware.</p> <p>Parameters:</p> Name Type Description Default <code>logger</code> <p>Logger instance</p> <code>None</code> <code>log_messages</code> <code>bool</code> <p>Whether to log message content</p> <code>False</code> Source code in <code>bruno_llm/base/middleware.py</code> <pre><code>def __init__(self, logger=None, log_messages: bool = False):\n    \"\"\"\n    Initialize logging middleware.\n\n    Args:\n        logger: Logger instance\n        log_messages: Whether to log message content\n    \"\"\"\n    self.log_messages = log_messages\n\n    if logger is None:\n        import structlog\n\n        self.logger = structlog.get_logger(__name__)\n    else:\n        self.logger = logger\n</code></pre>"},{"location":"api/base/#bruno_llm.base.LoggingMiddleware.before_request","title":"<code>before_request(messages, **kwargs)</code>  <code>async</code>","text":"<p>Log before request.</p> Source code in <code>bruno_llm/base/middleware.py</code> <pre><code>async def before_request(\n    self, messages: list[Message], **kwargs: Any\n) -&gt; tuple[list[Message], dict[str, Any]]:\n    \"\"\"Log before request.\"\"\"\n    log_data = {\n        \"event\": \"llm_request\",\n        \"message_count\": len(messages),\n        \"params\": {k: v for k, v in kwargs.items() if k not in [\"api_key\"]},\n    }\n\n    if self.log_messages:\n        log_data[\"messages\"] = [{\"role\": m.role.value, \"content\": m.content} for m in messages]\n\n    self.logger.info(\"LLM request\", **log_data)\n    return messages, kwargs\n</code></pre>"},{"location":"api/base/#bruno_llm.base.LoggingMiddleware.after_response","title":"<code>after_response(messages, response, **kwargs)</code>  <code>async</code>","text":"<p>Log after response.</p> Source code in <code>bruno_llm/base/middleware.py</code> <pre><code>async def after_response(self, messages: list[Message], response: str, **kwargs: Any) -&gt; str:\n    \"\"\"Log after response.\"\"\"\n    log_data = {\n        \"event\": \"llm_response\",\n        \"response_length\": len(response),\n    }\n\n    if self.log_messages:\n        log_data[\"response\"] = response\n\n    self.logger.info(\"LLM response\", **log_data)\n    return response\n</code></pre>"},{"location":"api/base/#bruno_llm.base.LoggingMiddleware.on_error","title":"<code>on_error(error, messages, **kwargs)</code>  <code>async</code>","text":"<p>Log errors.</p> Source code in <code>bruno_llm/base/middleware.py</code> <pre><code>async def on_error(self, error: Exception, messages: list[Message], **kwargs: Any) -&gt; None:\n    \"\"\"Log errors.\"\"\"\n    self.logger.error(\n        \"LLM error\",\n        event=\"llm_error\",\n        error=str(error),\n        error_type=type(error).__name__,\n        message_count=len(messages),\n    )\n</code></pre>"},{"location":"api/base/#bruno_llm.base.Middleware","title":"<code>Middleware</code>","text":"<p>               Bases: <code>ABC</code></p> <p>Base class for provider middleware.</p> <p>Middleware can intercept and modify: - Request messages before sending to provider - Response text after receiving from provider - Streaming chunks as they arrive - Request parameters (temperature, max_tokens, etc.)</p> Example <p>class LoggingMiddleware(Middleware): ...     async def before_request(self, messages, kwargs): ...         print(f\"Sending {len(messages)} messages\") ...         return messages, kwargs ... ...     async def after_response(self, messages, response, kwargs): ...         print(f\"Received {len(response)} chars\") ...         return response</p> Source code in <code>bruno_llm/base/middleware.py</code> <pre><code>class Middleware(ABC):\n    \"\"\"\n    Base class for provider middleware.\n\n    Middleware can intercept and modify:\n    - Request messages before sending to provider\n    - Response text after receiving from provider\n    - Streaming chunks as they arrive\n    - Request parameters (temperature, max_tokens, etc.)\n\n    Example:\n        &gt;&gt;&gt; class LoggingMiddleware(Middleware):\n        ...     async def before_request(self, messages, **kwargs):\n        ...         print(f\"Sending {len(messages)} messages\")\n        ...         return messages, kwargs\n        ...\n        ...     async def after_response(self, messages, response, **kwargs):\n        ...         print(f\"Received {len(response)} chars\")\n        ...         return response\n    \"\"\"\n\n    @abstractmethod\n    async def before_request(\n        self, messages: list[Message], **kwargs: Any\n    ) -&gt; tuple[list[Message], dict[str, Any]]:\n        \"\"\"\n        Process messages and parameters before request.\n\n        Args:\n            messages: Input messages\n            **kwargs: Request parameters\n\n        Returns:\n            Tuple of (modified_messages, modified_kwargs)\n        \"\"\"\n        pass\n\n    @abstractmethod\n    async def after_response(self, messages: list[Message], response: str, **kwargs: Any) -&gt; str:\n        \"\"\"\n        Process response after receiving.\n\n        Args:\n            messages: Original input messages\n            response: Provider response\n            **kwargs: Request parameters used\n\n        Returns:\n            Modified response\n        \"\"\"\n        pass\n\n    async def on_stream_chunk(self, chunk: str, **kwargs: Any) -&gt; str:\n        \"\"\"\n        Process individual stream chunks.\n\n        Args:\n            chunk: Stream chunk\n            **kwargs: Request parameters\n\n        Returns:\n            Modified chunk\n        \"\"\"\n        return chunk\n\n    async def on_error(self, error: Exception, messages: list[Message], **kwargs: Any) -&gt; None:  # noqa: B027\n        \"\"\"\n        Handle errors during request.\n\n        Args:\n            error: The exception that occurred\n            messages: Messages that were being processed\n            **kwargs: Request parameters\n        \"\"\"\n        pass\n</code></pre>"},{"location":"api/base/#bruno_llm.base.Middleware.before_request","title":"<code>before_request(messages, **kwargs)</code>  <code>abstractmethod</code> <code>async</code>","text":"<p>Process messages and parameters before request.</p> <p>Parameters:</p> Name Type Description Default <code>messages</code> <code>list[Message]</code> <p>Input messages</p> required <code>**kwargs</code> <code>Any</code> <p>Request parameters</p> <code>{}</code> <p>Returns:</p> Type Description <code>tuple[list[Message], dict[str, Any]]</code> <p>Tuple of (modified_messages, modified_kwargs)</p> Source code in <code>bruno_llm/base/middleware.py</code> <pre><code>@abstractmethod\nasync def before_request(\n    self, messages: list[Message], **kwargs: Any\n) -&gt; tuple[list[Message], dict[str, Any]]:\n    \"\"\"\n    Process messages and parameters before request.\n\n    Args:\n        messages: Input messages\n        **kwargs: Request parameters\n\n    Returns:\n        Tuple of (modified_messages, modified_kwargs)\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/base/#bruno_llm.base.Middleware.after_response","title":"<code>after_response(messages, response, **kwargs)</code>  <code>abstractmethod</code> <code>async</code>","text":"<p>Process response after receiving.</p> <p>Parameters:</p> Name Type Description Default <code>messages</code> <code>list[Message]</code> <p>Original input messages</p> required <code>response</code> <code>str</code> <p>Provider response</p> required <code>**kwargs</code> <code>Any</code> <p>Request parameters used</p> <code>{}</code> <p>Returns:</p> Type Description <code>str</code> <p>Modified response</p> Source code in <code>bruno_llm/base/middleware.py</code> <pre><code>@abstractmethod\nasync def after_response(self, messages: list[Message], response: str, **kwargs: Any) -&gt; str:\n    \"\"\"\n    Process response after receiving.\n\n    Args:\n        messages: Original input messages\n        response: Provider response\n        **kwargs: Request parameters used\n\n    Returns:\n        Modified response\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/base/#bruno_llm.base.Middleware.on_stream_chunk","title":"<code>on_stream_chunk(chunk, **kwargs)</code>  <code>async</code>","text":"<p>Process individual stream chunks.</p> <p>Parameters:</p> Name Type Description Default <code>chunk</code> <code>str</code> <p>Stream chunk</p> required <code>**kwargs</code> <code>Any</code> <p>Request parameters</p> <code>{}</code> <p>Returns:</p> Type Description <code>str</code> <p>Modified chunk</p> Source code in <code>bruno_llm/base/middleware.py</code> <pre><code>async def on_stream_chunk(self, chunk: str, **kwargs: Any) -&gt; str:\n    \"\"\"\n    Process individual stream chunks.\n\n    Args:\n        chunk: Stream chunk\n        **kwargs: Request parameters\n\n    Returns:\n        Modified chunk\n    \"\"\"\n    return chunk\n</code></pre>"},{"location":"api/base/#bruno_llm.base.Middleware.on_error","title":"<code>on_error(error, messages, **kwargs)</code>  <code>async</code>","text":"<p>Handle errors during request.</p> <p>Parameters:</p> Name Type Description Default <code>error</code> <code>Exception</code> <p>The exception that occurred</p> required <code>messages</code> <code>list[Message]</code> <p>Messages that were being processed</p> required <code>**kwargs</code> <code>Any</code> <p>Request parameters</p> <code>{}</code> Source code in <code>bruno_llm/base/middleware.py</code> <pre><code>async def on_error(self, error: Exception, messages: list[Message], **kwargs: Any) -&gt; None:  # noqa: B027\n    \"\"\"\n    Handle errors during request.\n\n    Args:\n        error: The exception that occurred\n        messages: Messages that were being processed\n        **kwargs: Request parameters\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/base/#bruno_llm.base.MiddlewareChain","title":"<code>MiddlewareChain</code>","text":"<p>Chain multiple middleware together.</p> <p>Executes middleware in order for before_request, and in reverse order for after_response.</p> <p>Parameters:</p> Name Type Description Default <code>middlewares</code> <code>list[Middleware]</code> <p>List of middleware instances</p> required Example <p>chain = MiddlewareChain([ ...     LoggingMiddleware(), ...     ValidationMiddleware(), ...     CachingMiddleware(cache), ... ]) messages, kwargs = await chain.before_request(messages, **kwargs)</p> Source code in <code>bruno_llm/base/middleware.py</code> <pre><code>class MiddlewareChain:\n    \"\"\"\n    Chain multiple middleware together.\n\n    Executes middleware in order for before_request,\n    and in reverse order for after_response.\n\n    Args:\n        middlewares: List of middleware instances\n\n    Example:\n        &gt;&gt;&gt; chain = MiddlewareChain([\n        ...     LoggingMiddleware(),\n        ...     ValidationMiddleware(),\n        ...     CachingMiddleware(cache),\n        ... ])\n        &gt;&gt;&gt; messages, kwargs = await chain.before_request(messages, **kwargs)\n    \"\"\"\n\n    def __init__(self, middlewares: list[Middleware]):\n        \"\"\"\n        Initialize middleware chain.\n\n        Args:\n            middlewares: List of middleware to chain\n        \"\"\"\n        self.middlewares = middlewares\n\n    async def before_request(\n        self, messages: list[Message], **kwargs: Any\n    ) -&gt; tuple[list[Message], dict[str, Any]]:\n        \"\"\"Execute all middleware before_request in order.\"\"\"\n        for middleware in self.middlewares:\n            messages, kwargs = await middleware.before_request(messages, **kwargs)\n        return messages, kwargs\n\n    async def after_response(self, messages: list[Message], response: str, **kwargs: Any) -&gt; str:\n        \"\"\"Execute all middleware after_response in reverse order.\"\"\"\n        for middleware in reversed(self.middlewares):\n            response = await middleware.after_response(messages, response, **kwargs)\n        return response\n\n    async def on_stream_chunk(self, chunk: str, **kwargs: Any) -&gt; str:\n        \"\"\"Execute all middleware on_stream_chunk in order.\"\"\"\n        for middleware in self.middlewares:\n            chunk = await middleware.on_stream_chunk(chunk, **kwargs)\n        return chunk\n\n    async def on_error(self, error: Exception, messages: list[Message], **kwargs: Any) -&gt; None:\n        \"\"\"Execute all middleware on_error.\"\"\"\n        for middleware in self.middlewares:\n            await middleware.on_error(error, messages, **kwargs)\n</code></pre>"},{"location":"api/base/#bruno_llm.base.MiddlewareChain.__init__","title":"<code>__init__(middlewares)</code>","text":"<p>Initialize middleware chain.</p> <p>Parameters:</p> Name Type Description Default <code>middlewares</code> <code>list[Middleware]</code> <p>List of middleware to chain</p> required Source code in <code>bruno_llm/base/middleware.py</code> <pre><code>def __init__(self, middlewares: list[Middleware]):\n    \"\"\"\n    Initialize middleware chain.\n\n    Args:\n        middlewares: List of middleware to chain\n    \"\"\"\n    self.middlewares = middlewares\n</code></pre>"},{"location":"api/base/#bruno_llm.base.MiddlewareChain.before_request","title":"<code>before_request(messages, **kwargs)</code>  <code>async</code>","text":"<p>Execute all middleware before_request in order.</p> Source code in <code>bruno_llm/base/middleware.py</code> <pre><code>async def before_request(\n    self, messages: list[Message], **kwargs: Any\n) -&gt; tuple[list[Message], dict[str, Any]]:\n    \"\"\"Execute all middleware before_request in order.\"\"\"\n    for middleware in self.middlewares:\n        messages, kwargs = await middleware.before_request(messages, **kwargs)\n    return messages, kwargs\n</code></pre>"},{"location":"api/base/#bruno_llm.base.MiddlewareChain.after_response","title":"<code>after_response(messages, response, **kwargs)</code>  <code>async</code>","text":"<p>Execute all middleware after_response in reverse order.</p> Source code in <code>bruno_llm/base/middleware.py</code> <pre><code>async def after_response(self, messages: list[Message], response: str, **kwargs: Any) -&gt; str:\n    \"\"\"Execute all middleware after_response in reverse order.\"\"\"\n    for middleware in reversed(self.middlewares):\n        response = await middleware.after_response(messages, response, **kwargs)\n    return response\n</code></pre>"},{"location":"api/base/#bruno_llm.base.MiddlewareChain.on_stream_chunk","title":"<code>on_stream_chunk(chunk, **kwargs)</code>  <code>async</code>","text":"<p>Execute all middleware on_stream_chunk in order.</p> Source code in <code>bruno_llm/base/middleware.py</code> <pre><code>async def on_stream_chunk(self, chunk: str, **kwargs: Any) -&gt; str:\n    \"\"\"Execute all middleware on_stream_chunk in order.\"\"\"\n    for middleware in self.middlewares:\n        chunk = await middleware.on_stream_chunk(chunk, **kwargs)\n    return chunk\n</code></pre>"},{"location":"api/base/#bruno_llm.base.MiddlewareChain.on_error","title":"<code>on_error(error, messages, **kwargs)</code>  <code>async</code>","text":"<p>Execute all middleware on_error.</p> Source code in <code>bruno_llm/base/middleware.py</code> <pre><code>async def on_error(self, error: Exception, messages: list[Message], **kwargs: Any) -&gt; None:\n    \"\"\"Execute all middleware on_error.\"\"\"\n    for middleware in self.middlewares:\n        await middleware.on_error(error, messages, **kwargs)\n</code></pre>"},{"location":"api/base/#bruno_llm.base.RetryMiddleware","title":"<code>RetryMiddleware</code>","text":"<p>               Bases: <code>Middleware</code></p> <p>Add retry logic with exponential backoff.</p> <p>Note: This is typically handled by BaseProvider's retry logic, but can be used for additional retry layers.</p> <p>Parameters:</p> Name Type Description Default <code>max_retries</code> <code>int</code> <p>Maximum number of retries</p> <code>3</code> <code>base_delay</code> <code>float</code> <p>Base delay in seconds</p> <code>1.0</code> Example <p>middleware = RetryMiddleware(max_retries=3, base_delay=1.0)</p> Source code in <code>bruno_llm/base/middleware.py</code> <pre><code>class RetryMiddleware(Middleware):\n    \"\"\"\n    Add retry logic with exponential backoff.\n\n    Note: This is typically handled by BaseProvider's retry logic,\n    but can be used for additional retry layers.\n\n    Args:\n        max_retries: Maximum number of retries\n        base_delay: Base delay in seconds\n\n    Example:\n        &gt;&gt;&gt; middleware = RetryMiddleware(max_retries=3, base_delay=1.0)\n    \"\"\"\n\n    def __init__(self, max_retries: int = 3, base_delay: float = 1.0):\n        \"\"\"\n        Initialize retry middleware.\n\n        Args:\n            max_retries: Maximum retry attempts\n            base_delay: Base delay for exponential backoff\n        \"\"\"\n        self.max_retries = max_retries\n        self.base_delay = base_delay\n        self.retry_count = 0\n\n    async def before_request(\n        self, messages: list[Message], **kwargs: Any\n    ) -&gt; tuple[list[Message], dict[str, Any]]:\n        \"\"\"Reset retry count before request.\"\"\"\n        self.retry_count = 0\n        return messages, kwargs\n\n    async def after_response(self, messages: list[Message], response: str, **kwargs: Any) -&gt; str:\n        \"\"\"No processing after successful response.\"\"\"\n        return response\n\n    async def on_error(self, error: Exception, messages: list[Message], **kwargs: Any) -&gt; None:\n        \"\"\"Handle retry logic on error.\"\"\"\n        import asyncio\n\n        self.retry_count += 1\n\n        if self.retry_count &lt;= self.max_retries:\n            delay = self.base_delay * (2 ** (self.retry_count - 1))\n            await asyncio.sleep(delay)\n</code></pre>"},{"location":"api/base/#bruno_llm.base.RetryMiddleware.__init__","title":"<code>__init__(max_retries=3, base_delay=1.0)</code>","text":"<p>Initialize retry middleware.</p> <p>Parameters:</p> Name Type Description Default <code>max_retries</code> <code>int</code> <p>Maximum retry attempts</p> <code>3</code> <code>base_delay</code> <code>float</code> <p>Base delay for exponential backoff</p> <code>1.0</code> Source code in <code>bruno_llm/base/middleware.py</code> <pre><code>def __init__(self, max_retries: int = 3, base_delay: float = 1.0):\n    \"\"\"\n    Initialize retry middleware.\n\n    Args:\n        max_retries: Maximum retry attempts\n        base_delay: Base delay for exponential backoff\n    \"\"\"\n    self.max_retries = max_retries\n    self.base_delay = base_delay\n    self.retry_count = 0\n</code></pre>"},{"location":"api/base/#bruno_llm.base.RetryMiddleware.before_request","title":"<code>before_request(messages, **kwargs)</code>  <code>async</code>","text":"<p>Reset retry count before request.</p> Source code in <code>bruno_llm/base/middleware.py</code> <pre><code>async def before_request(\n    self, messages: list[Message], **kwargs: Any\n) -&gt; tuple[list[Message], dict[str, Any]]:\n    \"\"\"Reset retry count before request.\"\"\"\n    self.retry_count = 0\n    return messages, kwargs\n</code></pre>"},{"location":"api/base/#bruno_llm.base.RetryMiddleware.after_response","title":"<code>after_response(messages, response, **kwargs)</code>  <code>async</code>","text":"<p>No processing after successful response.</p> Source code in <code>bruno_llm/base/middleware.py</code> <pre><code>async def after_response(self, messages: list[Message], response: str, **kwargs: Any) -&gt; str:\n    \"\"\"No processing after successful response.\"\"\"\n    return response\n</code></pre>"},{"location":"api/base/#bruno_llm.base.RetryMiddleware.on_error","title":"<code>on_error(error, messages, **kwargs)</code>  <code>async</code>","text":"<p>Handle retry logic on error.</p> Source code in <code>bruno_llm/base/middleware.py</code> <pre><code>async def on_error(self, error: Exception, messages: list[Message], **kwargs: Any) -&gt; None:\n    \"\"\"Handle retry logic on error.\"\"\"\n    import asyncio\n\n    self.retry_count += 1\n\n    if self.retry_count &lt;= self.max_retries:\n        delay = self.base_delay * (2 ** (self.retry_count - 1))\n        await asyncio.sleep(delay)\n</code></pre>"},{"location":"api/base/#bruno_llm.base.ValidationMiddleware","title":"<code>ValidationMiddleware</code>","text":"<p>               Bases: <code>Middleware</code></p> <p>Validate messages and parameters.</p> <p>Parameters:</p> Name Type Description Default <code>max_message_length</code> <code>Optional[int]</code> <p>Max length for individual messages</p> <code>None</code> <code>allowed_roles</code> <code>Optional[list[str]]</code> <p>Allowed message roles</p> <code>None</code> <code>required_params</code> <code>Optional[list[str]]</code> <p>Required parameter names</p> <code>None</code> Example <p>middleware = ValidationMiddleware( ...     max_message_length=10000, ...     allowed_roles=[\"user\", \"assistant\", \"system\"] ... )</p> Source code in <code>bruno_llm/base/middleware.py</code> <pre><code>class ValidationMiddleware(Middleware):\n    \"\"\"\n    Validate messages and parameters.\n\n    Args:\n        max_message_length: Max length for individual messages\n        allowed_roles: Allowed message roles\n        required_params: Required parameter names\n\n    Example:\n        &gt;&gt;&gt; middleware = ValidationMiddleware(\n        ...     max_message_length=10000,\n        ...     allowed_roles=[\"user\", \"assistant\", \"system\"]\n        ... )\n    \"\"\"\n\n    def __init__(\n        self,\n        max_message_length: Optional[int] = None,\n        allowed_roles: Optional[list[str]] = None,\n        required_params: Optional[list[str]] = None,\n    ):\n        \"\"\"\n        Initialize validation middleware.\n\n        Args:\n            max_message_length: Maximum message length\n            allowed_roles: Allowed message roles\n            required_params: Required parameters\n        \"\"\"\n        self.max_message_length = max_message_length\n        self.allowed_roles = allowed_roles\n        self.required_params = required_params or []\n\n    async def before_request(\n        self, messages: list[Message], **kwargs: Any\n    ) -&gt; tuple[list[Message], dict[str, Any]]:\n        \"\"\"Validate before request.\"\"\"\n        # Validate message lengths\n        if self.max_message_length:\n            for msg in messages:\n                if len(msg.content) &gt; self.max_message_length:\n                    raise ValueError(\n                        f\"Message content exceeds max length \"\n                        f\"({len(msg.content)} &gt; {self.max_message_length})\"\n                    )\n\n        # Validate roles\n        if self.allowed_roles:\n            for msg in messages:\n                if msg.role.value not in self.allowed_roles:\n                    raise ValueError(\n                        f\"Invalid message role: {msg.role.value}. Allowed: {self.allowed_roles}\"\n                    )\n\n        # Validate required parameters\n        for param in self.required_params:\n            if param not in kwargs:\n                raise ValueError(f\"Required parameter missing: {param}\")\n\n        return messages, kwargs\n\n    async def after_response(self, messages: list[Message], response: str, **kwargs: Any) -&gt; str:\n        \"\"\"No validation after response.\"\"\"\n        return response\n</code></pre>"},{"location":"api/base/#bruno_llm.base.ValidationMiddleware.__init__","title":"<code>__init__(max_message_length=None, allowed_roles=None, required_params=None)</code>","text":"<p>Initialize validation middleware.</p> <p>Parameters:</p> Name Type Description Default <code>max_message_length</code> <code>Optional[int]</code> <p>Maximum message length</p> <code>None</code> <code>allowed_roles</code> <code>Optional[list[str]]</code> <p>Allowed message roles</p> <code>None</code> <code>required_params</code> <code>Optional[list[str]]</code> <p>Required parameters</p> <code>None</code> Source code in <code>bruno_llm/base/middleware.py</code> <pre><code>def __init__(\n    self,\n    max_message_length: Optional[int] = None,\n    allowed_roles: Optional[list[str]] = None,\n    required_params: Optional[list[str]] = None,\n):\n    \"\"\"\n    Initialize validation middleware.\n\n    Args:\n        max_message_length: Maximum message length\n        allowed_roles: Allowed message roles\n        required_params: Required parameters\n    \"\"\"\n    self.max_message_length = max_message_length\n    self.allowed_roles = allowed_roles\n    self.required_params = required_params or []\n</code></pre>"},{"location":"api/base/#bruno_llm.base.ValidationMiddleware.before_request","title":"<code>before_request(messages, **kwargs)</code>  <code>async</code>","text":"<p>Validate before request.</p> Source code in <code>bruno_llm/base/middleware.py</code> <pre><code>async def before_request(\n    self, messages: list[Message], **kwargs: Any\n) -&gt; tuple[list[Message], dict[str, Any]]:\n    \"\"\"Validate before request.\"\"\"\n    # Validate message lengths\n    if self.max_message_length:\n        for msg in messages:\n            if len(msg.content) &gt; self.max_message_length:\n                raise ValueError(\n                    f\"Message content exceeds max length \"\n                    f\"({len(msg.content)} &gt; {self.max_message_length})\"\n                )\n\n    # Validate roles\n    if self.allowed_roles:\n        for msg in messages:\n            if msg.role.value not in self.allowed_roles:\n                raise ValueError(\n                    f\"Invalid message role: {msg.role.value}. Allowed: {self.allowed_roles}\"\n                )\n\n    # Validate required parameters\n    for param in self.required_params:\n        if param not in kwargs:\n            raise ValueError(f\"Required parameter missing: {param}\")\n\n    return messages, kwargs\n</code></pre>"},{"location":"api/base/#bruno_llm.base.ValidationMiddleware.after_response","title":"<code>after_response(messages, response, **kwargs)</code>  <code>async</code>","text":"<p>No validation after response.</p> Source code in <code>bruno_llm/base/middleware.py</code> <pre><code>async def after_response(self, messages: list[Message], response: str, **kwargs: Any) -&gt; str:\n    \"\"\"No validation after response.\"\"\"\n    return response\n</code></pre>"},{"location":"api/base/#bruno_llm.base.RateLimiter","title":"<code>RateLimiter</code>","text":"<p>Async rate limiter using token bucket algorithm.</p> <p>Controls the rate of API calls to prevent exceeding provider limits. Thread-safe and supports multiple concurrent requests.</p> Example <p>limiter = RateLimiter(requests_per_minute=60) async with limiter: ...     # Make API call ...     response = await api_call()</p> Source code in <code>bruno_llm/base/rate_limiter.py</code> <pre><code>class RateLimiter:\n    \"\"\"\n    Async rate limiter using token bucket algorithm.\n\n    Controls the rate of API calls to prevent exceeding provider limits.\n    Thread-safe and supports multiple concurrent requests.\n\n    Example:\n        &gt;&gt;&gt; limiter = RateLimiter(requests_per_minute=60)\n        &gt;&gt;&gt; async with limiter:\n        ...     # Make API call\n        ...     response = await api_call()\n    \"\"\"\n\n    def __init__(\n        self,\n        requests_per_minute: int = 60,\n        tokens_per_minute: Optional[int] = None,\n    ):\n        \"\"\"\n        Initialize rate limiter.\n\n        Args:\n            requests_per_minute: Maximum requests allowed per minute\n            tokens_per_minute: Maximum tokens allowed per minute (optional)\n        \"\"\"\n        self.requests_per_minute = requests_per_minute\n        self.tokens_per_minute = tokens_per_minute\n\n        # Calculate minimum interval between requests\n        self.min_interval = 60.0 / requests_per_minute if requests_per_minute &gt; 0 else 0\n\n        # Token bucket for requests\n        self._request_tokens = float(requests_per_minute)\n        self._max_request_tokens = float(requests_per_minute)\n        self._last_update = time.time()\n\n        # Token bucket for API tokens (if specified)\n        self._api_tokens = float(tokens_per_minute) if tokens_per_minute else None\n        self._max_api_tokens = float(tokens_per_minute) if tokens_per_minute else None\n\n        # Lock for thread safety\n        self._lock = asyncio.Lock()\n\n    async def _refill_tokens(self) -&gt; None:\n        \"\"\"Refill token buckets based on elapsed time.\"\"\"\n        now = time.time()\n        elapsed = now - self._last_update\n\n        if elapsed &lt;= 0:\n            return\n\n        # Refill request tokens\n        tokens_to_add = (elapsed * self.requests_per_minute) / 60.0\n        self._request_tokens = min(self._max_request_tokens, self._request_tokens + tokens_to_add)\n\n        # Refill API tokens if applicable\n        if self._api_tokens is not None and self.tokens_per_minute:\n            api_tokens_to_add = (elapsed * self.tokens_per_minute) / 60.0\n            self._api_tokens = min(self._max_api_tokens or 0, self._api_tokens + api_tokens_to_add)\n\n        self._last_update = now\n\n    async def acquire(self, api_tokens: int = 0) -&gt; None:\n        \"\"\"\n        Acquire permission to make a request.\n\n        Blocks until rate limit allows the request.\n\n        Args:\n            api_tokens: Number of API tokens the request will consume\n        \"\"\"\n        async with self._lock:\n            while True:\n                await self._refill_tokens()\n\n                # Check if we have enough request tokens\n                if self._request_tokens &lt; 1:\n                    # Calculate wait time\n                    wait_time = (1 - self._request_tokens) * (60.0 / self.requests_per_minute)\n                    await asyncio.sleep(wait_time)\n                    continue\n\n                # Check if we have enough API tokens (if applicable)\n                if self._api_tokens is not None and api_tokens &gt; 0:\n                    if self._api_tokens &lt; api_tokens:\n                        wait_time = (api_tokens - self._api_tokens) * (\n                            60.0 / (self.tokens_per_minute or 1)\n                        )\n                        await asyncio.sleep(wait_time)\n                        continue\n\n                # Consume tokens\n                self._request_tokens -= 1\n                if self._api_tokens is not None and api_tokens &gt; 0:\n                    self._api_tokens -= api_tokens\n\n                break\n\n    async def __aenter__(self) -&gt; \"RateLimiter\":\n        \"\"\"Context manager entry.\"\"\"\n        await self.acquire()\n        return self\n\n    async def __aexit__(self, exc_type, exc_val, exc_tb) -&gt; None:\n        \"\"\"Context manager exit.\"\"\"\n        pass\n\n    def get_stats(self) -&gt; dict:\n        \"\"\"\n        Get current rate limiter statistics.\n\n        Returns:\n            Dict with current token levels and limits\n        \"\"\"\n        return {\n            \"requests_per_minute\": self.requests_per_minute,\n            \"tokens_per_minute\": self.tokens_per_minute,\n            \"available_request_tokens\": self._request_tokens,\n            \"available_api_tokens\": self._api_tokens,\n            \"last_update\": self._last_update,\n        }\n</code></pre>"},{"location":"api/base/#bruno_llm.base.RateLimiter.__init__","title":"<code>__init__(requests_per_minute=60, tokens_per_minute=None)</code>","text":"<p>Initialize rate limiter.</p> <p>Parameters:</p> Name Type Description Default <code>requests_per_minute</code> <code>int</code> <p>Maximum requests allowed per minute</p> <code>60</code> <code>tokens_per_minute</code> <code>Optional[int]</code> <p>Maximum tokens allowed per minute (optional)</p> <code>None</code> Source code in <code>bruno_llm/base/rate_limiter.py</code> <pre><code>def __init__(\n    self,\n    requests_per_minute: int = 60,\n    tokens_per_minute: Optional[int] = None,\n):\n    \"\"\"\n    Initialize rate limiter.\n\n    Args:\n        requests_per_minute: Maximum requests allowed per minute\n        tokens_per_minute: Maximum tokens allowed per minute (optional)\n    \"\"\"\n    self.requests_per_minute = requests_per_minute\n    self.tokens_per_minute = tokens_per_minute\n\n    # Calculate minimum interval between requests\n    self.min_interval = 60.0 / requests_per_minute if requests_per_minute &gt; 0 else 0\n\n    # Token bucket for requests\n    self._request_tokens = float(requests_per_minute)\n    self._max_request_tokens = float(requests_per_minute)\n    self._last_update = time.time()\n\n    # Token bucket for API tokens (if specified)\n    self._api_tokens = float(tokens_per_minute) if tokens_per_minute else None\n    self._max_api_tokens = float(tokens_per_minute) if tokens_per_minute else None\n\n    # Lock for thread safety\n    self._lock = asyncio.Lock()\n</code></pre>"},{"location":"api/base/#bruno_llm.base.RateLimiter.acquire","title":"<code>acquire(api_tokens=0)</code>  <code>async</code>","text":"<p>Acquire permission to make a request.</p> <p>Blocks until rate limit allows the request.</p> <p>Parameters:</p> Name Type Description Default <code>api_tokens</code> <code>int</code> <p>Number of API tokens the request will consume</p> <code>0</code> Source code in <code>bruno_llm/base/rate_limiter.py</code> <pre><code>async def acquire(self, api_tokens: int = 0) -&gt; None:\n    \"\"\"\n    Acquire permission to make a request.\n\n    Blocks until rate limit allows the request.\n\n    Args:\n        api_tokens: Number of API tokens the request will consume\n    \"\"\"\n    async with self._lock:\n        while True:\n            await self._refill_tokens()\n\n            # Check if we have enough request tokens\n            if self._request_tokens &lt; 1:\n                # Calculate wait time\n                wait_time = (1 - self._request_tokens) * (60.0 / self.requests_per_minute)\n                await asyncio.sleep(wait_time)\n                continue\n\n            # Check if we have enough API tokens (if applicable)\n            if self._api_tokens is not None and api_tokens &gt; 0:\n                if self._api_tokens &lt; api_tokens:\n                    wait_time = (api_tokens - self._api_tokens) * (\n                        60.0 / (self.tokens_per_minute or 1)\n                    )\n                    await asyncio.sleep(wait_time)\n                    continue\n\n            # Consume tokens\n            self._request_tokens -= 1\n            if self._api_tokens is not None and api_tokens &gt; 0:\n                self._api_tokens -= api_tokens\n\n            break\n</code></pre>"},{"location":"api/base/#bruno_llm.base.RateLimiter.__aenter__","title":"<code>__aenter__()</code>  <code>async</code>","text":"<p>Context manager entry.</p> Source code in <code>bruno_llm/base/rate_limiter.py</code> <pre><code>async def __aenter__(self) -&gt; \"RateLimiter\":\n    \"\"\"Context manager entry.\"\"\"\n    await self.acquire()\n    return self\n</code></pre>"},{"location":"api/base/#bruno_llm.base.RateLimiter.__aexit__","title":"<code>__aexit__(exc_type, exc_val, exc_tb)</code>  <code>async</code>","text":"<p>Context manager exit.</p> Source code in <code>bruno_llm/base/rate_limiter.py</code> <pre><code>async def __aexit__(self, exc_type, exc_val, exc_tb) -&gt; None:\n    \"\"\"Context manager exit.\"\"\"\n    pass\n</code></pre>"},{"location":"api/base/#bruno_llm.base.RateLimiter.get_stats","title":"<code>get_stats()</code>","text":"<p>Get current rate limiter statistics.</p> <p>Returns:</p> Type Description <code>dict</code> <p>Dict with current token levels and limits</p> Source code in <code>bruno_llm/base/rate_limiter.py</code> <pre><code>def get_stats(self) -&gt; dict:\n    \"\"\"\n    Get current rate limiter statistics.\n\n    Returns:\n        Dict with current token levels and limits\n    \"\"\"\n    return {\n        \"requests_per_minute\": self.requests_per_minute,\n        \"tokens_per_minute\": self.tokens_per_minute,\n        \"available_request_tokens\": self._request_tokens,\n        \"available_api_tokens\": self._api_tokens,\n        \"last_update\": self._last_update,\n    }\n</code></pre>"},{"location":"api/base/#bruno_llm.base.RetryConfig","title":"<code>RetryConfig</code>","text":"<p>Configuration for retry behavior.</p> Example <p>config = RetryConfig( ...     max_retries=5, ...     initial_delay=1.0, ...     max_delay=60.0, ...     exponential_base=2.0 ... )</p> Source code in <code>bruno_llm/base/retry.py</code> <pre><code>class RetryConfig:\n    \"\"\"\n    Configuration for retry behavior.\n\n    Example:\n        &gt;&gt;&gt; config = RetryConfig(\n        ...     max_retries=5,\n        ...     initial_delay=1.0,\n        ...     max_delay=60.0,\n        ...     exponential_base=2.0\n        ... )\n    \"\"\"\n\n    def __init__(\n        self,\n        max_retries: int = 3,\n        initial_delay: float = 1.0,\n        max_delay: float = 60.0,\n        exponential_base: float = 2.0,\n        jitter: bool = True,\n        retry_on: Optional[tuple[type[Exception], ...]] = None,\n    ):\n        \"\"\"\n        Initialize retry configuration.\n\n        Args:\n            max_retries: Maximum number of retry attempts\n            initial_delay: Initial delay between retries in seconds\n            max_delay: Maximum delay between retries in seconds\n            exponential_base: Base for exponential backoff\n            jitter: Whether to add random jitter to delays\n            retry_on: Tuple of exception types to retry on (None = all)\n        \"\"\"\n        self.max_retries = max_retries\n        self.initial_delay = initial_delay\n        self.max_delay = max_delay\n        self.exponential_base = exponential_base\n        self.jitter = jitter\n        self.retry_on = retry_on or (Exception,)\n\n    def calculate_delay(self, attempt: int) -&gt; float:\n        \"\"\"\n        Calculate delay for given retry attempt.\n\n        Uses exponential backoff with optional jitter.\n\n        Args:\n            attempt: Retry attempt number (0-indexed)\n\n        Returns:\n            Delay in seconds\n        \"\"\"\n        # Exponential backoff\n        delay = min(self.initial_delay * (self.exponential_base**attempt), self.max_delay)\n\n        # Add jitter if enabled\n        if self.jitter:\n            jitter_amount = delay * 0.1  # 10% jitter\n            delay += random.uniform(-jitter_amount, jitter_amount)\n\n        return max(0, delay)\n\n    def should_retry(self, exception: Exception, attempt: int) -&gt; bool:\n        \"\"\"\n        Determine if retry should be attempted.\n\n        Args:\n            exception: Exception that was raised\n            attempt: Current attempt number (0-indexed)\n\n        Returns:\n            True if should retry, False otherwise\n        \"\"\"\n        # Check if we've exhausted retries\n        if attempt &gt;= self.max_retries:\n            return False\n\n        # Check if exception type is retryable\n        if not isinstance(exception, self.retry_on):\n            return False\n\n        # Special handling for rate limit errors\n        if isinstance(exception, RateLimitError):\n            return True\n\n        return True\n</code></pre>"},{"location":"api/base/#bruno_llm.base.RetryConfig.__init__","title":"<code>__init__(max_retries=3, initial_delay=1.0, max_delay=60.0, exponential_base=2.0, jitter=True, retry_on=None)</code>","text":"<p>Initialize retry configuration.</p> <p>Parameters:</p> Name Type Description Default <code>max_retries</code> <code>int</code> <p>Maximum number of retry attempts</p> <code>3</code> <code>initial_delay</code> <code>float</code> <p>Initial delay between retries in seconds</p> <code>1.0</code> <code>max_delay</code> <code>float</code> <p>Maximum delay between retries in seconds</p> <code>60.0</code> <code>exponential_base</code> <code>float</code> <p>Base for exponential backoff</p> <code>2.0</code> <code>jitter</code> <code>bool</code> <p>Whether to add random jitter to delays</p> <code>True</code> <code>retry_on</code> <code>Optional[tuple[type[Exception], ...]]</code> <p>Tuple of exception types to retry on (None = all)</p> <code>None</code> Source code in <code>bruno_llm/base/retry.py</code> <pre><code>def __init__(\n    self,\n    max_retries: int = 3,\n    initial_delay: float = 1.0,\n    max_delay: float = 60.0,\n    exponential_base: float = 2.0,\n    jitter: bool = True,\n    retry_on: Optional[tuple[type[Exception], ...]] = None,\n):\n    \"\"\"\n    Initialize retry configuration.\n\n    Args:\n        max_retries: Maximum number of retry attempts\n        initial_delay: Initial delay between retries in seconds\n        max_delay: Maximum delay between retries in seconds\n        exponential_base: Base for exponential backoff\n        jitter: Whether to add random jitter to delays\n        retry_on: Tuple of exception types to retry on (None = all)\n    \"\"\"\n    self.max_retries = max_retries\n    self.initial_delay = initial_delay\n    self.max_delay = max_delay\n    self.exponential_base = exponential_base\n    self.jitter = jitter\n    self.retry_on = retry_on or (Exception,)\n</code></pre>"},{"location":"api/base/#bruno_llm.base.RetryConfig.calculate_delay","title":"<code>calculate_delay(attempt)</code>","text":"<p>Calculate delay for given retry attempt.</p> <p>Uses exponential backoff with optional jitter.</p> <p>Parameters:</p> Name Type Description Default <code>attempt</code> <code>int</code> <p>Retry attempt number (0-indexed)</p> required <p>Returns:</p> Type Description <code>float</code> <p>Delay in seconds</p> Source code in <code>bruno_llm/base/retry.py</code> <pre><code>def calculate_delay(self, attempt: int) -&gt; float:\n    \"\"\"\n    Calculate delay for given retry attempt.\n\n    Uses exponential backoff with optional jitter.\n\n    Args:\n        attempt: Retry attempt number (0-indexed)\n\n    Returns:\n        Delay in seconds\n    \"\"\"\n    # Exponential backoff\n    delay = min(self.initial_delay * (self.exponential_base**attempt), self.max_delay)\n\n    # Add jitter if enabled\n    if self.jitter:\n        jitter_amount = delay * 0.1  # 10% jitter\n        delay += random.uniform(-jitter_amount, jitter_amount)\n\n    return max(0, delay)\n</code></pre>"},{"location":"api/base/#bruno_llm.base.RetryConfig.should_retry","title":"<code>should_retry(exception, attempt)</code>","text":"<p>Determine if retry should be attempted.</p> <p>Parameters:</p> Name Type Description Default <code>exception</code> <code>Exception</code> <p>Exception that was raised</p> required <code>attempt</code> <code>int</code> <p>Current attempt number (0-indexed)</p> required <p>Returns:</p> Type Description <code>bool</code> <p>True if should retry, False otherwise</p> Source code in <code>bruno_llm/base/retry.py</code> <pre><code>def should_retry(self, exception: Exception, attempt: int) -&gt; bool:\n    \"\"\"\n    Determine if retry should be attempted.\n\n    Args:\n        exception: Exception that was raised\n        attempt: Current attempt number (0-indexed)\n\n    Returns:\n        True if should retry, False otherwise\n    \"\"\"\n    # Check if we've exhausted retries\n    if attempt &gt;= self.max_retries:\n        return False\n\n    # Check if exception type is retryable\n    if not isinstance(exception, self.retry_on):\n        return False\n\n    # Special handling for rate limit errors\n    if isinstance(exception, RateLimitError):\n        return True\n\n    return True\n</code></pre>"},{"location":"api/base/#bruno_llm.base.RetryDecorator","title":"<code>RetryDecorator</code>","text":"<p>Decorator for adding retry logic to async functions.</p> Example <p>@RetryDecorator(max_retries=5) ... async def api_call(): ...     return await external_api()</p> Source code in <code>bruno_llm/base/retry.py</code> <pre><code>class RetryDecorator:\n    \"\"\"\n    Decorator for adding retry logic to async functions.\n\n    Example:\n        &gt;&gt;&gt; @RetryDecorator(max_retries=5)\n        ... async def api_call():\n        ...     return await external_api()\n    \"\"\"\n\n    def __init__(\n        self,\n        max_retries: int = 3,\n        initial_delay: float = 1.0,\n        max_delay: float = 60.0,\n        exponential_base: float = 2.0,\n        jitter: bool = True,\n    ):\n        \"\"\"\n        Initialize retry decorator.\n\n        Args:\n            max_retries: Maximum number of retry attempts\n            initial_delay: Initial delay between retries\n            max_delay: Maximum delay between retries\n            exponential_base: Base for exponential backoff\n            jitter: Whether to add jitter\n        \"\"\"\n        self.config = RetryConfig(\n            max_retries=max_retries,\n            initial_delay=initial_delay,\n            max_delay=max_delay,\n            exponential_base=exponential_base,\n            jitter=jitter,\n        )\n\n    def __call__(self, func: Callable[..., T]) -&gt; Callable[..., T]:\n        \"\"\"\n        Wrap function with retry logic.\n\n        Args:\n            func: Function to wrap\n\n        Returns:\n            Wrapped function\n        \"\"\"\n\n        async def wrapper(*args: Any, **kwargs: Any) -&gt; T:\n            return await retry_async(func, *args, config=self.config, **kwargs)\n\n        return wrapper\n</code></pre>"},{"location":"api/base/#bruno_llm.base.RetryDecorator.__init__","title":"<code>__init__(max_retries=3, initial_delay=1.0, max_delay=60.0, exponential_base=2.0, jitter=True)</code>","text":"<p>Initialize retry decorator.</p> <p>Parameters:</p> Name Type Description Default <code>max_retries</code> <code>int</code> <p>Maximum number of retry attempts</p> <code>3</code> <code>initial_delay</code> <code>float</code> <p>Initial delay between retries</p> <code>1.0</code> <code>max_delay</code> <code>float</code> <p>Maximum delay between retries</p> <code>60.0</code> <code>exponential_base</code> <code>float</code> <p>Base for exponential backoff</p> <code>2.0</code> <code>jitter</code> <code>bool</code> <p>Whether to add jitter</p> <code>True</code> Source code in <code>bruno_llm/base/retry.py</code> <pre><code>def __init__(\n    self,\n    max_retries: int = 3,\n    initial_delay: float = 1.0,\n    max_delay: float = 60.0,\n    exponential_base: float = 2.0,\n    jitter: bool = True,\n):\n    \"\"\"\n    Initialize retry decorator.\n\n    Args:\n        max_retries: Maximum number of retry attempts\n        initial_delay: Initial delay between retries\n        max_delay: Maximum delay between retries\n        exponential_base: Base for exponential backoff\n        jitter: Whether to add jitter\n    \"\"\"\n    self.config = RetryConfig(\n        max_retries=max_retries,\n        initial_delay=initial_delay,\n        max_delay=max_delay,\n        exponential_base=exponential_base,\n        jitter=jitter,\n    )\n</code></pre>"},{"location":"api/base/#bruno_llm.base.RetryDecorator.__call__","title":"<code>__call__(func)</code>","text":"<p>Wrap function with retry logic.</p> <p>Parameters:</p> Name Type Description Default <code>func</code> <code>Callable[..., T]</code> <p>Function to wrap</p> required <p>Returns:</p> Type Description <code>Callable[..., T]</code> <p>Wrapped function</p> Source code in <code>bruno_llm/base/retry.py</code> <pre><code>def __call__(self, func: Callable[..., T]) -&gt; Callable[..., T]:\n    \"\"\"\n    Wrap function with retry logic.\n\n    Args:\n        func: Function to wrap\n\n    Returns:\n        Wrapped function\n    \"\"\"\n\n    async def wrapper(*args: Any, **kwargs: Any) -&gt; T:\n        return await retry_async(func, *args, config=self.config, **kwargs)\n\n    return wrapper\n</code></pre>"},{"location":"api/base/#bruno_llm.base.StreamAggregator","title":"<code>StreamAggregator</code>","text":"<p>Aggregate streaming chunks with various strategies.</p> <p>Provides different aggregation strategies for streaming responses: - Word-by-word: Buffer until complete words - Sentence-by-sentence: Buffer until sentence boundaries - Fixed-size: Buffer until fixed character count - Time-based: Buffer for fixed time intervals</p> <p>Parameters:</p> Name Type Description Default <code>strategy</code> <code>str</code> <p>Aggregation strategy ('word', 'sentence', 'fixed', 'time')</p> <code>'word'</code> <code>size</code> <code>int</code> <p>Size parameter (chars for 'fixed', seconds for 'time')</p> <code>10</code> Example <p>aggregator = StreamAggregator(strategy='word') async for chunk in aggregator.aggregate(stream): ...     print(chunk)  # Prints complete words</p> Source code in <code>bruno_llm/base/streaming.py</code> <pre><code>class StreamAggregator:\n    \"\"\"\n    Aggregate streaming chunks with various strategies.\n\n    Provides different aggregation strategies for streaming responses:\n    - Word-by-word: Buffer until complete words\n    - Sentence-by-sentence: Buffer until sentence boundaries\n    - Fixed-size: Buffer until fixed character count\n    - Time-based: Buffer for fixed time intervals\n\n    Args:\n        strategy: Aggregation strategy ('word', 'sentence', 'fixed', 'time')\n        size: Size parameter (chars for 'fixed', seconds for 'time')\n\n    Example:\n        &gt;&gt;&gt; aggregator = StreamAggregator(strategy='word')\n        &gt;&gt;&gt; async for chunk in aggregator.aggregate(stream):\n        ...     print(chunk)  # Prints complete words\n    \"\"\"\n\n    def __init__(\n        self,\n        strategy: str = \"word\",\n        size: int = 10,\n    ):\n        \"\"\"\n        Initialize stream aggregator.\n\n        Args:\n            strategy: Aggregation strategy\n            size: Size parameter for aggregation\n        \"\"\"\n        self.strategy = strategy\n        self.size = size\n        self._buffer = \"\"\n\n    async def aggregate(self, stream: AsyncIterator[str]) -&gt; AsyncIterator[str]:\n        \"\"\"\n        Aggregate stream chunks according to strategy.\n\n        Args:\n            stream: Input stream to aggregate\n\n        Yields:\n            Aggregated chunks\n        \"\"\"\n        if self.strategy == \"word\":\n            async for chunk in self._aggregate_words(stream):\n                yield chunk\n        elif self.strategy == \"sentence\":\n            async for chunk in self._aggregate_sentences(stream):\n                yield chunk\n        elif self.strategy == \"fixed\":\n            async for chunk in self._aggregate_fixed(stream):\n                yield chunk\n        elif self.strategy == \"time\":\n            async for chunk in self._aggregate_time(stream):\n                yield chunk\n        else:\n            # No aggregation, pass through\n            async for chunk in stream:\n                yield chunk\n\n    async def _aggregate_words(self, stream: AsyncIterator[str]) -&gt; AsyncIterator[str]:\n        \"\"\"Aggregate chunks into complete words.\"\"\"\n        async for chunk in stream:\n            self._buffer += chunk\n\n            # Split on whitespace while keeping incomplete words\n            parts = self._buffer.split()\n\n            if len(parts) &gt; 1:\n                # Yield all complete words\n                for word in parts[:-1]:\n                    yield word + \" \"\n\n                # Keep the last part as buffer (might be incomplete)\n                self._buffer = parts[-1]\n            elif self._buffer.endswith((\" \", \"\\n\", \"\\t\")):\n                # Buffer ends with whitespace, yield it\n                yield self._buffer\n                self._buffer = \"\"\n\n        # Flush remaining buffer\n        if self._buffer:\n            yield self._buffer\n            self._buffer = \"\"\n\n    async def _aggregate_sentences(self, stream: AsyncIterator[str]) -&gt; AsyncIterator[str]:\n        \"\"\"Aggregate chunks into complete sentences.\"\"\"\n        sentence_endings = (\".\", \"!\", \"?\", \"\\n\")\n\n        async for chunk in stream:\n            self._buffer += chunk\n\n            # Check if buffer contains sentence ending\n            while any(self._buffer.endswith(end) for end in sentence_endings):\n                # Find last sentence ending\n                last_idx = -1\n                for ending in sentence_endings:\n                    idx = self._buffer.rfind(ending)\n                    if idx &gt; last_idx:\n                        last_idx = idx\n\n                if last_idx &gt;= 0:\n                    # Yield complete sentence(s)\n                    sentence = self._buffer[: last_idx + 1]\n                    yield sentence\n                    self._buffer = self._buffer[last_idx + 1 :]\n                else:\n                    break\n\n        # Flush remaining buffer\n        if self._buffer:\n            yield self._buffer\n            self._buffer = \"\"\n\n    async def _aggregate_fixed(self, stream: AsyncIterator[str]) -&gt; AsyncIterator[str]:\n        \"\"\"Aggregate chunks into fixed-size chunks.\"\"\"\n        async for chunk in stream:\n            self._buffer += chunk\n\n            # Yield chunks of fixed size\n            while len(self._buffer) &gt;= self.size:\n                yield self._buffer[: self.size]\n                self._buffer = self._buffer[self.size :]\n\n        # Flush remaining buffer\n        if self._buffer:\n            yield self._buffer\n            self._buffer = \"\"\n\n    async def _aggregate_time(self, stream: AsyncIterator[str]) -&gt; AsyncIterator[str]:\n        \"\"\"Aggregate chunks based on time intervals.\"\"\"\n        import time\n\n        last_yield = time.time()\n\n        async for chunk in stream:\n            self._buffer += chunk\n\n            current_time = time.time()\n            if current_time - last_yield &gt;= self.size:\n                if self._buffer:\n                    yield self._buffer\n                    self._buffer = \"\"\n                    last_yield = current_time\n\n        # Flush remaining buffer\n        if self._buffer:\n            yield self._buffer\n            self._buffer = \"\"\n</code></pre>"},{"location":"api/base/#bruno_llm.base.StreamAggregator.__init__","title":"<code>__init__(strategy='word', size=10)</code>","text":"<p>Initialize stream aggregator.</p> <p>Parameters:</p> Name Type Description Default <code>strategy</code> <code>str</code> <p>Aggregation strategy</p> <code>'word'</code> <code>size</code> <code>int</code> <p>Size parameter for aggregation</p> <code>10</code> Source code in <code>bruno_llm/base/streaming.py</code> <pre><code>def __init__(\n    self,\n    strategy: str = \"word\",\n    size: int = 10,\n):\n    \"\"\"\n    Initialize stream aggregator.\n\n    Args:\n        strategy: Aggregation strategy\n        size: Size parameter for aggregation\n    \"\"\"\n    self.strategy = strategy\n    self.size = size\n    self._buffer = \"\"\n</code></pre>"},{"location":"api/base/#bruno_llm.base.StreamAggregator.aggregate","title":"<code>aggregate(stream)</code>  <code>async</code>","text":"<p>Aggregate stream chunks according to strategy.</p> <p>Parameters:</p> Name Type Description Default <code>stream</code> <code>AsyncIterator[str]</code> <p>Input stream to aggregate</p> required <p>Yields:</p> Type Description <code>AsyncIterator[str]</code> <p>Aggregated chunks</p> Source code in <code>bruno_llm/base/streaming.py</code> <pre><code>async def aggregate(self, stream: AsyncIterator[str]) -&gt; AsyncIterator[str]:\n    \"\"\"\n    Aggregate stream chunks according to strategy.\n\n    Args:\n        stream: Input stream to aggregate\n\n    Yields:\n        Aggregated chunks\n    \"\"\"\n    if self.strategy == \"word\":\n        async for chunk in self._aggregate_words(stream):\n            yield chunk\n    elif self.strategy == \"sentence\":\n        async for chunk in self._aggregate_sentences(stream):\n            yield chunk\n    elif self.strategy == \"fixed\":\n        async for chunk in self._aggregate_fixed(stream):\n            yield chunk\n    elif self.strategy == \"time\":\n        async for chunk in self._aggregate_time(stream):\n            yield chunk\n    else:\n        # No aggregation, pass through\n        async for chunk in stream:\n            yield chunk\n</code></pre>"},{"location":"api/base/#bruno_llm.base.StreamBuffer","title":"<code>StreamBuffer</code>  <code>dataclass</code>","text":"<p>Buffer for managing streaming chunks.</p> <p>Provides buffering, batching, and aggregation of stream chunks.</p> <p>Attributes:</p> Name Type Description <code>buffer</code> <code>deque[str]</code> <p>Internal deque for storing chunks</p> <code>max_size</code> <code>int</code> <p>Maximum buffer size in characters</p> <code>batch_size</code> <code>int</code> <p>Number of chunks to batch before yielding</p> <code>stats</code> <code>StreamStats</code> <p>Stream statistics</p> Source code in <code>bruno_llm/base/streaming.py</code> <pre><code>@dataclass\nclass StreamBuffer:\n    \"\"\"\n    Buffer for managing streaming chunks.\n\n    Provides buffering, batching, and aggregation of stream chunks.\n\n    Attributes:\n        buffer: Internal deque for storing chunks\n        max_size: Maximum buffer size in characters\n        batch_size: Number of chunks to batch before yielding\n        stats: Stream statistics\n    \"\"\"\n\n    buffer: deque[str] = field(default_factory=deque)\n    max_size: int = 10000\n    batch_size: int = 1\n    stats: StreamStats = field(default_factory=StreamStats)\n\n    def add(self, chunk: str) -&gt; None:\n        \"\"\"\n        Add a chunk to the buffer.\n\n        Args:\n            chunk: Text chunk to buffer\n\n        Raises:\n            StreamError: If buffer is full\n        \"\"\"\n        current_size = sum(len(c) for c in self.buffer)\n\n        if current_size + len(chunk) &gt; self.max_size:\n            raise StreamError(\n                f\"Stream buffer full ({current_size} chars). \"\n                f\"Consider increasing max_size or consuming buffer faster.\"\n            )\n\n        self.buffer.append(chunk)\n        self.stats.chunks_received += 1\n        self.stats.total_chars += len(chunk)\n\n    def get_batch(self) -&gt; Optional[str]:\n        \"\"\"\n        Get a batch of chunks.\n\n        Returns:\n            Concatenated batch or None if not enough chunks\n        \"\"\"\n        if len(self.buffer) &lt; self.batch_size:\n            return None\n\n        chunks = []\n        for _ in range(min(self.batch_size, len(self.buffer))):\n            chunks.append(self.buffer.popleft())\n\n        return \"\".join(chunks)\n\n    def flush(self) -&gt; str:\n        \"\"\"\n        Flush all remaining chunks.\n\n        Returns:\n            All remaining chunks concatenated\n        \"\"\"\n        chunks = list(self.buffer)\n        self.buffer.clear()\n        return \"\".join(chunks)\n\n    def is_empty(self) -&gt; bool:\n        \"\"\"Check if buffer is empty.\"\"\"\n        return len(self.buffer) == 0\n\n    def clear(self) -&gt; None:\n        \"\"\"Clear buffer and reset stats.\"\"\"\n        self.buffer.clear()\n        self.stats = StreamStats()\n</code></pre>"},{"location":"api/base/#bruno_llm.base.StreamBuffer.add","title":"<code>add(chunk)</code>","text":"<p>Add a chunk to the buffer.</p> <p>Parameters:</p> Name Type Description Default <code>chunk</code> <code>str</code> <p>Text chunk to buffer</p> required <p>Raises:</p> Type Description <code>StreamError</code> <p>If buffer is full</p> Source code in <code>bruno_llm/base/streaming.py</code> <pre><code>def add(self, chunk: str) -&gt; None:\n    \"\"\"\n    Add a chunk to the buffer.\n\n    Args:\n        chunk: Text chunk to buffer\n\n    Raises:\n        StreamError: If buffer is full\n    \"\"\"\n    current_size = sum(len(c) for c in self.buffer)\n\n    if current_size + len(chunk) &gt; self.max_size:\n        raise StreamError(\n            f\"Stream buffer full ({current_size} chars). \"\n            f\"Consider increasing max_size or consuming buffer faster.\"\n        )\n\n    self.buffer.append(chunk)\n    self.stats.chunks_received += 1\n    self.stats.total_chars += len(chunk)\n</code></pre>"},{"location":"api/base/#bruno_llm.base.StreamBuffer.get_batch","title":"<code>get_batch()</code>","text":"<p>Get a batch of chunks.</p> <p>Returns:</p> Type Description <code>Optional[str]</code> <p>Concatenated batch or None if not enough chunks</p> Source code in <code>bruno_llm/base/streaming.py</code> <pre><code>def get_batch(self) -&gt; Optional[str]:\n    \"\"\"\n    Get a batch of chunks.\n\n    Returns:\n        Concatenated batch or None if not enough chunks\n    \"\"\"\n    if len(self.buffer) &lt; self.batch_size:\n        return None\n\n    chunks = []\n    for _ in range(min(self.batch_size, len(self.buffer))):\n        chunks.append(self.buffer.popleft())\n\n    return \"\".join(chunks)\n</code></pre>"},{"location":"api/base/#bruno_llm.base.StreamBuffer.flush","title":"<code>flush()</code>","text":"<p>Flush all remaining chunks.</p> <p>Returns:</p> Type Description <code>str</code> <p>All remaining chunks concatenated</p> Source code in <code>bruno_llm/base/streaming.py</code> <pre><code>def flush(self) -&gt; str:\n    \"\"\"\n    Flush all remaining chunks.\n\n    Returns:\n        All remaining chunks concatenated\n    \"\"\"\n    chunks = list(self.buffer)\n    self.buffer.clear()\n    return \"\".join(chunks)\n</code></pre>"},{"location":"api/base/#bruno_llm.base.StreamBuffer.is_empty","title":"<code>is_empty()</code>","text":"<p>Check if buffer is empty.</p> Source code in <code>bruno_llm/base/streaming.py</code> <pre><code>def is_empty(self) -&gt; bool:\n    \"\"\"Check if buffer is empty.\"\"\"\n    return len(self.buffer) == 0\n</code></pre>"},{"location":"api/base/#bruno_llm.base.StreamBuffer.clear","title":"<code>clear()</code>","text":"<p>Clear buffer and reset stats.</p> Source code in <code>bruno_llm/base/streaming.py</code> <pre><code>def clear(self) -&gt; None:\n    \"\"\"Clear buffer and reset stats.\"\"\"\n    self.buffer.clear()\n    self.stats = StreamStats()\n</code></pre>"},{"location":"api/base/#bruno_llm.base.StreamProcessor","title":"<code>StreamProcessor</code>","text":"<p>Process streaming responses with callbacks and error handling.</p> <p>Provides a framework for processing streams with: - Progress callbacks - Error recovery - Automatic retry on connection loss - Statistics tracking</p> <p>Parameters:</p> Name Type Description Default <code>on_chunk</code> <code>Optional[Callable[[str], None]]</code> <p>Callback for each chunk (chunk: str) -&gt; None</p> <code>None</code> <code>on_error</code> <code>Optional[Callable[[Exception], None]]</code> <p>Callback for errors (error: Exception) -&gt; None</p> <code>None</code> <code>on_complete</code> <code>Optional[Callable[[StreamStats], None]]</code> <p>Callback when stream completes (stats: StreamStats) -&gt; None</p> <code>None</code> <code>max_retries</code> <code>int</code> <p>Maximum number of retries on error</p> <code>3</code> Example <p>processor = StreamProcessor( ...     on_chunk=lambda chunk: print(chunk, end=\"\"), ...     on_error=lambda e: print(f\"Error: {e}\"), ...     on_complete=lambda stats: print(f\"\\nReceived {stats.chunks_received} chunks\") ... ) await processor.process(stream)</p> Source code in <code>bruno_llm/base/streaming.py</code> <pre><code>class StreamProcessor:\n    \"\"\"\n    Process streaming responses with callbacks and error handling.\n\n    Provides a framework for processing streams with:\n    - Progress callbacks\n    - Error recovery\n    - Automatic retry on connection loss\n    - Statistics tracking\n\n    Args:\n        on_chunk: Callback for each chunk (chunk: str) -&gt; None\n        on_error: Callback for errors (error: Exception) -&gt; None\n        on_complete: Callback when stream completes (stats: StreamStats) -&gt; None\n        max_retries: Maximum number of retries on error\n\n    Example:\n        &gt;&gt;&gt; processor = StreamProcessor(\n        ...     on_chunk=lambda chunk: print(chunk, end=\"\"),\n        ...     on_error=lambda e: print(f\"Error: {e}\"),\n        ...     on_complete=lambda stats: print(f\"\\\\nReceived {stats.chunks_received} chunks\")\n        ... )\n        &gt;&gt;&gt; await processor.process(stream)\n    \"\"\"\n\n    def __init__(\n        self,\n        on_chunk: Optional[Callable[[str], None]] = None,\n        on_error: Optional[Callable[[Exception], None]] = None,\n        on_complete: Optional[Callable[[StreamStats], None]] = None,\n        max_retries: int = 3,\n    ):\n        \"\"\"\n        Initialize stream processor.\n\n        Args:\n            on_chunk: Callback for each chunk\n            on_error: Callback for errors\n            on_complete: Callback when stream completes\n            max_retries: Maximum number of retries on error\n        \"\"\"\n        self.on_chunk = on_chunk\n        self.on_error = on_error\n        self.on_complete = on_complete\n        self.max_retries = max_retries\n        self.stats = StreamStats()\n\n    async def process(\n        self,\n        stream: AsyncIterator[str],\n        retry_on_error: bool = True,\n    ) -&gt; list[str]:\n        \"\"\"\n        Process a stream with callbacks and error handling.\n\n        Args:\n            stream: Input stream to process\n            retry_on_error: Whether to retry on errors\n\n        Returns:\n            List of all chunks received\n\n        Raises:\n            StreamError: If max retries exceeded\n        \"\"\"\n        import time\n\n        chunks = []\n        start_time = time.time()\n        retries = 0\n\n        try:\n            async for chunk in stream:\n                chunks.append(chunk)\n                self.stats.chunks_received += 1\n                self.stats.total_chars += len(chunk)\n\n                if self.on_chunk:\n                    self.on_chunk(chunk)\n\n            # Calculate duration after stream completes\n            self.stats.duration = time.time() - start_time\n\n            if self.on_complete:\n                self.on_complete(self.stats)\n\n            return chunks\n\n        except Exception as e:\n            self.stats.errors += 1\n            self.stats.duration = time.time() - start_time\n\n            if self.on_error:\n                self.on_error(e)\n\n            if retry_on_error and retries &lt; self.max_retries:\n                retries += 1\n                await asyncio.sleep(2**retries)  # Exponential backoff\n                return await self.process(stream, retry_on_error)\n\n            raise StreamError(f\"Stream processing failed: {e}\") from e\n</code></pre>"},{"location":"api/base/#bruno_llm.base.StreamProcessor.__init__","title":"<code>__init__(on_chunk=None, on_error=None, on_complete=None, max_retries=3)</code>","text":"<p>Initialize stream processor.</p> <p>Parameters:</p> Name Type Description Default <code>on_chunk</code> <code>Optional[Callable[[str], None]]</code> <p>Callback for each chunk</p> <code>None</code> <code>on_error</code> <code>Optional[Callable[[Exception], None]]</code> <p>Callback for errors</p> <code>None</code> <code>on_complete</code> <code>Optional[Callable[[StreamStats], None]]</code> <p>Callback when stream completes</p> <code>None</code> <code>max_retries</code> <code>int</code> <p>Maximum number of retries on error</p> <code>3</code> Source code in <code>bruno_llm/base/streaming.py</code> <pre><code>def __init__(\n    self,\n    on_chunk: Optional[Callable[[str], None]] = None,\n    on_error: Optional[Callable[[Exception], None]] = None,\n    on_complete: Optional[Callable[[StreamStats], None]] = None,\n    max_retries: int = 3,\n):\n    \"\"\"\n    Initialize stream processor.\n\n    Args:\n        on_chunk: Callback for each chunk\n        on_error: Callback for errors\n        on_complete: Callback when stream completes\n        max_retries: Maximum number of retries on error\n    \"\"\"\n    self.on_chunk = on_chunk\n    self.on_error = on_error\n    self.on_complete = on_complete\n    self.max_retries = max_retries\n    self.stats = StreamStats()\n</code></pre>"},{"location":"api/base/#bruno_llm.base.StreamProcessor.process","title":"<code>process(stream, retry_on_error=True)</code>  <code>async</code>","text":"<p>Process a stream with callbacks and error handling.</p> <p>Parameters:</p> Name Type Description Default <code>stream</code> <code>AsyncIterator[str]</code> <p>Input stream to process</p> required <code>retry_on_error</code> <code>bool</code> <p>Whether to retry on errors</p> <code>True</code> <p>Returns:</p> Type Description <code>list[str]</code> <p>List of all chunks received</p> <p>Raises:</p> Type Description <code>StreamError</code> <p>If max retries exceeded</p> Source code in <code>bruno_llm/base/streaming.py</code> <pre><code>async def process(\n    self,\n    stream: AsyncIterator[str],\n    retry_on_error: bool = True,\n) -&gt; list[str]:\n    \"\"\"\n    Process a stream with callbacks and error handling.\n\n    Args:\n        stream: Input stream to process\n        retry_on_error: Whether to retry on errors\n\n    Returns:\n        List of all chunks received\n\n    Raises:\n        StreamError: If max retries exceeded\n    \"\"\"\n    import time\n\n    chunks = []\n    start_time = time.time()\n    retries = 0\n\n    try:\n        async for chunk in stream:\n            chunks.append(chunk)\n            self.stats.chunks_received += 1\n            self.stats.total_chars += len(chunk)\n\n            if self.on_chunk:\n                self.on_chunk(chunk)\n\n        # Calculate duration after stream completes\n        self.stats.duration = time.time() - start_time\n\n        if self.on_complete:\n            self.on_complete(self.stats)\n\n        return chunks\n\n    except Exception as e:\n        self.stats.errors += 1\n        self.stats.duration = time.time() - start_time\n\n        if self.on_error:\n            self.on_error(e)\n\n        if retry_on_error and retries &lt; self.max_retries:\n            retries += 1\n            await asyncio.sleep(2**retries)  # Exponential backoff\n            return await self.process(stream, retry_on_error)\n\n        raise StreamError(f\"Stream processing failed: {e}\") from e\n</code></pre>"},{"location":"api/base/#bruno_llm.base.StreamStats","title":"<code>StreamStats</code>  <code>dataclass</code>","text":"<p>Statistics for a streaming session.</p> <p>Attributes:</p> Name Type Description <code>chunks_received</code> <code>int</code> <p>Number of chunks received</p> <code>total_chars</code> <code>int</code> <p>Total characters streamed</p> <code>total_tokens</code> <code>int</code> <p>Estimated token count</p> <code>duration</code> <code>float</code> <p>Duration of stream in seconds</p> <code>errors</code> <code>int</code> <p>Number of errors encountered</p> Source code in <code>bruno_llm/base/streaming.py</code> <pre><code>@dataclass\nclass StreamStats:\n    \"\"\"\n    Statistics for a streaming session.\n\n    Attributes:\n        chunks_received: Number of chunks received\n        total_chars: Total characters streamed\n        total_tokens: Estimated token count\n        duration: Duration of stream in seconds\n        errors: Number of errors encountered\n    \"\"\"\n\n    chunks_received: int = 0\n    total_chars: int = 0\n    total_tokens: int = 0\n    duration: float = 0.0\n    errors: int = 0\n</code></pre>"},{"location":"api/base/#bruno_llm.base.SimpleTokenCounter","title":"<code>SimpleTokenCounter</code>","text":"<p>               Bases: <code>TokenCounter</code></p> <p>Simple token counter using word splitting.</p> <p>This is a fallback implementation that approximates token count by counting words. Not as accurate as provider-specific tokenizers but works universally.</p> Example <p>counter = SimpleTokenCounter() tokens = counter.count_tokens(\"Hello world!\") print(tokens)  # Approximately 2-3</p> Source code in <code>bruno_llm/base/token_counter.py</code> <pre><code>class SimpleTokenCounter(TokenCounter):\n    \"\"\"\n    Simple token counter using word splitting.\n\n    This is a fallback implementation that approximates token count\n    by counting words. Not as accurate as provider-specific tokenizers\n    but works universally.\n\n    Example:\n        &gt;&gt;&gt; counter = SimpleTokenCounter()\n        &gt;&gt;&gt; tokens = counter.count_tokens(\"Hello world!\")\n        &gt;&gt;&gt; print(tokens)  # Approximately 2-3\n    \"\"\"\n\n    def __init__(self, chars_per_token: float = 4.0):\n        \"\"\"\n        Initialize simple token counter.\n\n        Args:\n            chars_per_token: Average characters per token (default: 4)\n        \"\"\"\n        self.chars_per_token = chars_per_token\n\n    def count_tokens(self, text: str) -&gt; int:\n        \"\"\"\n        Count tokens using character-based estimation.\n\n        Uses the common approximation that 1 token \u2248 4 characters\n        in English text.\n\n        Args:\n            text: Text to count tokens for\n\n        Returns:\n            Estimated token count\n        \"\"\"\n        if not text:\n            return 0\n        return max(1, int(len(text) / self.chars_per_token))\n</code></pre>"},{"location":"api/base/#bruno_llm.base.SimpleTokenCounter.__init__","title":"<code>__init__(chars_per_token=4.0)</code>","text":"<p>Initialize simple token counter.</p> <p>Parameters:</p> Name Type Description Default <code>chars_per_token</code> <code>float</code> <p>Average characters per token (default: 4)</p> <code>4.0</code> Source code in <code>bruno_llm/base/token_counter.py</code> <pre><code>def __init__(self, chars_per_token: float = 4.0):\n    \"\"\"\n    Initialize simple token counter.\n\n    Args:\n        chars_per_token: Average characters per token (default: 4)\n    \"\"\"\n    self.chars_per_token = chars_per_token\n</code></pre>"},{"location":"api/base/#bruno_llm.base.SimpleTokenCounter.count_tokens","title":"<code>count_tokens(text)</code>","text":"<p>Count tokens using character-based estimation.</p> <p>Uses the common approximation that 1 token \u2248 4 characters in English text.</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>str</code> <p>Text to count tokens for</p> required <p>Returns:</p> Type Description <code>int</code> <p>Estimated token count</p> Source code in <code>bruno_llm/base/token_counter.py</code> <pre><code>def count_tokens(self, text: str) -&gt; int:\n    \"\"\"\n    Count tokens using character-based estimation.\n\n    Uses the common approximation that 1 token \u2248 4 characters\n    in English text.\n\n    Args:\n        text: Text to count tokens for\n\n    Returns:\n        Estimated token count\n    \"\"\"\n    if not text:\n        return 0\n    return max(1, int(len(text) / self.chars_per_token))\n</code></pre>"},{"location":"api/base/#bruno_llm.base.TikTokenCounter","title":"<code>TikTokenCounter</code>","text":"<p>               Bases: <code>TokenCounter</code></p> <p>Token counter using OpenAI's tiktoken library.</p> <p>Provides accurate token counting for OpenAI models. Falls back to SimpleTokenCounter if tiktoken is not available.</p> Example <p>counter = TikTokenCounter(model=\"gpt-4\") tokens = counter.count_tokens(\"Hello world!\") print(tokens)</p> Source code in <code>bruno_llm/base/token_counter.py</code> <pre><code>class TikTokenCounter(TokenCounter):\n    \"\"\"\n    Token counter using OpenAI's tiktoken library.\n\n    Provides accurate token counting for OpenAI models.\n    Falls back to SimpleTokenCounter if tiktoken is not available.\n\n    Example:\n        &gt;&gt;&gt; counter = TikTokenCounter(model=\"gpt-4\")\n        &gt;&gt;&gt; tokens = counter.count_tokens(\"Hello world!\")\n        &gt;&gt;&gt; print(tokens)\n    \"\"\"\n\n    def __init__(self, model: str = \"gpt-4\"):\n        \"\"\"\n        Initialize tiktoken-based counter.\n\n        Args:\n            model: Model name for tiktoken encoding\n        \"\"\"\n        self.model = model\n        self._encoding = None\n        self._fallback = SimpleTokenCounter()\n\n        try:\n            import tiktoken\n\n            self._encoding = tiktoken.encoding_for_model(model)\n        except ImportError:\n            # tiktoken not available, will use fallback\n            pass\n        except Exception:\n            # Model not found or other error, use fallback\n            pass\n\n    def count_tokens(self, text: str) -&gt; int:\n        \"\"\"\n        Count tokens using tiktoken or fallback.\n\n        Args:\n            text: Text to count tokens for\n\n        Returns:\n            Accurate token count (if tiktoken available) or estimate\n        \"\"\"\n        if not text:\n            return 0\n\n        if self._encoding is not None:\n            try:\n                return len(self._encoding.encode(text))\n            except Exception:\n                pass\n\n        # Fallback to simple counting\n        return self._fallback.count_tokens(text)\n</code></pre>"},{"location":"api/base/#bruno_llm.base.TikTokenCounter.__init__","title":"<code>__init__(model='gpt-4')</code>","text":"<p>Initialize tiktoken-based counter.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>str</code> <p>Model name for tiktoken encoding</p> <code>'gpt-4'</code> Source code in <code>bruno_llm/base/token_counter.py</code> <pre><code>def __init__(self, model: str = \"gpt-4\"):\n    \"\"\"\n    Initialize tiktoken-based counter.\n\n    Args:\n        model: Model name for tiktoken encoding\n    \"\"\"\n    self.model = model\n    self._encoding = None\n    self._fallback = SimpleTokenCounter()\n\n    try:\n        import tiktoken\n\n        self._encoding = tiktoken.encoding_for_model(model)\n    except ImportError:\n        # tiktoken not available, will use fallback\n        pass\n    except Exception:\n        # Model not found or other error, use fallback\n        pass\n</code></pre>"},{"location":"api/base/#bruno_llm.base.TikTokenCounter.count_tokens","title":"<code>count_tokens(text)</code>","text":"<p>Count tokens using tiktoken or fallback.</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>str</code> <p>Text to count tokens for</p> required <p>Returns:</p> Type Description <code>int</code> <p>Accurate token count (if tiktoken available) or estimate</p> Source code in <code>bruno_llm/base/token_counter.py</code> <pre><code>def count_tokens(self, text: str) -&gt; int:\n    \"\"\"\n    Count tokens using tiktoken or fallback.\n\n    Args:\n        text: Text to count tokens for\n\n    Returns:\n        Accurate token count (if tiktoken available) or estimate\n    \"\"\"\n    if not text:\n        return 0\n\n    if self._encoding is not None:\n        try:\n            return len(self._encoding.encode(text))\n        except Exception:\n            pass\n\n    # Fallback to simple counting\n    return self._fallback.count_tokens(text)\n</code></pre>"},{"location":"api/base/#bruno_llm.base.TokenCounter","title":"<code>TokenCounter</code>","text":"<p>               Bases: <code>ABC</code></p> <p>Abstract base class for token counting.</p> <p>Different providers may have different tokenization methods. Subclasses should implement provider-specific counting logic.</p> Source code in <code>bruno_llm/base/token_counter.py</code> <pre><code>class TokenCounter(ABC):\n    \"\"\"\n    Abstract base class for token counting.\n\n    Different providers may have different tokenization methods.\n    Subclasses should implement provider-specific counting logic.\n    \"\"\"\n\n    @abstractmethod\n    def count_tokens(self, text: str) -&gt; int:\n        \"\"\"\n        Count tokens in text.\n\n        Args:\n            text: Text to count tokens for\n\n        Returns:\n            Number of tokens\n        \"\"\"\n        pass\n\n    def count_message_tokens(self, message: Message) -&gt; int:\n        \"\"\"\n        Count tokens in a message.\n\n        Args:\n            message: Message to count tokens for\n\n        Returns:\n            Number of tokens\n        \"\"\"\n        return self.count_tokens(message.content)\n\n    def count_messages_tokens(self, messages: list[Message]) -&gt; int:\n        \"\"\"\n        Count tokens in multiple messages.\n\n        Args:\n            messages: List of messages\n\n        Returns:\n            Total number of tokens\n        \"\"\"\n        total = 0\n        for message in messages:\n            total += self.count_message_tokens(message)\n            # Add overhead for message formatting (role, etc.)\n            total += 4  # Approximate overhead per message\n        return total\n</code></pre>"},{"location":"api/base/#bruno_llm.base.TokenCounter.count_tokens","title":"<code>count_tokens(text)</code>  <code>abstractmethod</code>","text":"<p>Count tokens in text.</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>str</code> <p>Text to count tokens for</p> required <p>Returns:</p> Type Description <code>int</code> <p>Number of tokens</p> Source code in <code>bruno_llm/base/token_counter.py</code> <pre><code>@abstractmethod\ndef count_tokens(self, text: str) -&gt; int:\n    \"\"\"\n    Count tokens in text.\n\n    Args:\n        text: Text to count tokens for\n\n    Returns:\n        Number of tokens\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/base/#bruno_llm.base.TokenCounter.count_message_tokens","title":"<code>count_message_tokens(message)</code>","text":"<p>Count tokens in a message.</p> <p>Parameters:</p> Name Type Description Default <code>message</code> <code>Message</code> <p>Message to count tokens for</p> required <p>Returns:</p> Type Description <code>int</code> <p>Number of tokens</p> Source code in <code>bruno_llm/base/token_counter.py</code> <pre><code>def count_message_tokens(self, message: Message) -&gt; int:\n    \"\"\"\n    Count tokens in a message.\n\n    Args:\n        message: Message to count tokens for\n\n    Returns:\n        Number of tokens\n    \"\"\"\n    return self.count_tokens(message.content)\n</code></pre>"},{"location":"api/base/#bruno_llm.base.TokenCounter.count_messages_tokens","title":"<code>count_messages_tokens(messages)</code>","text":"<p>Count tokens in multiple messages.</p> <p>Parameters:</p> Name Type Description Default <code>messages</code> <code>list[Message]</code> <p>List of messages</p> required <p>Returns:</p> Type Description <code>int</code> <p>Total number of tokens</p> Source code in <code>bruno_llm/base/token_counter.py</code> <pre><code>def count_messages_tokens(self, messages: list[Message]) -&gt; int:\n    \"\"\"\n    Count tokens in multiple messages.\n\n    Args:\n        messages: List of messages\n\n    Returns:\n        Total number of tokens\n    \"\"\"\n    total = 0\n    for message in messages:\n        total += self.count_message_tokens(message)\n        # Add overhead for message formatting (role, etc.)\n        total += 4  # Approximate overhead per message\n    return total\n</code></pre>"},{"location":"api/base/#bruno_llm.base.retry_async","title":"<code>retry_async(func, *args, config=None, **kwargs)</code>  <code>async</code>","text":"<p>Execute async function with retry logic.</p> <p>Parameters:</p> Name Type Description Default <code>func</code> <code>Callable[..., T]</code> <p>Async function to execute</p> required <code>*args</code> <code>Any</code> <p>Positional arguments for func</p> <code>()</code> <code>config</code> <code>Optional[RetryConfig]</code> <p>Retry configuration (uses defaults if None)</p> <code>None</code> <code>**kwargs</code> <code>Any</code> <p>Keyword arguments for func</p> <code>{}</code> <p>Returns:</p> Type Description <code>T</code> <p>Result from func</p> <p>Raises:</p> Type Description <code>Exception</code> <p>Last exception if all retries fail</p> Example <p>async def api_call(): ...     # May fail transiently ...     return await external_api() result = await retry_async(api_call, config=RetryConfig(max_retries=5))</p> Source code in <code>bruno_llm/base/retry.py</code> <pre><code>async def retry_async(\n    func: Callable[..., T],\n    *args: Any,\n    config: Optional[RetryConfig] = None,\n    **kwargs: Any,\n) -&gt; T:\n    \"\"\"\n    Execute async function with retry logic.\n\n    Args:\n        func: Async function to execute\n        *args: Positional arguments for func\n        config: Retry configuration (uses defaults if None)\n        **kwargs: Keyword arguments for func\n\n    Returns:\n        Result from func\n\n    Raises:\n        Exception: Last exception if all retries fail\n\n    Example:\n        &gt;&gt;&gt; async def api_call():\n        ...     # May fail transiently\n        ...     return await external_api()\n        &gt;&gt;&gt; result = await retry_async(api_call, config=RetryConfig(max_retries=5))\n    \"\"\"\n    if config is None:\n        config = RetryConfig()\n\n    last_exception = None\n\n    for attempt in range(config.max_retries + 1):\n        try:\n            return await func(*args, **kwargs)\n        except Exception as e:\n            last_exception = e\n\n            # Check if we should retry\n            if not config.should_retry(e, attempt):\n                raise\n\n            # Calculate and wait\n            if attempt &lt; config.max_retries:\n                delay = config.calculate_delay(attempt)\n\n                # Special handling for rate limit with retry_after\n                if isinstance(e, RateLimitError) and e.retry_after:\n                    delay = max(delay, e.retry_after)\n\n                await asyncio.sleep(delay)\n\n    # Should not reach here, but for safety\n    if last_exception:\n        raise last_exception\n    raise LLMError(\"Retry loop exited unexpectedly\")\n</code></pre>"},{"location":"api/base/#bruno_llm.base.stream_with_timeout","title":"<code>stream_with_timeout(stream, timeout=30.0)</code>  <code>async</code>","text":"<p>Wrap a stream with timeout protection.</p> <p>Raises TimeoutError if no chunk is received within timeout.</p> <p>Parameters:</p> Name Type Description Default <code>stream</code> <code>AsyncIterator[str]</code> <p>Input stream to wrap</p> required <code>timeout</code> <code>float</code> <p>Timeout in seconds for each chunk</p> <code>30.0</code> <p>Yields:</p> Type Description <code>AsyncIterator[str]</code> <p>Stream chunks</p> <p>Raises:</p> Type Description <code>TimeoutError</code> <p>If timeout is exceeded</p> Example <p>async for chunk in stream_with_timeout(stream, timeout=10.0): ...     print(chunk)</p> Source code in <code>bruno_llm/base/streaming.py</code> <pre><code>async def stream_with_timeout(\n    stream: AsyncIterator[str],\n    timeout: float = 30.0,\n) -&gt; AsyncIterator[str]:\n    \"\"\"\n    Wrap a stream with timeout protection.\n\n    Raises TimeoutError if no chunk is received within timeout.\n\n    Args:\n        stream: Input stream to wrap\n        timeout: Timeout in seconds for each chunk\n\n    Yields:\n        Stream chunks\n\n    Raises:\n        TimeoutError: If timeout is exceeded\n\n    Example:\n        &gt;&gt;&gt; async for chunk in stream_with_timeout(stream, timeout=10.0):\n        ...     print(chunk)\n    \"\"\"\n    async for chunk in stream:\n        try:\n            yield await asyncio.wait_for(_async_identity(chunk), timeout=timeout)\n        except asyncio.TimeoutError as e:\n            raise TimeoutError(f\"No chunk received within {timeout} seconds\") from e\n</code></pre>"},{"location":"api/base/#bruno_llm.base.create_token_counter","title":"<code>create_token_counter(provider='simple', model=None)</code>","text":"<p>Factory function to create appropriate token counter.</p> <p>Parameters:</p> Name Type Description Default <code>provider</code> <code>str</code> <p>Provider name (\"simple\", \"openai\", \"tiktoken\")</p> <code>'simple'</code> <code>model</code> <code>Optional[str]</code> <p>Optional model name for provider-specific counting</p> <code>None</code> <p>Returns:</p> Type Description <code>TokenCounter</code> <p>TokenCounter instance</p> Example <p>counter = create_token_counter(\"openai\", model=\"gpt-4\") tokens = counter.count_tokens(\"Hello!\")</p> Source code in <code>bruno_llm/base/token_counter.py</code> <pre><code>def create_token_counter(\n    provider: str = \"simple\",\n    model: Optional[str] = None,\n) -&gt; TokenCounter:\n    \"\"\"\n    Factory function to create appropriate token counter.\n\n    Args:\n        provider: Provider name (\"simple\", \"openai\", \"tiktoken\")\n        model: Optional model name for provider-specific counting\n\n    Returns:\n        TokenCounter instance\n\n    Example:\n        &gt;&gt;&gt; counter = create_token_counter(\"openai\", model=\"gpt-4\")\n        &gt;&gt;&gt; tokens = counter.count_tokens(\"Hello!\")\n    \"\"\"\n    if provider in (\"openai\", \"tiktoken\"):\n        if model:\n            return TikTokenCounter(model=model)\n        return TikTokenCounter()\n\n    return SimpleTokenCounter()\n</code></pre>"},{"location":"api/exceptions/","title":"Exceptions","text":""},{"location":"api/exceptions/#bruno_llm.exceptions","title":"<code>bruno_llm.exceptions</code>","text":"<p>Exception hierarchy for bruno-llm.</p> <p>Defines custom exceptions for LLM provider errors, enabling consistent error handling across all providers.</p>"},{"location":"api/exceptions/#bruno_llm.exceptions.LLMError","title":"<code>LLMError</code>","text":"<p>               Bases: <code>Exception</code></p> <p>Base exception for all LLM-related errors.</p> <p>All custom exceptions in bruno-llm inherit from this class.</p> <p>Parameters:</p> Name Type Description Default <code>message</code> <code>str</code> <p>Error message</p> required <code>provider</code> <code>Optional[str]</code> <p>Name of the provider that raised the error</p> <code>None</code> <code>original_error</code> <code>Optional[Exception]</code> <p>Original exception if available</p> <code>None</code> Source code in <code>bruno_llm/exceptions.py</code> <pre><code>class LLMError(Exception):\n    \"\"\"\n    Base exception for all LLM-related errors.\n\n    All custom exceptions in bruno-llm inherit from this class.\n\n    Args:\n        message: Error message\n        provider: Name of the provider that raised the error\n        original_error: Original exception if available\n    \"\"\"\n\n    def __init__(\n        self,\n        message: str,\n        provider: Optional[str] = None,\n        original_error: Optional[Exception] = None,\n    ):\n        self.message = message\n        self.provider = provider\n        self.original_error = original_error\n        super().__init__(self.format_message())\n\n    def format_message(self) -&gt; str:\n        \"\"\"Format the error message with provider context.\"\"\"\n        parts = []\n        if self.provider:\n            parts.append(f\"[{self.provider}]\")\n        parts.append(self.message)\n        if self.original_error:\n            parts.append(\n                f\"(caused by: {type(self.original_error).__name__}: {self.original_error})\"\n            )\n        return \" \".join(parts)\n</code></pre>"},{"location":"api/exceptions/#bruno_llm.exceptions.LLMError.format_message","title":"<code>format_message()</code>","text":"<p>Format the error message with provider context.</p> Source code in <code>bruno_llm/exceptions.py</code> <pre><code>def format_message(self) -&gt; str:\n    \"\"\"Format the error message with provider context.\"\"\"\n    parts = []\n    if self.provider:\n        parts.append(f\"[{self.provider}]\")\n    parts.append(self.message)\n    if self.original_error:\n        parts.append(\n            f\"(caused by: {type(self.original_error).__name__}: {self.original_error})\"\n        )\n    return \" \".join(parts)\n</code></pre>"},{"location":"api/exceptions/#bruno_llm.exceptions.AuthenticationError","title":"<code>AuthenticationError</code>","text":"<p>               Bases: <code>LLMError</code></p> <p>Raised when authentication with LLM provider fails.</p> <p>Common causes: - Invalid API key - Expired API key - Missing API key - Invalid organization ID</p> Example <p>raise AuthenticationError(\"Invalid API key\", provider=\"openai\")</p> Source code in <code>bruno_llm/exceptions.py</code> <pre><code>class AuthenticationError(LLMError):\n    \"\"\"\n    Raised when authentication with LLM provider fails.\n\n    Common causes:\n    - Invalid API key\n    - Expired API key\n    - Missing API key\n    - Invalid organization ID\n\n    Example:\n        &gt;&gt;&gt; raise AuthenticationError(\"Invalid API key\", provider=\"openai\")\n    \"\"\"\n\n    pass\n</code></pre>"},{"location":"api/exceptions/#bruno_llm.exceptions.RateLimitError","title":"<code>RateLimitError</code>","text":"<p>               Bases: <code>LLMError</code></p> <p>Raised when API rate limits are exceeded.</p> <p>Attributes:</p> Name Type Description <code>retry_after</code> <p>Seconds to wait before retrying (if provided)</p> Example <p>raise RateLimitError(\"Rate limit exceeded\", provider=\"openai\", retry_after=60)</p> Source code in <code>bruno_llm/exceptions.py</code> <pre><code>class RateLimitError(LLMError):\n    \"\"\"\n    Raised when API rate limits are exceeded.\n\n    Attributes:\n        retry_after: Seconds to wait before retrying (if provided)\n\n    Example:\n        &gt;&gt;&gt; raise RateLimitError(\"Rate limit exceeded\", provider=\"openai\", retry_after=60)\n    \"\"\"\n\n    def __init__(\n        self,\n        message: str,\n        provider: Optional[str] = None,\n        original_error: Optional[Exception] = None,\n        retry_after: Optional[int] = None,\n    ):\n        super().__init__(message, provider, original_error)\n        self.retry_after = retry_after\n</code></pre>"},{"location":"api/exceptions/#bruno_llm.exceptions.ModelNotFoundError","title":"<code>ModelNotFoundError</code>","text":"<p>               Bases: <code>LLMError</code></p> <p>Raised when requested model is not found or not available.</p> <p>Common causes: - Model name misspelled - Model not available in region - Model access not granted - Model has been deprecated</p> Example <p>raise ModelNotFoundError(\"Model 'gpt-5' not found\", provider=\"openai\")</p> Source code in <code>bruno_llm/exceptions.py</code> <pre><code>class ModelNotFoundError(LLMError):\n    \"\"\"\n    Raised when requested model is not found or not available.\n\n    Common causes:\n    - Model name misspelled\n    - Model not available in region\n    - Model access not granted\n    - Model has been deprecated\n\n    Example:\n        &gt;&gt;&gt; raise ModelNotFoundError(\"Model 'gpt-5' not found\", provider=\"openai\")\n    \"\"\"\n\n    pass\n</code></pre>"},{"location":"api/exceptions/#bruno_llm.exceptions.ContextLengthExceededError","title":"<code>ContextLengthExceededError</code>","text":"<p>               Bases: <code>LLMError</code></p> <p>Raised when input exceeds model's context length.</p> <p>Attributes:</p> Name Type Description <code>max_tokens</code> <p>Maximum tokens allowed</p> <code>actual_tokens</code> <p>Actual token count in request</p> Example <p>raise ContextLengthExceededError( ...     \"Context length exceeded\", ...     provider=\"openai\", ...     max_tokens=8192, ...     actual_tokens=10000 ... )</p> Source code in <code>bruno_llm/exceptions.py</code> <pre><code>class ContextLengthExceededError(LLMError):\n    \"\"\"\n    Raised when input exceeds model's context length.\n\n    Attributes:\n        max_tokens: Maximum tokens allowed\n        actual_tokens: Actual token count in request\n\n    Example:\n        &gt;&gt;&gt; raise ContextLengthExceededError(\n        ...     \"Context length exceeded\",\n        ...     provider=\"openai\",\n        ...     max_tokens=8192,\n        ...     actual_tokens=10000\n        ... )\n    \"\"\"\n\n    def __init__(\n        self,\n        message: str,\n        provider: Optional[str] = None,\n        original_error: Optional[Exception] = None,\n        max_tokens: Optional[int] = None,\n        actual_tokens: Optional[int] = None,\n    ):\n        super().__init__(message, provider, original_error)\n        self.max_tokens = max_tokens\n        self.actual_tokens = actual_tokens\n</code></pre>"},{"location":"api/exceptions/#bruno_llm.exceptions.StreamError","title":"<code>StreamError</code>","text":"<p>               Bases: <code>LLMError</code></p> <p>Raised when streaming response encounters an error.</p> <p>Common causes: - Network connection lost - Server-side error during streaming - Invalid chunk format - Stream interrupted</p> Example <p>raise StreamError(\"Stream connection lost\", provider=\"openai\")</p> Source code in <code>bruno_llm/exceptions.py</code> <pre><code>class StreamError(LLMError):\n    \"\"\"\n    Raised when streaming response encounters an error.\n\n    Common causes:\n    - Network connection lost\n    - Server-side error during streaming\n    - Invalid chunk format\n    - Stream interrupted\n\n    Example:\n        &gt;&gt;&gt; raise StreamError(\"Stream connection lost\", provider=\"openai\")\n    \"\"\"\n\n    pass\n</code></pre>"},{"location":"api/exceptions/#bruno_llm.exceptions.ConfigurationError","title":"<code>ConfigurationError</code>","text":"<p>               Bases: <code>LLMError</code></p> <p>Raised when provider configuration is invalid.</p> <p>Common causes: - Missing required configuration - Invalid configuration values - Conflicting configuration options</p> Example <p>raise ConfigurationError(\"Missing base_url\", provider=\"ollama\")</p> Source code in <code>bruno_llm/exceptions.py</code> <pre><code>class ConfigurationError(LLMError):\n    \"\"\"\n    Raised when provider configuration is invalid.\n\n    Common causes:\n    - Missing required configuration\n    - Invalid configuration values\n    - Conflicting configuration options\n\n    Example:\n        &gt;&gt;&gt; raise ConfigurationError(\"Missing base_url\", provider=\"ollama\")\n    \"\"\"\n\n    pass\n</code></pre>"},{"location":"api/exceptions/#bruno_llm.exceptions.TimeoutError","title":"<code>TimeoutError</code>","text":"<p>               Bases: <code>LLMError</code></p> <p>Raised when request times out.</p> <p>Attributes:</p> Name Type Description <code>timeout</code> <p>Timeout value in seconds</p> Example <p>raise TimeoutError(\"Request timed out after 30s\", provider=\"openai\", timeout=30)</p> Source code in <code>bruno_llm/exceptions.py</code> <pre><code>class TimeoutError(LLMError):\n    \"\"\"\n    Raised when request times out.\n\n    Attributes:\n        timeout: Timeout value in seconds\n\n    Example:\n        &gt;&gt;&gt; raise TimeoutError(\"Request timed out after 30s\", provider=\"openai\", timeout=30)\n    \"\"\"\n\n    def __init__(\n        self,\n        message: str,\n        provider: Optional[str] = None,\n        original_error: Optional[Exception] = None,\n        timeout: Optional[float] = None,\n    ):\n        super().__init__(message, provider, original_error)\n        self.timeout = timeout\n</code></pre>"},{"location":"api/exceptions/#bruno_llm.exceptions.InvalidResponseError","title":"<code>InvalidResponseError</code>","text":"<p>               Bases: <code>LLMError</code></p> <p>Raised when provider returns invalid or unexpected response.</p> <p>Common causes: - Malformed JSON response - Missing required fields - Unexpected response structure</p> Example <p>raise InvalidResponseError(\"Missing 'content' field\", provider=\"openai\")</p> Source code in <code>bruno_llm/exceptions.py</code> <pre><code>class InvalidResponseError(LLMError):\n    \"\"\"\n    Raised when provider returns invalid or unexpected response.\n\n    Common causes:\n    - Malformed JSON response\n    - Missing required fields\n    - Unexpected response structure\n\n    Example:\n        &gt;&gt;&gt; raise InvalidResponseError(\"Missing 'content' field\", provider=\"openai\")\n    \"\"\"\n\n    pass\n</code></pre>"},{"location":"api/factory/","title":"Factory","text":""},{"location":"api/factory/#bruno_llm.factory.LLMFactory","title":"<code>bruno_llm.factory.LLMFactory</code>","text":"<p>Factory for creating LLM provider instances.</p> <p>Provides multiple ways to instantiate providers: - Direct creation with configuration - Environment-based configuration - Fallback chain for resilience</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; # Direct creation\n&gt;&gt;&gt; llm = LLMFactory.create(\n...     provider=\"ollama\",\n...     config={\"model\": \"llama2\"}\n... )\n</code></pre> <pre><code>&gt;&gt;&gt; # From environment variables\n&gt;&gt;&gt; llm = LLMFactory.create_from_env(provider=\"openai\")\n</code></pre> <pre><code>&gt;&gt;&gt; # With fallback\n&gt;&gt;&gt; llm = LLMFactory.create_with_fallback(\n...     providers=[\"openai\", \"ollama\"],\n...     configs=[openai_config, ollama_config]\n... )\n</code></pre> Source code in <code>bruno_llm/factory.py</code> <pre><code>class LLMFactory:\n    \"\"\"\n    Factory for creating LLM provider instances.\n\n    Provides multiple ways to instantiate providers:\n    - Direct creation with configuration\n    - Environment-based configuration\n    - Fallback chain for resilience\n\n    Examples:\n        &gt;&gt;&gt; # Direct creation\n        &gt;&gt;&gt; llm = LLMFactory.create(\n        ...     provider=\"ollama\",\n        ...     config={\"model\": \"llama2\"}\n        ... )\n\n        &gt;&gt;&gt; # From environment variables\n        &gt;&gt;&gt; llm = LLMFactory.create_from_env(provider=\"openai\")\n\n        &gt;&gt;&gt; # With fallback\n        &gt;&gt;&gt; llm = LLMFactory.create_with_fallback(\n        ...     providers=[\"openai\", \"ollama\"],\n        ...     configs=[openai_config, ollama_config]\n        ... )\n    \"\"\"\n\n    _providers: dict[str, Callable[..., LLMInterface]] = {}\n\n    @classmethod\n    def register(cls, name: str, provider_class: Callable[..., LLMInterface]) -&gt; None:\n        \"\"\"\n        Register a provider class with the factory.\n\n        Args:\n            name: Provider name (e.g., \"ollama\", \"openai\")\n            provider_class: Provider class or factory function\n        \"\"\"\n        cls._providers[name.lower()] = provider_class\n\n    @classmethod\n    def create(\n        cls, provider: str, config: Optional[dict[str, Any]] = None, **kwargs: Any\n    ) -&gt; LLMInterface:\n        \"\"\"\n        Create a provider instance.\n\n        Args:\n            provider: Provider name (\"ollama\", \"openai\", etc.)\n            config: Configuration dictionary (optional)\n            **kwargs: Additional arguments passed to provider\n\n        Returns:\n            Configured LLMInterface instance\n\n        Raises:\n            ConfigurationError: If provider not found or config invalid\n\n        Examples:\n            &gt;&gt;&gt; llm = LLMFactory.create(\"ollama\", {\"model\": \"llama2\"})\n            &gt;&gt;&gt; llm = LLMFactory.create(\"openai\", api_key=\"sk-...\", model=\"gpt-4\")\n        \"\"\"\n        provider_lower = provider.lower()\n\n        if provider_lower not in cls._providers:\n            available = \", \".join(cls._providers.keys())\n            raise ConfigurationError(\n                f\"Provider '{provider}' not found. Available providers: {available}\"\n            )\n\n        provider_class = cls._providers[provider_lower]\n\n        # Merge config dict and kwargs\n        final_config = config.copy() if config else {}\n        final_config.update(kwargs)\n\n        try:\n            return provider_class(**final_config)\n        except TypeError as e:\n            raise ConfigurationError(f\"Invalid configuration for provider '{provider}': {e}\") from e\n\n    @classmethod\n    def create_from_env(cls, provider: str, prefix: Optional[str] = None) -&gt; LLMInterface:\n        \"\"\"\n        Create provider from environment variables.\n\n        Environment variables are read using the pattern:\n        {PREFIX}_{PROVIDER}_{SETTING}\n\n        Args:\n            provider: Provider name\n            prefix: Environment variable prefix (default: \"BRUNO_LLM\")\n\n        Returns:\n            Configured LLMInterface instance\n\n        Raises:\n            ConfigurationError: If required env vars missing\n\n        Examples:\n            &gt;&gt;&gt; # With BRUNO_LLM_OPENAI_API_KEY=sk-...\n            &gt;&gt;&gt; # and BRUNO_LLM_OPENAI_MODEL=gpt-4\n            &gt;&gt;&gt; llm = LLMFactory.create_from_env(\"openai\")\n\n            &gt;&gt;&gt; # Custom prefix\n            &gt;&gt;&gt; # MY_APP_OLLAMA_MODEL=llama2\n            &gt;&gt;&gt; llm = LLMFactory.create_from_env(\"ollama\", prefix=\"MY_APP\")\n        \"\"\"\n        prefix = prefix or \"BRUNO_LLM\"\n        provider_upper = provider.upper()\n        env_prefix = f\"{prefix}_{provider_upper}_\"\n\n        # Collect all matching environment variables\n        config: dict[str, Any] = {}\n        for key, value in os.environ.items():\n            if key.startswith(env_prefix):\n                # Remove prefix and convert to lowercase\n                setting_name = key[len(env_prefix) :].lower()\n                config[setting_name] = value\n\n        if not config:\n            raise ConfigurationError(\n                f\"No environment variables found for provider '{provider}'. \"\n                f\"Expected variables starting with {env_prefix}\"\n            )\n\n        return cls.create(provider, config)\n\n    @classmethod\n    async def create_with_fallback(\n        cls, providers: list[str], configs: Optional[list[dict[str, Any]]] = None\n    ) -&gt; LLMInterface:\n        \"\"\"\n        Create provider with fallback chain.\n\n        Tries each provider in order until one connects successfully.\n\n        Args:\n            providers: List of provider names in priority order\n            configs: Optional list of configurations (must match providers length)\n\n        Returns:\n            First successfully connected LLMInterface instance\n\n        Raises:\n            LLMError: If all providers fail to connect\n\n        Examples:\n            &gt;&gt;&gt; llm = await LLMFactory.create_with_fallback(\n            ...     providers=[\"openai\", \"ollama\"],\n            ...     configs=[\n            ...         {\"api_key\": \"sk-...\", \"model\": \"gpt-4\"},\n            ...         {\"model\": \"llama2\"}\n            ...     ]\n            ... )\n        \"\"\"\n        if not providers:\n            raise ConfigurationError(\"No providers specified for fallback\")\n\n        if configs and len(configs) != len(providers):\n            raise ConfigurationError(\n                f\"configs length ({len(configs)}) must match providers length ({len(providers)})\"\n            )\n\n        errors = []\n\n        for i, provider_name in enumerate(providers):\n            try:\n                config = configs[i] if configs else {}\n                provider = cls.create(provider_name, config)\n\n                # Test connection\n                if await provider.check_connection():\n                    return provider\n                else:\n                    errors.append(f\"{provider_name}: Connection check failed\")\n\n            except Exception as e:\n                errors.append(f\"{provider_name}: {e}\")\n                continue\n\n        # All providers failed\n        error_details = \"; \".join(errors)\n        raise LLMError(\n            f\"All providers failed to connect. Tried: {', '.join(providers)}. \"\n            f\"Errors: {error_details}\"\n        )\n\n    @classmethod\n    def list_providers(cls) -&gt; list[str]:\n        \"\"\"\n        List all registered providers.\n\n        Returns:\n            List of provider names\n        \"\"\"\n        return sorted(cls._providers.keys())\n\n    @classmethod\n    def is_registered(cls, provider: str) -&gt; bool:\n        \"\"\"\n        Check if a provider is registered.\n\n        Args:\n            provider: Provider name\n\n        Returns:\n            True if provider is registered\n        \"\"\"\n        return provider.lower() in cls._providers\n</code></pre>"},{"location":"api/factory/#bruno_llm.factory.LLMFactory.register","title":"<code>register(name, provider_class)</code>  <code>classmethod</code>","text":"<p>Register a provider class with the factory.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Provider name (e.g., \"ollama\", \"openai\")</p> required <code>provider_class</code> <code>Callable[..., LLMInterface]</code> <p>Provider class or factory function</p> required Source code in <code>bruno_llm/factory.py</code> <pre><code>@classmethod\ndef register(cls, name: str, provider_class: Callable[..., LLMInterface]) -&gt; None:\n    \"\"\"\n    Register a provider class with the factory.\n\n    Args:\n        name: Provider name (e.g., \"ollama\", \"openai\")\n        provider_class: Provider class or factory function\n    \"\"\"\n    cls._providers[name.lower()] = provider_class\n</code></pre>"},{"location":"api/factory/#bruno_llm.factory.LLMFactory.create","title":"<code>create(provider, config=None, **kwargs)</code>  <code>classmethod</code>","text":"<p>Create a provider instance.</p> <p>Parameters:</p> Name Type Description Default <code>provider</code> <code>str</code> <p>Provider name (\"ollama\", \"openai\", etc.)</p> required <code>config</code> <code>Optional[dict[str, Any]]</code> <p>Configuration dictionary (optional)</p> <code>None</code> <code>**kwargs</code> <code>Any</code> <p>Additional arguments passed to provider</p> <code>{}</code> <p>Returns:</p> Type Description <code>LLMInterface</code> <p>Configured LLMInterface instance</p> <p>Raises:</p> Type Description <code>ConfigurationError</code> <p>If provider not found or config invalid</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; llm = LLMFactory.create(\"ollama\", {\"model\": \"llama2\"})\n&gt;&gt;&gt; llm = LLMFactory.create(\"openai\", api_key=\"sk-...\", model=\"gpt-4\")\n</code></pre> Source code in <code>bruno_llm/factory.py</code> <pre><code>@classmethod\ndef create(\n    cls, provider: str, config: Optional[dict[str, Any]] = None, **kwargs: Any\n) -&gt; LLMInterface:\n    \"\"\"\n    Create a provider instance.\n\n    Args:\n        provider: Provider name (\"ollama\", \"openai\", etc.)\n        config: Configuration dictionary (optional)\n        **kwargs: Additional arguments passed to provider\n\n    Returns:\n        Configured LLMInterface instance\n\n    Raises:\n        ConfigurationError: If provider not found or config invalid\n\n    Examples:\n        &gt;&gt;&gt; llm = LLMFactory.create(\"ollama\", {\"model\": \"llama2\"})\n        &gt;&gt;&gt; llm = LLMFactory.create(\"openai\", api_key=\"sk-...\", model=\"gpt-4\")\n    \"\"\"\n    provider_lower = provider.lower()\n\n    if provider_lower not in cls._providers:\n        available = \", \".join(cls._providers.keys())\n        raise ConfigurationError(\n            f\"Provider '{provider}' not found. Available providers: {available}\"\n        )\n\n    provider_class = cls._providers[provider_lower]\n\n    # Merge config dict and kwargs\n    final_config = config.copy() if config else {}\n    final_config.update(kwargs)\n\n    try:\n        return provider_class(**final_config)\n    except TypeError as e:\n        raise ConfigurationError(f\"Invalid configuration for provider '{provider}': {e}\") from e\n</code></pre>"},{"location":"api/factory/#bruno_llm.factory.LLMFactory.create_from_env","title":"<code>create_from_env(provider, prefix=None)</code>  <code>classmethod</code>","text":"<p>Create provider from environment variables.</p> <p>Environment variables are read using the pattern: {PREFIX}{PROVIDER}{SETTING}</p> <p>Parameters:</p> Name Type Description Default <code>provider</code> <code>str</code> <p>Provider name</p> required <code>prefix</code> <code>Optional[str]</code> <p>Environment variable prefix (default: \"BRUNO_LLM\")</p> <code>None</code> <p>Returns:</p> Type Description <code>LLMInterface</code> <p>Configured LLMInterface instance</p> <p>Raises:</p> Type Description <code>ConfigurationError</code> <p>If required env vars missing</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; # With BRUNO_LLM_OPENAI_API_KEY=sk-...\n&gt;&gt;&gt; # and BRUNO_LLM_OPENAI_MODEL=gpt-4\n&gt;&gt;&gt; llm = LLMFactory.create_from_env(\"openai\")\n</code></pre> <pre><code>&gt;&gt;&gt; # Custom prefix\n&gt;&gt;&gt; # MY_APP_OLLAMA_MODEL=llama2\n&gt;&gt;&gt; llm = LLMFactory.create_from_env(\"ollama\", prefix=\"MY_APP\")\n</code></pre> Source code in <code>bruno_llm/factory.py</code> <pre><code>@classmethod\ndef create_from_env(cls, provider: str, prefix: Optional[str] = None) -&gt; LLMInterface:\n    \"\"\"\n    Create provider from environment variables.\n\n    Environment variables are read using the pattern:\n    {PREFIX}_{PROVIDER}_{SETTING}\n\n    Args:\n        provider: Provider name\n        prefix: Environment variable prefix (default: \"BRUNO_LLM\")\n\n    Returns:\n        Configured LLMInterface instance\n\n    Raises:\n        ConfigurationError: If required env vars missing\n\n    Examples:\n        &gt;&gt;&gt; # With BRUNO_LLM_OPENAI_API_KEY=sk-...\n        &gt;&gt;&gt; # and BRUNO_LLM_OPENAI_MODEL=gpt-4\n        &gt;&gt;&gt; llm = LLMFactory.create_from_env(\"openai\")\n\n        &gt;&gt;&gt; # Custom prefix\n        &gt;&gt;&gt; # MY_APP_OLLAMA_MODEL=llama2\n        &gt;&gt;&gt; llm = LLMFactory.create_from_env(\"ollama\", prefix=\"MY_APP\")\n    \"\"\"\n    prefix = prefix or \"BRUNO_LLM\"\n    provider_upper = provider.upper()\n    env_prefix = f\"{prefix}_{provider_upper}_\"\n\n    # Collect all matching environment variables\n    config: dict[str, Any] = {}\n    for key, value in os.environ.items():\n        if key.startswith(env_prefix):\n            # Remove prefix and convert to lowercase\n            setting_name = key[len(env_prefix) :].lower()\n            config[setting_name] = value\n\n    if not config:\n        raise ConfigurationError(\n            f\"No environment variables found for provider '{provider}'. \"\n            f\"Expected variables starting with {env_prefix}\"\n        )\n\n    return cls.create(provider, config)\n</code></pre>"},{"location":"api/factory/#bruno_llm.factory.LLMFactory.create_with_fallback","title":"<code>create_with_fallback(providers, configs=None)</code>  <code>async</code> <code>classmethod</code>","text":"<p>Create provider with fallback chain.</p> <p>Tries each provider in order until one connects successfully.</p> <p>Parameters:</p> Name Type Description Default <code>providers</code> <code>list[str]</code> <p>List of provider names in priority order</p> required <code>configs</code> <code>Optional[list[dict[str, Any]]]</code> <p>Optional list of configurations (must match providers length)</p> <code>None</code> <p>Returns:</p> Type Description <code>LLMInterface</code> <p>First successfully connected LLMInterface instance</p> <p>Raises:</p> Type Description <code>LLMError</code> <p>If all providers fail to connect</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; llm = await LLMFactory.create_with_fallback(\n...     providers=[\"openai\", \"ollama\"],\n...     configs=[\n...         {\"api_key\": \"sk-...\", \"model\": \"gpt-4\"},\n...         {\"model\": \"llama2\"}\n...     ]\n... )\n</code></pre> Source code in <code>bruno_llm/factory.py</code> <pre><code>@classmethod\nasync def create_with_fallback(\n    cls, providers: list[str], configs: Optional[list[dict[str, Any]]] = None\n) -&gt; LLMInterface:\n    \"\"\"\n    Create provider with fallback chain.\n\n    Tries each provider in order until one connects successfully.\n\n    Args:\n        providers: List of provider names in priority order\n        configs: Optional list of configurations (must match providers length)\n\n    Returns:\n        First successfully connected LLMInterface instance\n\n    Raises:\n        LLMError: If all providers fail to connect\n\n    Examples:\n        &gt;&gt;&gt; llm = await LLMFactory.create_with_fallback(\n        ...     providers=[\"openai\", \"ollama\"],\n        ...     configs=[\n        ...         {\"api_key\": \"sk-...\", \"model\": \"gpt-4\"},\n        ...         {\"model\": \"llama2\"}\n        ...     ]\n        ... )\n    \"\"\"\n    if not providers:\n        raise ConfigurationError(\"No providers specified for fallback\")\n\n    if configs and len(configs) != len(providers):\n        raise ConfigurationError(\n            f\"configs length ({len(configs)}) must match providers length ({len(providers)})\"\n        )\n\n    errors = []\n\n    for i, provider_name in enumerate(providers):\n        try:\n            config = configs[i] if configs else {}\n            provider = cls.create(provider_name, config)\n\n            # Test connection\n            if await provider.check_connection():\n                return provider\n            else:\n                errors.append(f\"{provider_name}: Connection check failed\")\n\n        except Exception as e:\n            errors.append(f\"{provider_name}: {e}\")\n            continue\n\n    # All providers failed\n    error_details = \"; \".join(errors)\n    raise LLMError(\n        f\"All providers failed to connect. Tried: {', '.join(providers)}. \"\n        f\"Errors: {error_details}\"\n    )\n</code></pre>"},{"location":"api/factory/#bruno_llm.factory.LLMFactory.list_providers","title":"<code>list_providers()</code>  <code>classmethod</code>","text":"<p>List all registered providers.</p> <p>Returns:</p> Type Description <code>list[str]</code> <p>List of provider names</p> Source code in <code>bruno_llm/factory.py</code> <pre><code>@classmethod\ndef list_providers(cls) -&gt; list[str]:\n    \"\"\"\n    List all registered providers.\n\n    Returns:\n        List of provider names\n    \"\"\"\n    return sorted(cls._providers.keys())\n</code></pre>"},{"location":"api/factory/#bruno_llm.factory.LLMFactory.is_registered","title":"<code>is_registered(provider)</code>  <code>classmethod</code>","text":"<p>Check if a provider is registered.</p> <p>Parameters:</p> Name Type Description Default <code>provider</code> <code>str</code> <p>Provider name</p> required <p>Returns:</p> Type Description <code>bool</code> <p>True if provider is registered</p> Source code in <code>bruno_llm/factory.py</code> <pre><code>@classmethod\ndef is_registered(cls, provider: str) -&gt; bool:\n    \"\"\"\n    Check if a provider is registered.\n\n    Args:\n        provider: Provider name\n\n    Returns:\n        True if provider is registered\n    \"\"\"\n    return provider.lower() in cls._providers\n</code></pre>"},{"location":"api/providers/","title":"Providers","text":""},{"location":"api/providers/#bruno_llm.providers.ollama.OllamaProvider","title":"<code>bruno_llm.providers.ollama.OllamaProvider</code>","text":"<p>               Bases: <code>BaseProvider</code>, <code>LLMInterface</code></p> <p>Ollama provider for local LLM inference.</p> <p>Ollama runs models locally without API keys. Requires Ollama to be installed and running on the specified base_url.</p> <p>Parameters:</p> Name Type Description Default <code>base_url</code> <code>str</code> <p>Ollama API endpoint (default: http://localhost:11434)</p> <code>'http://localhost:11434'</code> <code>model</code> <code>str</code> <p>Model name (default: llama2)</p> <code>'llama2'</code> <code>timeout</code> <code>float</code> <p>Request timeout in seconds (default: 30.0)</p> <code>30.0</code> <code>**kwargs</code> <code>Any</code> <p>Additional configuration parameters</p> <code>{}</code> <p>Examples:</p> <pre><code>&gt;&gt;&gt; provider = OllamaProvider(model=\"llama2\")\n&gt;&gt;&gt; response = await provider.generate([\n...     Message(role=MessageRole.USER, content=\"Hello\")\n... ])\n</code></pre> <pre><code>&gt;&gt;&gt; # Streaming\n&gt;&gt;&gt; async for chunk in provider.stream([\n...     Message(role=MessageRole.USER, content=\"Tell me a story\")\n... ]):\n...     print(chunk, end=\"\")\n</code></pre> See Also <ul> <li>https://ollama.ai/</li> <li>bruno-core LLMInterface documentation</li> </ul> Source code in <code>bruno_llm/providers/ollama/provider.py</code> <pre><code>class OllamaProvider(BaseProvider, LLMInterface):\n    \"\"\"\n    Ollama provider for local LLM inference.\n\n    Ollama runs models locally without API keys. Requires Ollama\n    to be installed and running on the specified base_url.\n\n    Args:\n        base_url: Ollama API endpoint (default: http://localhost:11434)\n        model: Model name (default: llama2)\n        timeout: Request timeout in seconds (default: 30.0)\n        **kwargs: Additional configuration parameters\n\n    Examples:\n        &gt;&gt;&gt; provider = OllamaProvider(model=\"llama2\")\n        &gt;&gt;&gt; response = await provider.generate([\n        ...     Message(role=MessageRole.USER, content=\"Hello\")\n        ... ])\n\n        &gt;&gt;&gt; # Streaming\n        &gt;&gt;&gt; async for chunk in provider.stream([\n        ...     Message(role=MessageRole.USER, content=\"Tell me a story\")\n        ... ]):\n        ...     print(chunk, end=\"\")\n\n    See Also:\n        - https://ollama.ai/\n        - bruno-core LLMInterface documentation\n    \"\"\"\n\n    def __init__(\n        self,\n        base_url: str = \"http://localhost:11434\",\n        model: str = \"llama2\",\n        timeout: float = 30.0,\n        **kwargs: Any,\n    ):\n        \"\"\"Initialize Ollama provider.\"\"\"\n        # Create config\n        config = OllamaConfig(base_url=base_url, model=model, timeout=timeout, **kwargs)\n\n        # Initialize base provider\n        super().__init__(\n            provider_name=\"ollama\",\n            max_retries=3,\n            timeout=timeout,\n        )\n\n        # Store config\n        self._config = config\n        self._model = config.model\n\n        # Create HTTP client\n        self._client = httpx.AsyncClient(\n            base_url=config.base_url,\n            timeout=httpx.Timeout(config.timeout),\n        )\n\n        # Token counter (simple estimation for local models)\n        self._token_counter = SimpleTokenCounter()\n\n    @property\n    def model(self) -&gt; str:\n        \"\"\"Get current model name.\"\"\"\n        return self._model\n\n    @property\n    def config(self) -&gt; OllamaConfig:\n        \"\"\"Get provider configuration.\"\"\"\n        return self._config\n\n    def _format_messages(self, messages: list[Message]) -&gt; list[dict[str, str]]:\n        \"\"\"Convert bruno-core messages to Ollama format.\"\"\"\n        return [{\"role\": msg.role.value, \"content\": msg.content} for msg in messages]\n\n    def _build_request(\n        self, messages: list[Message], stream: bool = False, **kwargs: Any\n    ) -&gt; dict[str, Any]:\n        \"\"\"Build Ollama API request.\"\"\"\n        request = {\n            \"model\": self._model,\n            \"messages\": self._format_messages(messages),\n            \"stream\": stream,\n        }\n\n        # Add optional parameters\n        options: dict[str, Any] = {}\n\n        if self._config.temperature is not None:\n            options[\"temperature\"] = self._config.temperature\n        if self._config.top_p is not None:\n            options[\"top_p\"] = self._config.top_p\n        if self._config.top_k is not None:\n            options[\"top_k\"] = self._config.top_k\n        if self._config.num_predict is not None:\n            options[\"num_predict\"] = self._config.num_predict\n        if self._config.stop is not None:\n            request[\"stop\"] = self._config.stop\n\n        # Override with kwargs\n        options.update(kwargs)\n\n        if options:\n            request[\"options\"] = options\n\n        return request\n\n    async def generate(self, messages: list[Message], **kwargs: Any) -&gt; str:\n        \"\"\"\n        Generate a complete response from Ollama.\n\n        Args:\n            messages: List of conversation messages\n            **kwargs: Additional generation parameters\n\n        Returns:\n            Generated text response\n\n        Raises:\n            ModelNotFoundError: If model doesn't exist\n            LLMTimeoutError: If request times out\n            LLMError: For other API errors\n        \"\"\"\n        try:\n            request = self._build_request(messages, stream=False, **kwargs)\n\n            response = await self._client.post(\n                \"/api/chat\",\n                json=request,\n            )\n            response.raise_for_status()\n\n            data = response.json()\n\n            # Extract response content\n            if \"message\" not in data:\n                raise InvalidResponseError(\"No 'message' in response\")\n\n            content = data[\"message\"].get(\"content\", \"\")\n            return content\n\n        except httpx.TimeoutException as e:\n            raise LLMTimeoutError(f\"Request timed out: {e}\") from e\n        except httpx.HTTPStatusError as e:\n            if e.response.status_code == 404:\n                raise ModelNotFoundError(\n                    f\"Model '{self._model}' not found. \"\n                    f\"Run 'ollama pull {self._model}' to download it.\"\n                ) from e\n            raise LLMError(f\"HTTP error: {e}\") from e\n        except httpx.RequestError as e:\n            raise LLMError(\n                f\"Failed to connect to Ollama at {self._config.base_url}. \"\n                f\"Make sure Ollama is running: {e}\"\n            ) from e\n        except (KeyError, json.JSONDecodeError) as e:\n            raise InvalidResponseError(f\"Invalid response format: {e}\") from e\n\n    async def stream(self, messages: list[Message], **kwargs: Any) -&gt; AsyncIterator[str]:\n        \"\"\"\n        Stream response tokens from Ollama.\n\n        Args:\n            messages: List of conversation messages\n            **kwargs: Additional generation parameters\n\n        Yields:\n            Response text chunks\n\n        Raises:\n            StreamError: If streaming fails\n        \"\"\"\n        try:\n            request = self._build_request(messages, stream=True, **kwargs)\n\n            async with self._client.stream(\n                \"POST\",\n                \"/api/chat\",\n                json=request,\n            ) as response:\n                response.raise_for_status()\n\n                async for line in response.aiter_lines():\n                    if not line.strip():\n                        continue\n\n                    try:\n                        data = json.loads(line)\n\n                        # Check if streaming is done\n                        if data.get(\"done\", False):\n                            break\n\n                        # Extract content chunk\n                        if \"message\" in data:\n                            content = data[\"message\"].get(\"content\", \"\")\n                            if content:\n                                yield content\n\n                    except json.JSONDecodeError:\n                        # Skip malformed lines\n                        continue\n\n        except httpx.HTTPStatusError as e:\n            if e.response.status_code == 404:\n                raise ModelNotFoundError(\n                    f\"Model '{self._model}' not found. \"\n                    f\"Run 'ollama pull {self._model}' to download it.\"\n                ) from e\n            raise StreamError(f\"Streaming failed: {e}\") from e\n        except httpx.RequestError as e:\n            raise StreamError(f\"Failed to connect to Ollama: {e}\") from e\n        except Exception as e:\n            raise StreamError(f\"Unexpected streaming error: {e}\") from e\n\n    async def list_models(self) -&gt; list[str]:\n        \"\"\"\n        List available models in Ollama.\n\n        Returns:\n            List of model names\n\n        Raises:\n            LLMError: If request fails\n        \"\"\"\n        try:\n            response = await self._client.get(\"/api/tags\")\n            response.raise_for_status()\n\n            data = response.json()\n            models = data.get(\"models\", [])\n\n            return [model[\"name\"] for model in models]\n\n        except httpx.RequestError as e:\n            raise LLMError(f\"Failed to list models: {e}\") from e\n        except (KeyError, json.JSONDecodeError) as e:\n            raise InvalidResponseError(f\"Invalid response format: {e}\") from e\n\n    async def check_connection(self) -&gt; bool:\n        \"\"\"\n        Check if Ollama is accessible.\n\n        Returns:\n            True if Ollama is running and accessible\n        \"\"\"\n        try:\n            response = await self._client.get(\"/api/tags\")\n            return response.status_code == 200\n        except Exception:\n            return False\n\n    def get_token_count(self, text: str) -&gt; int:\n        \"\"\"\n        Estimate token count for text.\n\n        Args:\n            text: Text to count tokens for\n\n        Returns:\n            Estimated token count\n        \"\"\"\n        return self._token_counter.count_tokens(text)\n\n    def get_model_info(self) -&gt; dict[str, Any]:\n        \"\"\"\n        Get current model information.\n\n        Returns:\n            Dictionary with model details\n        \"\"\"\n        return {\n            \"provider\": \"ollama\",\n            \"model\": self._model,\n            \"base_url\": self._config.base_url,\n            \"temperature\": self._config.temperature,\n            \"max_tokens\": self._config.num_predict,\n        }\n\n    async def close(self) -&gt; None:\n        \"\"\"Close HTTP client and cleanup resources.\"\"\"\n        await self._client.aclose()\n\n    async def __aenter__(self):\n        \"\"\"Context manager entry.\"\"\"\n        return self\n\n    async def __aexit__(self, exc_type, exc_val, exc_tb):\n        \"\"\"Context manager exit.\"\"\"\n        await self.close()\n</code></pre>"},{"location":"api/providers/#bruno_llm.providers.ollama.OllamaProvider.model","title":"<code>model</code>  <code>property</code>","text":"<p>Get current model name.</p>"},{"location":"api/providers/#bruno_llm.providers.ollama.OllamaProvider.config","title":"<code>config</code>  <code>property</code>","text":"<p>Get provider configuration.</p>"},{"location":"api/providers/#bruno_llm.providers.ollama.OllamaProvider.__init__","title":"<code>__init__(base_url='http://localhost:11434', model='llama2', timeout=30.0, **kwargs)</code>","text":"<p>Initialize Ollama provider.</p> Source code in <code>bruno_llm/providers/ollama/provider.py</code> <pre><code>def __init__(\n    self,\n    base_url: str = \"http://localhost:11434\",\n    model: str = \"llama2\",\n    timeout: float = 30.0,\n    **kwargs: Any,\n):\n    \"\"\"Initialize Ollama provider.\"\"\"\n    # Create config\n    config = OllamaConfig(base_url=base_url, model=model, timeout=timeout, **kwargs)\n\n    # Initialize base provider\n    super().__init__(\n        provider_name=\"ollama\",\n        max_retries=3,\n        timeout=timeout,\n    )\n\n    # Store config\n    self._config = config\n    self._model = config.model\n\n    # Create HTTP client\n    self._client = httpx.AsyncClient(\n        base_url=config.base_url,\n        timeout=httpx.Timeout(config.timeout),\n    )\n\n    # Token counter (simple estimation for local models)\n    self._token_counter = SimpleTokenCounter()\n</code></pre>"},{"location":"api/providers/#bruno_llm.providers.ollama.OllamaProvider.generate","title":"<code>generate(messages, **kwargs)</code>  <code>async</code>","text":"<p>Generate a complete response from Ollama.</p> <p>Parameters:</p> Name Type Description Default <code>messages</code> <code>list[Message]</code> <p>List of conversation messages</p> required <code>**kwargs</code> <code>Any</code> <p>Additional generation parameters</p> <code>{}</code> <p>Returns:</p> Type Description <code>str</code> <p>Generated text response</p> <p>Raises:</p> Type Description <code>ModelNotFoundError</code> <p>If model doesn't exist</p> <code>TimeoutError</code> <p>If request times out</p> <code>LLMError</code> <p>For other API errors</p> Source code in <code>bruno_llm/providers/ollama/provider.py</code> <pre><code>async def generate(self, messages: list[Message], **kwargs: Any) -&gt; str:\n    \"\"\"\n    Generate a complete response from Ollama.\n\n    Args:\n        messages: List of conversation messages\n        **kwargs: Additional generation parameters\n\n    Returns:\n        Generated text response\n\n    Raises:\n        ModelNotFoundError: If model doesn't exist\n        LLMTimeoutError: If request times out\n        LLMError: For other API errors\n    \"\"\"\n    try:\n        request = self._build_request(messages, stream=False, **kwargs)\n\n        response = await self._client.post(\n            \"/api/chat\",\n            json=request,\n        )\n        response.raise_for_status()\n\n        data = response.json()\n\n        # Extract response content\n        if \"message\" not in data:\n            raise InvalidResponseError(\"No 'message' in response\")\n\n        content = data[\"message\"].get(\"content\", \"\")\n        return content\n\n    except httpx.TimeoutException as e:\n        raise LLMTimeoutError(f\"Request timed out: {e}\") from e\n    except httpx.HTTPStatusError as e:\n        if e.response.status_code == 404:\n            raise ModelNotFoundError(\n                f\"Model '{self._model}' not found. \"\n                f\"Run 'ollama pull {self._model}' to download it.\"\n            ) from e\n        raise LLMError(f\"HTTP error: {e}\") from e\n    except httpx.RequestError as e:\n        raise LLMError(\n            f\"Failed to connect to Ollama at {self._config.base_url}. \"\n            f\"Make sure Ollama is running: {e}\"\n        ) from e\n    except (KeyError, json.JSONDecodeError) as e:\n        raise InvalidResponseError(f\"Invalid response format: {e}\") from e\n</code></pre>"},{"location":"api/providers/#bruno_llm.providers.ollama.OllamaProvider.stream","title":"<code>stream(messages, **kwargs)</code>  <code>async</code>","text":"<p>Stream response tokens from Ollama.</p> <p>Parameters:</p> Name Type Description Default <code>messages</code> <code>list[Message]</code> <p>List of conversation messages</p> required <code>**kwargs</code> <code>Any</code> <p>Additional generation parameters</p> <code>{}</code> <p>Yields:</p> Type Description <code>AsyncIterator[str]</code> <p>Response text chunks</p> <p>Raises:</p> Type Description <code>StreamError</code> <p>If streaming fails</p> Source code in <code>bruno_llm/providers/ollama/provider.py</code> <pre><code>async def stream(self, messages: list[Message], **kwargs: Any) -&gt; AsyncIterator[str]:\n    \"\"\"\n    Stream response tokens from Ollama.\n\n    Args:\n        messages: List of conversation messages\n        **kwargs: Additional generation parameters\n\n    Yields:\n        Response text chunks\n\n    Raises:\n        StreamError: If streaming fails\n    \"\"\"\n    try:\n        request = self._build_request(messages, stream=True, **kwargs)\n\n        async with self._client.stream(\n            \"POST\",\n            \"/api/chat\",\n            json=request,\n        ) as response:\n            response.raise_for_status()\n\n            async for line in response.aiter_lines():\n                if not line.strip():\n                    continue\n\n                try:\n                    data = json.loads(line)\n\n                    # Check if streaming is done\n                    if data.get(\"done\", False):\n                        break\n\n                    # Extract content chunk\n                    if \"message\" in data:\n                        content = data[\"message\"].get(\"content\", \"\")\n                        if content:\n                            yield content\n\n                except json.JSONDecodeError:\n                    # Skip malformed lines\n                    continue\n\n    except httpx.HTTPStatusError as e:\n        if e.response.status_code == 404:\n            raise ModelNotFoundError(\n                f\"Model '{self._model}' not found. \"\n                f\"Run 'ollama pull {self._model}' to download it.\"\n            ) from e\n        raise StreamError(f\"Streaming failed: {e}\") from e\n    except httpx.RequestError as e:\n        raise StreamError(f\"Failed to connect to Ollama: {e}\") from e\n    except Exception as e:\n        raise StreamError(f\"Unexpected streaming error: {e}\") from e\n</code></pre>"},{"location":"api/providers/#bruno_llm.providers.ollama.OllamaProvider.list_models","title":"<code>list_models()</code>  <code>async</code>","text":"<p>List available models in Ollama.</p> <p>Returns:</p> Type Description <code>list[str]</code> <p>List of model names</p> <p>Raises:</p> Type Description <code>LLMError</code> <p>If request fails</p> Source code in <code>bruno_llm/providers/ollama/provider.py</code> <pre><code>async def list_models(self) -&gt; list[str]:\n    \"\"\"\n    List available models in Ollama.\n\n    Returns:\n        List of model names\n\n    Raises:\n        LLMError: If request fails\n    \"\"\"\n    try:\n        response = await self._client.get(\"/api/tags\")\n        response.raise_for_status()\n\n        data = response.json()\n        models = data.get(\"models\", [])\n\n        return [model[\"name\"] for model in models]\n\n    except httpx.RequestError as e:\n        raise LLMError(f\"Failed to list models: {e}\") from e\n    except (KeyError, json.JSONDecodeError) as e:\n        raise InvalidResponseError(f\"Invalid response format: {e}\") from e\n</code></pre>"},{"location":"api/providers/#bruno_llm.providers.ollama.OllamaProvider.check_connection","title":"<code>check_connection()</code>  <code>async</code>","text":"<p>Check if Ollama is accessible.</p> <p>Returns:</p> Type Description <code>bool</code> <p>True if Ollama is running and accessible</p> Source code in <code>bruno_llm/providers/ollama/provider.py</code> <pre><code>async def check_connection(self) -&gt; bool:\n    \"\"\"\n    Check if Ollama is accessible.\n\n    Returns:\n        True if Ollama is running and accessible\n    \"\"\"\n    try:\n        response = await self._client.get(\"/api/tags\")\n        return response.status_code == 200\n    except Exception:\n        return False\n</code></pre>"},{"location":"api/providers/#bruno_llm.providers.ollama.OllamaProvider.get_token_count","title":"<code>get_token_count(text)</code>","text":"<p>Estimate token count for text.</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>str</code> <p>Text to count tokens for</p> required <p>Returns:</p> Type Description <code>int</code> <p>Estimated token count</p> Source code in <code>bruno_llm/providers/ollama/provider.py</code> <pre><code>def get_token_count(self, text: str) -&gt; int:\n    \"\"\"\n    Estimate token count for text.\n\n    Args:\n        text: Text to count tokens for\n\n    Returns:\n        Estimated token count\n    \"\"\"\n    return self._token_counter.count_tokens(text)\n</code></pre>"},{"location":"api/providers/#bruno_llm.providers.ollama.OllamaProvider.get_model_info","title":"<code>get_model_info()</code>","text":"<p>Get current model information.</p> <p>Returns:</p> Type Description <code>dict[str, Any]</code> <p>Dictionary with model details</p> Source code in <code>bruno_llm/providers/ollama/provider.py</code> <pre><code>def get_model_info(self) -&gt; dict[str, Any]:\n    \"\"\"\n    Get current model information.\n\n    Returns:\n        Dictionary with model details\n    \"\"\"\n    return {\n        \"provider\": \"ollama\",\n        \"model\": self._model,\n        \"base_url\": self._config.base_url,\n        \"temperature\": self._config.temperature,\n        \"max_tokens\": self._config.num_predict,\n    }\n</code></pre>"},{"location":"api/providers/#bruno_llm.providers.ollama.OllamaProvider.close","title":"<code>close()</code>  <code>async</code>","text":"<p>Close HTTP client and cleanup resources.</p> Source code in <code>bruno_llm/providers/ollama/provider.py</code> <pre><code>async def close(self) -&gt; None:\n    \"\"\"Close HTTP client and cleanup resources.\"\"\"\n    await self._client.aclose()\n</code></pre>"},{"location":"api/providers/#bruno_llm.providers.ollama.OllamaProvider.__aenter__","title":"<code>__aenter__()</code>  <code>async</code>","text":"<p>Context manager entry.</p> Source code in <code>bruno_llm/providers/ollama/provider.py</code> <pre><code>async def __aenter__(self):\n    \"\"\"Context manager entry.\"\"\"\n    return self\n</code></pre>"},{"location":"api/providers/#bruno_llm.providers.ollama.OllamaProvider.__aexit__","title":"<code>__aexit__(exc_type, exc_val, exc_tb)</code>  <code>async</code>","text":"<p>Context manager exit.</p> Source code in <code>bruno_llm/providers/ollama/provider.py</code> <pre><code>async def __aexit__(self, exc_type, exc_val, exc_tb):\n    \"\"\"Context manager exit.\"\"\"\n    await self.close()\n</code></pre>"},{"location":"api/providers/#bruno_llm.providers.openai.OpenAIProvider","title":"<code>bruno_llm.providers.openai.OpenAIProvider</code>","text":"<p>               Bases: <code>BaseProvider</code>, <code>LLMInterface</code></p> <p>OpenAI provider for GPT models.</p> <p>Provides access to OpenAI's GPT models (GPT-4, GPT-3.5-turbo, etc.) via the official OpenAI API. Requires an API key.</p> <p>Parameters:</p> Name Type Description Default <code>api_key</code> <code>str</code> <p>OpenAI API key (required)</p> required <code>model</code> <code>str</code> <p>Model name (default: gpt-4)</p> <code>'gpt-4'</code> <code>organization</code> <code>Optional[str]</code> <p>Organization ID (optional)</p> <code>None</code> <code>timeout</code> <code>float</code> <p>Request timeout in seconds (default: 30.0)</p> <code>30.0</code> <code>**kwargs</code> <code>Any</code> <p>Additional configuration parameters</p> <code>{}</code> <p>Examples:</p> <pre><code>&gt;&gt;&gt; provider = OpenAIProvider(api_key=\"sk-...\", model=\"gpt-4\")\n&gt;&gt;&gt; response = await provider.generate([\n...     Message(role=MessageRole.USER, content=\"Hello\")\n... ])\n</code></pre> <pre><code>&gt;&gt;&gt; # Streaming\n&gt;&gt;&gt; async for chunk in provider.stream([\n...     Message(role=MessageRole.USER, content=\"Tell me a story\")\n... ]):\n...     print(chunk, end=\"\")\n</code></pre> <pre><code>&gt;&gt;&gt; # With cost tracking\n&gt;&gt;&gt; provider = OpenAIProvider(api_key=\"sk-...\", track_cost=True)\n&gt;&gt;&gt; await provider.generate([...])\n&gt;&gt;&gt; report = provider.cost_tracker.get_usage_report()\n</code></pre> See Also <ul> <li>https://platform.openai.com/docs/api-reference</li> <li>bruno-core LLMInterface documentation</li> </ul> Source code in <code>bruno_llm/providers/openai/provider.py</code> <pre><code>class OpenAIProvider(BaseProvider, LLMInterface):\n    \"\"\"\n    OpenAI provider for GPT models.\n\n    Provides access to OpenAI's GPT models (GPT-4, GPT-3.5-turbo, etc.)\n    via the official OpenAI API. Requires an API key.\n\n    Args:\n        api_key: OpenAI API key (required)\n        model: Model name (default: gpt-4)\n        organization: Organization ID (optional)\n        timeout: Request timeout in seconds (default: 30.0)\n        **kwargs: Additional configuration parameters\n\n    Examples:\n        &gt;&gt;&gt; provider = OpenAIProvider(api_key=\"sk-...\", model=\"gpt-4\")\n        &gt;&gt;&gt; response = await provider.generate([\n        ...     Message(role=MessageRole.USER, content=\"Hello\")\n        ... ])\n\n        &gt;&gt;&gt; # Streaming\n        &gt;&gt;&gt; async for chunk in provider.stream([\n        ...     Message(role=MessageRole.USER, content=\"Tell me a story\")\n        ... ]):\n        ...     print(chunk, end=\"\")\n\n        &gt;&gt;&gt; # With cost tracking\n        &gt;&gt;&gt; provider = OpenAIProvider(api_key=\"sk-...\", track_cost=True)\n        &gt;&gt;&gt; await provider.generate([...])\n        &gt;&gt;&gt; report = provider.cost_tracker.get_usage_report()\n\n    See Also:\n        - https://platform.openai.com/docs/api-reference\n        - bruno-core LLMInterface documentation\n    \"\"\"\n\n    def __init__(\n        self,\n        api_key: str,\n        model: str = \"gpt-4\",\n        organization: Optional[str] = None,\n        timeout: float = 30.0,\n        track_cost: bool = True,\n        **kwargs: Any,\n    ):\n        \"\"\"Initialize OpenAI provider.\"\"\"\n        # Create config\n        config = OpenAIConfig(\n            api_key=api_key, model=model, organization=organization, timeout=timeout, **kwargs\n        )\n\n        # Initialize base provider\n        super().__init__(\n            provider_name=\"openai\",\n            max_retries=config.max_retries,\n            timeout=timeout,\n        )\n\n        # Store config\n        self._config = config\n        self._model = config.model\n\n        # Create OpenAI client\n        self._client = AsyncOpenAI(\n            api_key=config.api_key.get_secret_value(),\n            organization=config.organization,\n            base_url=config.base_url,\n            timeout=config.timeout,\n            max_retries=0,  # We handle retries in BaseProvider\n        )\n\n        # Token counter (tiktoken for accurate counting)\n        self._token_counter = create_token_counter(\"openai\", model=model)\n\n        # Cost tracker\n        self._track_cost = track_cost\n        if track_cost:\n            self.cost_tracker = CostTracker(\n                provider_name=\"openai\",\n                pricing=PRICING_OPENAI,\n            )\n\n    @property\n    def model(self) -&gt; str:\n        \"\"\"Get current model name.\"\"\"\n        return self._model\n\n    @property\n    def config(self) -&gt; OpenAIConfig:\n        \"\"\"Get provider configuration.\"\"\"\n        return self._config\n\n    def _format_messages(self, messages: list[Message]) -&gt; list[dict[str, str]]:\n        \"\"\"Convert bruno-core messages to OpenAI format.\"\"\"\n        return [{\"role\": msg.role.value, \"content\": msg.content} for msg in messages]\n\n    def _build_request_params(self, **kwargs: Any) -&gt; dict[str, Any]:\n        \"\"\"Build OpenAI API request parameters.\"\"\"\n        params: dict[str, Any] = {\n            \"model\": self._model,\n            \"temperature\": self._config.temperature,\n            \"top_p\": self._config.top_p,\n        }\n\n        # Add optional parameters\n        if self._config.max_tokens is not None:\n            params[\"max_tokens\"] = self._config.max_tokens\n        if self._config.presence_penalty != 0.0:\n            params[\"presence_penalty\"] = self._config.presence_penalty\n        if self._config.frequency_penalty != 0.0:\n            params[\"frequency_penalty\"] = self._config.frequency_penalty\n        if self._config.stop is not None:\n            params[\"stop\"] = self._config.stop\n\n        # Override with kwargs\n        params.update(kwargs)\n\n        return params\n\n    def _track_usage(\n        self,\n        messages: list[Message],\n        response_text: str,\n        completion: Optional[ChatCompletion] = None,\n    ) -&gt; None:\n        \"\"\"Track token usage and costs.\"\"\"\n        if not self._track_cost:\n            return\n\n        # Get token counts from response or estimate\n        if completion and hasattr(completion, \"usage\") and completion.usage:\n            input_tokens = completion.usage.prompt_tokens\n            output_tokens = completion.usage.completion_tokens\n        else:\n            # Estimate if usage not available\n            input_text = \" \".join(msg.content for msg in messages)\n            input_tokens = self._token_counter.count_tokens(input_text)\n            output_tokens = self._token_counter.count_tokens(response_text)\n\n        # Track in cost tracker\n        self.cost_tracker.track_request(\n            model=self._model,\n            input_tokens=input_tokens,\n            output_tokens=output_tokens,\n        )\n\n    async def generate(self, messages: list[Message], **kwargs: Any) -&gt; str:\n        \"\"\"\n        Generate a complete response from OpenAI.\n\n        Args:\n            messages: List of conversation messages\n            **kwargs: Additional generation parameters\n\n        Returns:\n            Generated text response\n\n        Raises:\n            AuthenticationError: If API key is invalid\n            RateLimitError: If rate limit exceeded\n            ModelNotFoundError: If model doesn't exist\n            LLMTimeoutError: If request times out\n            LLMError: For other API errors\n        \"\"\"\n        try:\n            params = self._build_request_params(**kwargs)\n\n            completion: ChatCompletion = await self._client.chat.completions.create(\n                messages=self._format_messages(messages), **params\n            )\n\n            # Extract response content\n            if not completion.choices:\n                raise InvalidResponseError(\"No choices in response\")\n\n            content = completion.choices[0].message.content or \"\"\n\n            # Track usage\n            self._track_usage(messages, content, completion)\n\n            return content\n\n        except APITimeoutError as e:\n            raise LLMTimeoutError(f\"Request timed out: {e}\") from e\n        except APIConnectionError as e:\n            raise LLMError(f\"Connection error: {e}\") from e\n        except OpenAIError as e:\n            # Parse OpenAI-specific errors by exception type\n            if isinstance(e, OpenAIAuthError):\n                raise AuthenticationError(f\"Invalid API key: {e}\") from e\n            elif isinstance(e, OpenAIRateLimitError):\n                raise RateLimitError(f\"Rate limit exceeded: {e}\") from e\n            elif isinstance(e, OpenAINotFoundError):\n                raise ModelNotFoundError(f\"Model '{self._model}' not found: {e}\") from e\n            else:\n                raise LLMError(f\"OpenAI API error: {e}\") from e\n\n    async def stream(self, messages: list[Message], **kwargs: Any) -&gt; AsyncIterator[str]:\n        \"\"\"\n        Stream response tokens from OpenAI.\n\n        Args:\n            messages: List of conversation messages\n            **kwargs: Additional generation parameters\n\n        Yields:\n            Response text chunks\n\n        Raises:\n            StreamError: If streaming fails\n        \"\"\"\n        try:\n            params = self._build_request_params(stream=True, **kwargs)\n\n            stream = await self._client.chat.completions.create(\n                messages=self._format_messages(messages), **params\n            )\n\n            full_response = []\n\n            async for chunk in stream:\n                if not chunk.choices:\n                    continue\n\n                delta = chunk.choices[0].delta\n                content = delta.content\n\n                if content:\n                    full_response.append(content)\n                    yield content\n\n            # Track usage after streaming completes\n            response_text = \"\".join(full_response)\n            self._track_usage(messages, response_text)\n\n        except APITimeoutError as e:\n            raise StreamError(f\"Stream timed out: {e}\") from e\n        except APIConnectionError as e:\n            raise StreamError(f\"Connection error: {e}\") from e\n        except OpenAIError as e:\n            # Parse OpenAI-specific errors by exception type\n            if isinstance(e, OpenAIAuthError):\n                raise AuthenticationError(f\"Invalid API key: {e}\") from e\n            elif isinstance(e, OpenAIRateLimitError):\n                raise RateLimitError(f\"Rate limit exceeded: {e}\") from e\n            elif isinstance(e, OpenAINotFoundError):\n                raise ModelNotFoundError(f\"Model '{self._model}' not found: {e}\") from e\n            else:\n                raise StreamError(f\"Streaming failed: {e}\") from e\n        except Exception as e:\n            raise StreamError(f\"Unexpected streaming error: {e}\") from e\n\n    async def list_models(self) -&gt; list[str]:\n        \"\"\"\n        List available OpenAI models.\n\n        Returns:\n            List of model IDs\n\n        Raises:\n            LLMError: If request fails\n        \"\"\"\n        try:\n            models = await self._client.models.list()\n            return [model.id for model in models.data]\n        except OpenAIError as e:\n            raise LLMError(f\"Failed to list models: {e}\") from e\n\n    async def check_connection(self) -&gt; bool:\n        \"\"\"\n        Check if OpenAI API is accessible.\n\n        Returns:\n            True if API is accessible with valid credentials\n        \"\"\"\n        try:\n            await self._client.models.list()\n            return True\n        except Exception:\n            return False\n\n    def get_token_count(self, text: str) -&gt; int:\n        \"\"\"\n        Get accurate token count for text using tiktoken.\n\n        Args:\n            text: Text to count tokens for\n\n        Returns:\n            Exact token count\n        \"\"\"\n        return self._token_counter.count_tokens(text)\n\n    def get_model_info(self) -&gt; dict[str, Any]:\n        \"\"\"\n        Get current model information.\n\n        Returns:\n            Dictionary with model details\n        \"\"\"\n        info = {\n            \"provider\": \"openai\",\n            \"model\": self._model,\n            \"base_url\": self._config.base_url,\n            \"temperature\": self._config.temperature,\n            \"max_tokens\": self._config.max_tokens,\n        }\n\n        # Add cost tracking info if enabled\n        if self._track_cost:\n            info[\"cost_tracking\"] = {\n                \"enabled\": True,\n                \"total_cost\": self.cost_tracker.get_total_cost(),\n                \"total_requests\": self.cost_tracker.get_request_count(),\n            }\n\n        return info\n\n    async def close(self) -&gt; None:\n        \"\"\"Close OpenAI client and cleanup resources.\"\"\"\n        await self._client.close()\n\n    async def __aenter__(self):\n        \"\"\"Context manager entry.\"\"\"\n        return self\n\n    async def __aexit__(self, exc_type, exc_val, exc_tb):\n        \"\"\"Context manager exit.\"\"\"\n        await self.close()\n</code></pre>"},{"location":"api/providers/#bruno_llm.providers.openai.OpenAIProvider.model","title":"<code>model</code>  <code>property</code>","text":"<p>Get current model name.</p>"},{"location":"api/providers/#bruno_llm.providers.openai.OpenAIProvider.config","title":"<code>config</code>  <code>property</code>","text":"<p>Get provider configuration.</p>"},{"location":"api/providers/#bruno_llm.providers.openai.OpenAIProvider.__init__","title":"<code>__init__(api_key, model='gpt-4', organization=None, timeout=30.0, track_cost=True, **kwargs)</code>","text":"<p>Initialize OpenAI provider.</p> Source code in <code>bruno_llm/providers/openai/provider.py</code> <pre><code>def __init__(\n    self,\n    api_key: str,\n    model: str = \"gpt-4\",\n    organization: Optional[str] = None,\n    timeout: float = 30.0,\n    track_cost: bool = True,\n    **kwargs: Any,\n):\n    \"\"\"Initialize OpenAI provider.\"\"\"\n    # Create config\n    config = OpenAIConfig(\n        api_key=api_key, model=model, organization=organization, timeout=timeout, **kwargs\n    )\n\n    # Initialize base provider\n    super().__init__(\n        provider_name=\"openai\",\n        max_retries=config.max_retries,\n        timeout=timeout,\n    )\n\n    # Store config\n    self._config = config\n    self._model = config.model\n\n    # Create OpenAI client\n    self._client = AsyncOpenAI(\n        api_key=config.api_key.get_secret_value(),\n        organization=config.organization,\n        base_url=config.base_url,\n        timeout=config.timeout,\n        max_retries=0,  # We handle retries in BaseProvider\n    )\n\n    # Token counter (tiktoken for accurate counting)\n    self._token_counter = create_token_counter(\"openai\", model=model)\n\n    # Cost tracker\n    self._track_cost = track_cost\n    if track_cost:\n        self.cost_tracker = CostTracker(\n            provider_name=\"openai\",\n            pricing=PRICING_OPENAI,\n        )\n</code></pre>"},{"location":"api/providers/#bruno_llm.providers.openai.OpenAIProvider.generate","title":"<code>generate(messages, **kwargs)</code>  <code>async</code>","text":"<p>Generate a complete response from OpenAI.</p> <p>Parameters:</p> Name Type Description Default <code>messages</code> <code>list[Message]</code> <p>List of conversation messages</p> required <code>**kwargs</code> <code>Any</code> <p>Additional generation parameters</p> <code>{}</code> <p>Returns:</p> Type Description <code>str</code> <p>Generated text response</p> <p>Raises:</p> Type Description <code>AuthenticationError</code> <p>If API key is invalid</p> <code>RateLimitError</code> <p>If rate limit exceeded</p> <code>ModelNotFoundError</code> <p>If model doesn't exist</p> <code>TimeoutError</code> <p>If request times out</p> <code>LLMError</code> <p>For other API errors</p> Source code in <code>bruno_llm/providers/openai/provider.py</code> <pre><code>async def generate(self, messages: list[Message], **kwargs: Any) -&gt; str:\n    \"\"\"\n    Generate a complete response from OpenAI.\n\n    Args:\n        messages: List of conversation messages\n        **kwargs: Additional generation parameters\n\n    Returns:\n        Generated text response\n\n    Raises:\n        AuthenticationError: If API key is invalid\n        RateLimitError: If rate limit exceeded\n        ModelNotFoundError: If model doesn't exist\n        LLMTimeoutError: If request times out\n        LLMError: For other API errors\n    \"\"\"\n    try:\n        params = self._build_request_params(**kwargs)\n\n        completion: ChatCompletion = await self._client.chat.completions.create(\n            messages=self._format_messages(messages), **params\n        )\n\n        # Extract response content\n        if not completion.choices:\n            raise InvalidResponseError(\"No choices in response\")\n\n        content = completion.choices[0].message.content or \"\"\n\n        # Track usage\n        self._track_usage(messages, content, completion)\n\n        return content\n\n    except APITimeoutError as e:\n        raise LLMTimeoutError(f\"Request timed out: {e}\") from e\n    except APIConnectionError as e:\n        raise LLMError(f\"Connection error: {e}\") from e\n    except OpenAIError as e:\n        # Parse OpenAI-specific errors by exception type\n        if isinstance(e, OpenAIAuthError):\n            raise AuthenticationError(f\"Invalid API key: {e}\") from e\n        elif isinstance(e, OpenAIRateLimitError):\n            raise RateLimitError(f\"Rate limit exceeded: {e}\") from e\n        elif isinstance(e, OpenAINotFoundError):\n            raise ModelNotFoundError(f\"Model '{self._model}' not found: {e}\") from e\n        else:\n            raise LLMError(f\"OpenAI API error: {e}\") from e\n</code></pre>"},{"location":"api/providers/#bruno_llm.providers.openai.OpenAIProvider.stream","title":"<code>stream(messages, **kwargs)</code>  <code>async</code>","text":"<p>Stream response tokens from OpenAI.</p> <p>Parameters:</p> Name Type Description Default <code>messages</code> <code>list[Message]</code> <p>List of conversation messages</p> required <code>**kwargs</code> <code>Any</code> <p>Additional generation parameters</p> <code>{}</code> <p>Yields:</p> Type Description <code>AsyncIterator[str]</code> <p>Response text chunks</p> <p>Raises:</p> Type Description <code>StreamError</code> <p>If streaming fails</p> Source code in <code>bruno_llm/providers/openai/provider.py</code> <pre><code>async def stream(self, messages: list[Message], **kwargs: Any) -&gt; AsyncIterator[str]:\n    \"\"\"\n    Stream response tokens from OpenAI.\n\n    Args:\n        messages: List of conversation messages\n        **kwargs: Additional generation parameters\n\n    Yields:\n        Response text chunks\n\n    Raises:\n        StreamError: If streaming fails\n    \"\"\"\n    try:\n        params = self._build_request_params(stream=True, **kwargs)\n\n        stream = await self._client.chat.completions.create(\n            messages=self._format_messages(messages), **params\n        )\n\n        full_response = []\n\n        async for chunk in stream:\n            if not chunk.choices:\n                continue\n\n            delta = chunk.choices[0].delta\n            content = delta.content\n\n            if content:\n                full_response.append(content)\n                yield content\n\n        # Track usage after streaming completes\n        response_text = \"\".join(full_response)\n        self._track_usage(messages, response_text)\n\n    except APITimeoutError as e:\n        raise StreamError(f\"Stream timed out: {e}\") from e\n    except APIConnectionError as e:\n        raise StreamError(f\"Connection error: {e}\") from e\n    except OpenAIError as e:\n        # Parse OpenAI-specific errors by exception type\n        if isinstance(e, OpenAIAuthError):\n            raise AuthenticationError(f\"Invalid API key: {e}\") from e\n        elif isinstance(e, OpenAIRateLimitError):\n            raise RateLimitError(f\"Rate limit exceeded: {e}\") from e\n        elif isinstance(e, OpenAINotFoundError):\n            raise ModelNotFoundError(f\"Model '{self._model}' not found: {e}\") from e\n        else:\n            raise StreamError(f\"Streaming failed: {e}\") from e\n    except Exception as e:\n        raise StreamError(f\"Unexpected streaming error: {e}\") from e\n</code></pre>"},{"location":"api/providers/#bruno_llm.providers.openai.OpenAIProvider.list_models","title":"<code>list_models()</code>  <code>async</code>","text":"<p>List available OpenAI models.</p> <p>Returns:</p> Type Description <code>list[str]</code> <p>List of model IDs</p> <p>Raises:</p> Type Description <code>LLMError</code> <p>If request fails</p> Source code in <code>bruno_llm/providers/openai/provider.py</code> <pre><code>async def list_models(self) -&gt; list[str]:\n    \"\"\"\n    List available OpenAI models.\n\n    Returns:\n        List of model IDs\n\n    Raises:\n        LLMError: If request fails\n    \"\"\"\n    try:\n        models = await self._client.models.list()\n        return [model.id for model in models.data]\n    except OpenAIError as e:\n        raise LLMError(f\"Failed to list models: {e}\") from e\n</code></pre>"},{"location":"api/providers/#bruno_llm.providers.openai.OpenAIProvider.check_connection","title":"<code>check_connection()</code>  <code>async</code>","text":"<p>Check if OpenAI API is accessible.</p> <p>Returns:</p> Type Description <code>bool</code> <p>True if API is accessible with valid credentials</p> Source code in <code>bruno_llm/providers/openai/provider.py</code> <pre><code>async def check_connection(self) -&gt; bool:\n    \"\"\"\n    Check if OpenAI API is accessible.\n\n    Returns:\n        True if API is accessible with valid credentials\n    \"\"\"\n    try:\n        await self._client.models.list()\n        return True\n    except Exception:\n        return False\n</code></pre>"},{"location":"api/providers/#bruno_llm.providers.openai.OpenAIProvider.get_token_count","title":"<code>get_token_count(text)</code>","text":"<p>Get accurate token count for text using tiktoken.</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>str</code> <p>Text to count tokens for</p> required <p>Returns:</p> Type Description <code>int</code> <p>Exact token count</p> Source code in <code>bruno_llm/providers/openai/provider.py</code> <pre><code>def get_token_count(self, text: str) -&gt; int:\n    \"\"\"\n    Get accurate token count for text using tiktoken.\n\n    Args:\n        text: Text to count tokens for\n\n    Returns:\n        Exact token count\n    \"\"\"\n    return self._token_counter.count_tokens(text)\n</code></pre>"},{"location":"api/providers/#bruno_llm.providers.openai.OpenAIProvider.get_model_info","title":"<code>get_model_info()</code>","text":"<p>Get current model information.</p> <p>Returns:</p> Type Description <code>dict[str, Any]</code> <p>Dictionary with model details</p> Source code in <code>bruno_llm/providers/openai/provider.py</code> <pre><code>def get_model_info(self) -&gt; dict[str, Any]:\n    \"\"\"\n    Get current model information.\n\n    Returns:\n        Dictionary with model details\n    \"\"\"\n    info = {\n        \"provider\": \"openai\",\n        \"model\": self._model,\n        \"base_url\": self._config.base_url,\n        \"temperature\": self._config.temperature,\n        \"max_tokens\": self._config.max_tokens,\n    }\n\n    # Add cost tracking info if enabled\n    if self._track_cost:\n        info[\"cost_tracking\"] = {\n            \"enabled\": True,\n            \"total_cost\": self.cost_tracker.get_total_cost(),\n            \"total_requests\": self.cost_tracker.get_request_count(),\n        }\n\n    return info\n</code></pre>"},{"location":"api/providers/#bruno_llm.providers.openai.OpenAIProvider.close","title":"<code>close()</code>  <code>async</code>","text":"<p>Close OpenAI client and cleanup resources.</p> Source code in <code>bruno_llm/providers/openai/provider.py</code> <pre><code>async def close(self) -&gt; None:\n    \"\"\"Close OpenAI client and cleanup resources.\"\"\"\n    await self._client.close()\n</code></pre>"},{"location":"api/providers/#bruno_llm.providers.openai.OpenAIProvider.__aenter__","title":"<code>__aenter__()</code>  <code>async</code>","text":"<p>Context manager entry.</p> Source code in <code>bruno_llm/providers/openai/provider.py</code> <pre><code>async def __aenter__(self):\n    \"\"\"Context manager entry.\"\"\"\n    return self\n</code></pre>"},{"location":"api/providers/#bruno_llm.providers.openai.OpenAIProvider.__aexit__","title":"<code>__aexit__(exc_type, exc_val, exc_tb)</code>  <code>async</code>","text":"<p>Context manager exit.</p> Source code in <code>bruno_llm/providers/openai/provider.py</code> <pre><code>async def __aexit__(self, exc_type, exc_val, exc_tb):\n    \"\"\"Context manager exit.\"\"\"\n    await self.close()\n</code></pre>"},{"location":"development/contributing/","title":"Contributing","text":"<p>See CONTRIBUTING.md in the repository for detailed contribution guidelines.</p>"},{"location":"development/contributing/#quick-start","title":"Quick Start","text":"<ol> <li>Fork and clone the repository</li> <li>Install dependencies: <code>pip install -e \".[all]\"</code></li> <li>Install pre-commit hooks: <code>pre-commit install</code></li> <li>Make changes and run tests: <code>pytest tests/</code></li> <li>Submit a pull request</li> </ol>"},{"location":"development/contributing/#pre-commit-hooks","title":"Pre-commit Hooks","text":"<p>Critical: Always install pre-commit hooks to catch issues before CI:</p> <pre><code>pip install pre-commit\npre-commit install\n</code></pre> <p>See Pre-commit Setup for complete guide.</p>"},{"location":"development/pre-commit/","title":"Pre-commit Hooks","text":"<p>See PRE_COMMIT_SETUP.md for complete documentation.</p>"},{"location":"development/pre-commit/#quick-setup","title":"Quick Setup","text":"<pre><code># Install pre-commit\npip install pre-commit\n\n# Install hooks (run once per repository)\npre-commit install\n</code></pre>"},{"location":"development/pre-commit/#what-it-does","title":"What It Does","text":"<p>On every commit, automatically:</p> <ul> <li>\u2705 Formats code with ruff</li> <li>\u2705 Runs linting checks</li> <li>\u2705 Performs type checking</li> <li>\u2705 Validates YAML/JSON/TOML</li> <li>\u2705 Removes trailing whitespace</li> <li>\u2705 Checks for large files and secrets</li> </ul>"},{"location":"development/pre-commit/#benefits","title":"Benefits","text":"<ul> <li>Catches issues before CI</li> <li>Auto-fixes formatting and linting</li> <li>Faster feedback (seconds vs minutes)</li> <li>Prevents CI failures</li> <li>Saves time and CI minutes</li> </ul>"},{"location":"development/testing/","title":"Testing","text":"<p>See TESTING.md for complete testing guide.</p>"},{"location":"development/testing/#running-tests","title":"Running Tests","text":"<pre><code># Run all tests\npytest tests/\n\n# Run with coverage\npytest tests/ --cov=bruno_llm --cov-report=html\n\n# Run specific test file\npytest tests/test_factory.py\n\n# Run integration tests\npytest -m integration\n</code></pre>"},{"location":"development/testing/#test-organization","title":"Test Organization","text":"<ul> <li><code>tests/providers/</code> - Provider-specific tests</li> <li><code>tests/test_*.py</code> - Feature tests</li> <li>203 total tests, 91% coverage</li> </ul>"},{"location":"getting-started/installation/","title":"Installation","text":""},{"location":"getting-started/installation/#requirements","title":"Requirements","text":"<ul> <li>Python 3.9 or higher</li> <li>pip</li> </ul>"},{"location":"getting-started/installation/#install-from-pypi","title":"Install from PyPI","text":"<pre><code># Core installation\npip install bruno-llm\n\n# With OpenAI support\npip install bruno-llm[openai]\n\n# With all optional dependencies\npip install bruno-llm[all]\n</code></pre>"},{"location":"getting-started/installation/#install-from-source","title":"Install from Source","text":"<pre><code># Clone repository\ngit clone https://github.com/meggy-ai/bruno-llm.git\ncd bruno-llm\n\n# Install bruno-core dependency\npip install git+https://github.com/meggy-ai/bruno-core.git@main\n\n# Install in development mode\npip install -e \".[all]\"\n</code></pre>"},{"location":"getting-started/installation/#verify-installation","title":"Verify Installation","text":"<pre><code>import bruno_llm\nprint(bruno_llm.__version__)\n</code></pre>"},{"location":"getting-started/installation/#next-steps","title":"Next Steps","text":"<ul> <li>Quick Start Guide</li> <li>User Guide</li> </ul>"},{"location":"getting-started/quickstart/","title":"Quick Start","text":""},{"location":"getting-started/quickstart/#basic-usage","title":"Basic Usage","text":""},{"location":"getting-started/quickstart/#1-create-a-provider","title":"1. Create a Provider","text":"<pre><code>from bruno_llm import LLMFactory\n\n# Ollama (local)\nllm = LLMFactory.create(\"ollama\", {\n    \"model\": \"llama2\",\n    \"base_url\": \"http://localhost:11434\"\n})\n\n# OpenAI (cloud)\nllm = LLMFactory.create(\"openai\", {\n    \"api_key\": \"sk-...\",\n    \"model\": \"gpt-4\"\n})\n</code></pre>"},{"location":"getting-started/quickstart/#2-generate-responses","title":"2. Generate Responses","text":"<pre><code>from bruno_core.models import Message, MessageRole\n\nmessages = [\n    Message(role=MessageRole.SYSTEM, content=\"You are a helpful assistant.\"),\n    Message(role=MessageRole.USER, content=\"What is Python?\")\n]\n\n# Generate complete response\nresponse = await llm.generate(messages)\nprint(response)\n</code></pre>"},{"location":"getting-started/quickstart/#3-stream-responses","title":"3. Stream Responses","text":"<pre><code># Stream response tokens\nasync for chunk in llm.stream(messages):\n    print(chunk, end=\"\", flush=True)\n</code></pre>"},{"location":"getting-started/quickstart/#complete-example","title":"Complete Example","text":"<pre><code>import asyncio\nfrom bruno_core.models import Message, MessageRole\nfrom bruno_llm import LLMFactory\n\nasync def main():\n    # Create provider\n    llm = LLMFactory.create(\"ollama\", {\"model\": \"llama2\"})\n\n    # Prepare messages\n    messages = [\n        Message(role=MessageRole.USER, content=\"Tell me a joke\")\n    ]\n\n    # Generate response\n    response = await llm.generate(messages)\n    print(f\"Response: {response}\")\n\n    # Get token count\n    tokens = llm.get_token_count(response)\n    print(f\"Tokens: {tokens}\")\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n</code></pre>"},{"location":"getting-started/quickstart/#next-steps","title":"Next Steps","text":"<ul> <li>User Guide - Complete documentation</li> <li>Ollama Provider - Local LLM setup</li> <li>OpenAI Provider - Cloud LLM setup</li> <li>Advanced Features - Caching, streaming, etc.</li> </ul>"},{"location":"user-guide/overview/","title":"User Guide Overview","text":"<p>Complete guide to using bruno-llm with bruno-core framework.</p>"},{"location":"user-guide/overview/#what-is-bruno-llm","title":"What is bruno-llm?","text":"<p>bruno-llm provides production-ready LLM provider implementations that integrate with the bruno-core framework. It enables you to:</p> <ul> <li>Use multiple LLM providers with a unified interface</li> <li>Switch between local and cloud models</li> <li>Leverage advanced features like caching, streaming, and cost tracking</li> </ul>"},{"location":"user-guide/overview/#providers","title":"Providers","text":"<ul> <li>Ollama - Local LLM inference</li> <li>OpenAI - Cloud GPT models</li> </ul>"},{"location":"user-guide/overview/#advanced-features","title":"Advanced Features","text":"<ul> <li>Response Caching - Cache LLM responses to reduce costs</li> <li>Context Management - Handle context window limits</li> <li>Streaming - Stream tokens as they're generated</li> <li>Cost Tracking - Track API costs per request</li> <li>Middleware - Add logging, validation, and custom processing</li> </ul>"},{"location":"user-guide/overview/#see-also","title":"See Also","text":"<ul> <li>API Reference</li> <li>Examples</li> </ul>"},{"location":"user-guide/advanced/caching/","title":"Response Caching","text":"<p>Documentation coming soon.</p>"},{"location":"user-guide/advanced/context/","title":"Context Management","text":"<p>Documentation coming soon.</p>"},{"location":"user-guide/advanced/cost-tracking/","title":"Cost Tracking","text":"<p>Documentation coming soon.</p>"},{"location":"user-guide/advanced/middleware/","title":"Middleware","text":"<p>Documentation coming soon.</p>"},{"location":"user-guide/advanced/streaming/","title":"Streaming","text":"<p>Documentation coming soon.</p>"},{"location":"user-guide/providers/ollama/","title":"Ollama Provider","text":"<p>Documentation coming soon. See examples/basic_usage.py for usage.</p>"},{"location":"user-guide/providers/openai/","title":"OpenAI Provider","text":"<p>Documentation coming soon. See examples/basic_usage.py for usage.</p>"}]}